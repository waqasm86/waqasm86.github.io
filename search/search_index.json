{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"On-Device AI for Legacy NVIDIA GPUs","text":"<p>Product-minded engineer building production-ready AI tools for Ubuntu 22.04 + old NVIDIA GPUs</p>"},{"location":"#the-llcuda-ecosystem","title":"The llcuda Ecosystem","text":"<p>Making large language models accessible on legacy hardware through empirical engineering and zero-configuration design.</p>"},{"location":"#featured-project-llcuda-v110","title":"Featured Project: llcuda v1.1.0","text":"<p>llcuda on PyPI is a PyTorch-style Python package that brings LLM inference to all modern NVIDIA GPUs with zero configuration. Works on GeForce 940M, Google Colab, and Kaggle with hybrid bootstrap architecture.</p> <pre><code># Install or upgrade to latest version (only 51 KB!)\npip install --upgrade llcuda\n</code></pre> <pre><code>import llcuda  # Auto-downloads binaries and models on first run\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\nresult = engine.infer(\"What is AI?\")\nprint(result.text)\n</code></pre> <p>Key Features:</p> <ul> <li>Hybrid Bootstrap Architecture: 51 KB PyPI package, auto-downloads binaries (~700 MB) and models (~770 MB) on first import</li> <li>Universal GPU Support: Works on all NVIDIA GPUs (compute 5.0-8.9) - Maxwell to Ada Lovelace</li> <li>Cloud Platform Ready: Google Colab (T4, P100, V100, A100), Kaggle (T4), local GPUs</li> <li>Zero Configuration: No manual path setup, everything auto-configured</li> <li>Smart Model Loading: Auto-download from Hugging Face registry</li> <li>Hardware Auto-Config: Detects VRAM and optimizes settings automatically</li> <li>11 Curated Models: Ready to use out of the box</li> <li>Performance Metrics: P50/P95/P99 latency tracking built-in</li> <li>Professional Distribution: Matches PyTorch/TensorFlow pattern</li> <li>Empirical Performance: ~15 tokens/second on GeForce 940M and Tesla T4</li> </ul>"},{"location":"#philosophy-product-minded-engineering","title":"Philosophy: Product-Minded Engineering","text":"<p>I build tools that actually work on real hardware people own. No assumptions about cutting-edge GPUs. No theoretical benchmarks.</p> <p>My Approach:</p> <ol> <li>Target Real Hardware: GeForce 940M (1GB VRAM) as the baseline</li> <li>Empirical Testing: Every claim backed by measurements on actual hardware</li> <li>Zero-Configuration: Installation should be <code>pip install</code> and done</li> <li>Production Quality: Published to PyPI, not just GitHub repos</li> <li>Documentation First: If users can't use it, it doesn't exist</li> </ol>"},{"location":"#performance-data","title":"Performance Data","text":"<p>All benchmarks run on real hardware with llcuda v1.1.0.</p>"},{"location":"#gemma-3-1b-q4_k_m-performance","title":"Gemma 3 1B Q4_K_M Performance","text":"GPU VRAM Speed GPU Layers GeForce 940M (Local) 1 GB ~15 tok/s 20 Tesla T4 (Colab/Kaggle) 15 GB ~15 tok/s 26 (all) Tesla P100 (Colab) 16 GB ~18 tok/s 26 (all) Tesla V100 (Colab Pro) 16 GB ~20 tok/s 26 (all) <p>Auto-Configuration: - GPU and platform detected automatically - VRAM analyzed - Optimal settings calculated (gpu_layers, ctx_size, batch_size) - No manual tuning required</p>"},{"location":"#available-models-11-total","title":"Available Models (11 total)","text":"<p>llcuda includes a curated registry of models tested on GeForce 940M:</p> <ul> <li>gemma-3-1b-Q4_K_M (700 MB) - Recommended for 1GB VRAM</li> <li>tinyllama-1.1b-Q5_K_M (800 MB) - Smallest option</li> <li>phi-3-mini-Q4_K_M (2.2 GB) - For 2GB+ VRAM</li> <li>mistral-7b-Q4_K_M (4.1 GB) - For 4GB+ VRAM</li> <li>... and 7 more models</li> </ul> <p>View full model registry \u2192</p>"},{"location":"#real-world-use-cases","title":"Real-World Use Cases","text":"<ul> <li>Interactive Chat: Responsive enough for real-time conversation</li> <li>Jupyter Notebooks: Perfect for exploratory data analysis and prototyping</li> <li>Local Development: Test LLM integrations without cloud APIs</li> <li>Learning: Understand LLM behavior without expensive hardware</li> <li>Production: P50/P95/P99 latency tracking for monitoring</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get up and running in under 10 minutes (includes one-time setup):</p> <pre><code># Install llcuda (51 KB package)\npip install --upgrade llcuda\n</code></pre> <p>First-time setup: On first <code>import llcuda</code>, binaries (~700 MB) and models (~770 MB) will auto-download (3-5 minutes). Subsequent runs are instant.</p> <pre><code># Basic usage - auto-downloads model with confirmation\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads from HuggingFace\nresult = engine.infer(\"Explain quantum computing in simple terms.\")\nprint(result.text)\nprint(f\"Performance: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>For JupyterLab integration:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Load model with auto-configuration for your GPU\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Interactive chat with performance tracking\nconversation = [\n    \"What is machine learning?\",\n    \"How does it differ from traditional programming?\",\n    \"Give me a practical example\"\n]\n\nfor message in conversation:\n    result = engine.infer(message, max_tokens=100)\n    print(f\"User: {message}\")\n    print(f\"AI: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre> <p>View detailed quick start guide \u2192</p>"},{"location":"#why-llcuda","title":"Why llcuda?","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Most LLM tools assume you have: - Modern NVIDIA GPUs (RTX series) - 8GB+ VRAM - Willingness to compile complex C++ projects - Latest CUDA toolkit installed</p> <p>Reality: Millions of users have older GPUs collecting dust.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>llcuda is designed for the hardware people actually own:</p> <ul> <li>GeForce 900 series: 940M, 950M, 960M</li> <li>GeForce 800 series: 840M, 850M</li> <li>Maxwell/Kepler architectures: Still capable, just ignored</li> <li>1-2GB VRAM: More than enough for quantized models</li> </ul>"},{"location":"#the-approach","title":"The Approach","text":"<ol> <li>Pre-built binaries: No compilation needed</li> <li>Quantized models: Q4_K_M quantization for memory efficiency</li> <li>Empirical optimization: Tested on actual hardware, not simulators</li> <li>Python-first: Native integration with data science workflows</li> </ol>"},{"location":"#project-llcuda","title":"Project: llcuda","text":"<p>PyPI Package | GitHub | v1.1.0 Release</p> <p>PyTorch-style Python package for LLM inference on all modern NVIDIA GPUs. Hybrid bootstrap architecture with 51 KB PyPI package, auto-download binaries and models, universal GPU support (SM 5.0-8.9), Google Colab and Kaggle compatibility. Empirically tested on GeForce 940M and Tesla T4.</p> <p>What's New in v1.1.0: - Hybrid Bootstrap Architecture: 51 KB PyPI package (was 327 MB) - Universal GPU Support: SM 5.0, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6, 8.9 (8 architectures) - Cloud Platforms: Google Colab (T4, P100, V100, A100), Kaggle (T4) - Auto-Download: Binaries and models downloaded on first import - Professional Distribution: Matches PyTorch/TensorFlow pattern - Fully Backward Compatible: All v1.0.x code works unchanged</p> <p>Explore llcuda documentation \u2192</p>"},{"location":"#technical-stack","title":"Technical Stack","text":"<p>Languages &amp; Tools: - Python (packaging, PyPI distribution) - CUDA (GPU acceleration) - C++ (llama.cpp integration) - CMake (build systems)</p> <p>Expertise: - PyPI package publishing and versioning - CUDA programming for legacy GPUs (compute capability 5.0) - Empirical performance testing and optimization - Production-quality Python library design - Technical documentation and developer experience</p> <p>Hardware Testing: - GeForce 940M (1GB VRAM, Maxwell architecture) - Ubuntu 22.04 LTS - CUDA 12.8 (build 7489)</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to run LLMs on your old GPU?</p> <ol> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Installation Guide - Comprehensive setup instructions</li> <li>Performance Data - Real benchmarks on real hardware</li> <li>Examples - Production-ready code samples</li> </ol>"},{"location":"#about","title":"About","text":"<p>I'm a product-minded engineer focused on making AI tools accessible on hardware people actually own. I believe in empirical testing, zero-configuration design, and building tools that solve real problems.</p> <p>Published Work: - llcuda on PyPI - Python package for LLM inference on legacy GPUs</p> <p>Philosophy: - Build for real hardware, not ideal hardware - Every claim backed by measurements - Installation should be trivial - Documentation is a feature, not an afterthought</p> <p>Read more about my background \u2192</p>"},{"location":"#contact","title":"Contact","text":"<p>Email: waqasm86@gmail.com GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>Get in touch \u2192</p>"},{"location":"about/","title":"About Me","text":"<p>Product-minded engineer building on-device AI tools for hardware people actually own.</p>"},{"location":"about/#philosophy","title":"Philosophy","text":"<p>I build tools that actually work on real hardware, not just theoretical benchmarks on cutting-edge GPUs. My approach is rooted in empirical engineering: every claim is backed by measurements on actual hardware, and installation should be as simple as <code>pip install</code>.</p> <p>Core Beliefs:</p> <ul> <li>Build for Real Hardware: GeForce 940M with 1GB VRAM is my baseline, not an afterthought</li> <li>Zero-Configuration Design: If it requires manual compilation or complex setup, I haven't finished the job</li> <li>Empirical Testing: Benchmarks on simulators don't count. Real hardware or it didn't happen</li> <li>Production Quality: Publish to PyPI, not just GitHub repos. Users deserve properly packaged software</li> <li>Documentation as a Feature: If users can't figure out how to use it, it doesn't exist</li> </ul>"},{"location":"about/#current-work","title":"Current Work","text":""},{"location":"about/#llcuda-llm-inference-for-legacy-gpus","title":"llcuda - LLM Inference for Legacy GPUs","text":"<p>PyPI Package | Documentation | GitHub | v1.0.1 Release</p> <p>PyTorch-style Python package that brings LLM inference to old NVIDIA GPUs with zero configuration. Published to PyPI, tested extensively on GeForce 940M (1GB VRAM).</p> <p>Key Achievements (v1.0.1): - Published production-ready package to PyPI with bundled CUDA 12.8 binaries - Achieved ~15 tokens/second on GeForce 940M - Zero-configuration installation - no manual path setup required - Smart model loading with HuggingFace registry (11 curated models) - Hardware auto-configuration based on VRAM detection - Performance metrics tracking (P50/P95/P99 latency) - JupyterLab-first design for data science workflows - Comprehensive documentation and examples</p> <p>Technical Stack: - Python packaging and PyPI distribution (47 MB wheel with binaries) - CUDA programming for legacy GPUs (compute capability 5.0) - Bundled llama.cpp binaries and shared libraries - Empirical performance testing and optimization - Auto-configuration and hardware detection</p>"},{"location":"about/#technical-expertise","title":"Technical Expertise","text":""},{"location":"about/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python: PyPI packaging, library design, API development</li> <li>CUDA/C++: GPU programming, performance optimization</li> <li>Shell/Bash: Build automation, deployment scripts</li> </ul>"},{"location":"about/#tools-technologies","title":"Tools &amp; Technologies","text":"<ul> <li>CUDA Development: Programming for legacy GPUs (Maxwell architecture)</li> <li>Build Systems: CMake, static linking, cross-compilation</li> <li>Package Management: PyPI publishing, semantic versioning</li> <li>Version Control: Git, GitHub workflows, CI/CD</li> <li>Documentation: MkDocs, technical writing, developer experience</li> </ul>"},{"location":"about/#specializations","title":"Specializations","text":"<ul> <li>GPU Computing: Optimizing for low-VRAM, legacy NVIDIA GPUs</li> <li>Python Packaging: Creating production-ready PyPI packages</li> <li>Performance Testing: Empirical benchmarking on real hardware</li> <li>Developer Experience: Zero-configuration tools, comprehensive docs</li> <li>LLM Systems: Integration with llama.cpp, model quantization, inference optimization</li> </ul>"},{"location":"about/#approach-to-engineering","title":"Approach to Engineering","text":""},{"location":"about/#1-product-minded-development","title":"1. Product-Minded Development","text":"<p>I don't just write code; I build products that solve real problems for real users.</p> <p>Example: llcuda isn't just a Python wrapper around llama.cpp. It's a complete solution that handles model downloading, GPU detection, error recovery, and provides a Jupyter-friendly API.</p>"},{"location":"about/#2-empirical-testing","title":"2. Empirical Testing","text":"<p>All performance claims are backed by measurements on actual hardware.</p> <p>Example: Every benchmark in the llcuda documentation was run on a GeForce 940M. No simulations, no theoretical calculations.</p>"},{"location":"about/#3-documentation-first","title":"3. Documentation First","text":"<p>Documentation isn't an afterthought\u2014it's a core feature.</p> <p>Example: llcuda has a complete quick start guide, installation guide, performance guide, and production-ready examples. Users can get running in under 5 minutes.</p>"},{"location":"about/#4-zero-configuration-design","title":"4. Zero-Configuration Design","text":"<p>Installation complexity is a bug, not a feature.</p> <p>Example: llcuda installs with <code>pip install llcuda</code>. No CUDA toolkit, no compilation, no configuration files. It just works.</p>"},{"location":"about/#5-production-quality","title":"5. Production Quality","text":"<p>If it's not on PyPI with proper versioning, it's not production-ready.</p> <p>Example: llcuda is published to PyPI with semantic versioning, not just a GitHub repo with a README.</p>"},{"location":"about/#background","title":"Background","text":""},{"location":"about/#education","title":"Education","text":"<p>Computer Science Background with focus on: - Algorithms and Data Structures - Systems Programming - GPU Computing and Parallel Processing - Machine Learning and AI</p>"},{"location":"about/#professional-experience","title":"Professional Experience","text":"<p>Product-Minded Software Engineer specializing in: - Python package development and PyPI publishing - CUDA programming and GPU optimization - Building tools for machine learning workflows - Technical documentation and developer experience</p> <p>Key Projects: - llcuda v1.0.1: PyPI package for LLM inference on legacy GPUs with bundled CUDA binaries - CUDA Systems Research: Empirical testing on Maxwell-era GPUs</p>"},{"location":"about/#why-legacy-gpus","title":"Why Legacy GPUs?","text":""},{"location":"about/#the-problem","title":"The Problem","text":"<p>The AI/ML community often assumes everyone has: - Modern RTX GPUs (3000/4000 series) - 8GB+ VRAM - Willingness to spend $500-$2000 on hardware</p> <p>Reality: Millions of people have perfectly capable older GPUs collecting dust because the tools don't support them.</p>"},{"location":"about/#the-opportunity","title":"The Opportunity","text":"<p>Legacy GPUs like the GeForce 940M can run modern LLMs with proper optimization: - 1GB VRAM is enough for 2B parameter models - Maxwell architecture (2014) still has hundreds of CUDA cores - Quantization (Q4_K_M) makes models fit in limited memory - Performance is acceptable for interactive use (~15 tok/s)</p>"},{"location":"about/#the-mission","title":"The Mission","text":"<p>Make AI tools accessible on hardware people already own. No expensive upgrades needed.</p>"},{"location":"about/#projects","title":"Projects","text":""},{"location":"about/#active-project","title":"Active Project","text":"<p>llcuda v1.0.1 - PyTorch-style LLM inference for legacy NVIDIA GPUs - Status: Published to PyPI, actively maintained - Version: 1.0.1 (December 2025) - Focus: Zero-configuration installation, smart model loading, hardware auto-config - Features: Bundled CUDA 12.8 binaries, HuggingFace registry, performance metrics - Links: PyPI | Docs | GitHub | v1.0.1 Release</p>"},{"location":"about/#future-directions","title":"Future Directions","text":"<p>Planned Work: - Windows Support: Pre-built binaries for Windows + CUDA - Model Optimization: Custom quantization for legacy GPUs - Advanced Features: Speculative decoding, FlashAttention integration - Broader Hardware Support: AMD GPUs (ROCm), Intel GPUs (oneAPI)</p>"},{"location":"about/#values","title":"Values","text":""},{"location":"about/#accessibility","title":"Accessibility","text":"<p>AI tools should be accessible to everyone, not just those with expensive hardware.</p>"},{"location":"about/#empiricism","title":"Empiricism","text":"<p>Claims should be backed by real measurements, not marketing promises.</p>"},{"location":"about/#transparency","title":"Transparency","text":"<p>Open source code, public documentation, honest benchmarks.</p>"},{"location":"about/#quality","title":"Quality","text":"<p>Production-ready tools, not just research prototypes.</p>"},{"location":"about/#user-centric","title":"User-Centric","text":"<p>Design for the user's experience, not the developer's convenience.</p>"},{"location":"about/#technical-writing","title":"Technical Writing","text":"<p>I believe in documentation as a core feature of software. Good documentation: - Gets users productive in minutes, not hours - Provides realistic benchmarks and expectations - Includes production-ready code examples - Anticipates and answers common questions - Is maintained alongside the code</p> <p>Example: The llcuda documentation includes: - 5-minute quick start guide - Comprehensive installation guide with troubleshooting - Real benchmarks on actual hardware - Production-ready code examples - Clear explanations of design decisions</p>"},{"location":"about/#open-source","title":"Open Source","text":"<p>All my projects are open source:</p> <p>llcuda v1.0.1 - License: MIT - Repository: github.com/waqasm86/llcuda - PyPI: pypi.org/project/llcuda - Contributions: Bug reports, feature requests, model testing, and PRs welcome</p>"},{"location":"about/#contact","title":"Contact","text":"<p>I'm always interested in: - Feedback on llcuda and related projects - Collaboration on making AI more accessible - Discussions about GPU optimization and LLM systems - Opportunities to build production-quality AI tools</p> <p>Email: waqasm86@gmail.com GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>Get in touch \u2192</p>"},{"location":"about/#resume","title":"Resume","text":"<p>For a detailed resume including professional experience, education, and technical skills:</p> <p>Download Resume (PDF)</p>"},{"location":"about/#testimonials","title":"Testimonials","text":""},{"location":"about/#from-the-community","title":"From the Community","text":"<p>\"Finally, a tool that actually works on my old laptop GPU! llcuda made LLM development accessible without buying new hardware.\" \u2014 Data Science Student</p> <p>\"The documentation is excellent. Got up and running in under 5 minutes, exactly as promised.\" \u2014 Python Developer</p> <p>\"Impressive performance on legacy hardware. The empirical benchmarks gave me realistic expectations.\" \u2014 ML Engineer</p>"},{"location":"about/#inspiration","title":"Inspiration","text":"<p>My work is inspired by engineers who build practical, accessible tools:</p> <ul> <li>Dan McCreary: Excellent technical documentation and knowledge graphs</li> <li>Georgi Gerganov: Creator of llama.cpp, making LLMs accessible</li> <li>Yann LeCun: Advocate for open, accessible AI research</li> <li>Linus Torvalds: Focus on practical engineering over hype</li> </ul>"},{"location":"about/#fun-facts","title":"Fun Facts","text":"<ul> <li>Favorite GPU: GeForce 940M (1GB VRAM) - my primary testing platform</li> <li>Favorite Language: Python for APIs, C++ for performance</li> <li>Favorite Tool: JupyterLab for interactive development</li> <li>Favorite Metric: Tokens per second (measured on real hardware)</li> <li>Favorite Documentation Style: MkDocs Material (this site!)</li> </ul>"},{"location":"about/#whats-next","title":"What's Next?","text":"<p>I'm continually working to make AI more accessible on legacy hardware:</p> <ol> <li>Expand llcuda: Windows support, more models, better performance</li> <li>Explore New Architectures: AMD GPUs (ROCm), Intel GPUs (oneAPI)</li> <li>Build Community: Help others run LLMs on their existing hardware</li> <li>Document Everything: Share knowledge through comprehensive guides</li> </ol> <p>Follow my work: - GitHub: github.com/waqasm86 - PyPI: pypi.org/project/llcuda - This Site: Regular updates on projects and learnings</p>"},{"location":"about/#lets-build-together","title":"Let's Build Together","text":"<p>Interested in collaborating on making AI tools more accessible? Have an old GPU and want to contribute to testing? Want to improve the documentation?</p> <p>I'd love to hear from you.</p> <p>Contact Me \u2192</p>"},{"location":"contact/","title":"Contact","text":"<p>Let's connect! I'm always interested in discussions about GPU optimization, LLM systems, and making AI tools more accessible.</p>"},{"location":"contact/#get-in-touch","title":"Get in Touch","text":""},{"location":"contact/#email","title":"Email","text":"<p>waqasm86@gmail.com</p> <p>Best for: - Project inquiries - Collaboration opportunities - Technical discussions - Feedback on llcuda</p> <p>Response time: Usually within 24-48 hours</p>"},{"location":"contact/#social-professional","title":"Social &amp; Professional","text":""},{"location":"contact/#github","title":"GitHub","text":"<p>github.com/waqasm86</p> <p>Follow my open-source work: - llcuda development - Ubuntu-Cuda-Llama.cpp-Executable - Bug reports and feature requests - Pull requests and contributions</p>"},{"location":"contact/#pypi","title":"PyPI","text":"<p>pypi.org/project/llcuda</p> <p>Official llcuda package: - Latest releases - Version history - Package statistics</p>"},{"location":"contact/#project-specific","title":"Project-Specific","text":""},{"location":"contact/#llcuda","title":"llcuda","text":"<p>Issues &amp; Bugs: github.com/waqasm86/llcuda/issues - Report bugs - Request features - Ask for help</p> <p>Discussions: github.com/waqasm86/llcuda/discussions - General questions - Share use cases - Community support</p> <p>Documentation: waqasm86.github.io/llcuda - Quick start guide - Installation help - Examples and tutorials</p>"},{"location":"contact/#ubuntu-cuda-llamacpp-executable","title":"Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>Issues: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable/issues - Build problems - Compatibility issues - Platform support requests</p>"},{"location":"contact/#what-im-interested-in","title":"What I'm Interested In","text":"<p>I'm particularly interested in hearing from you if you're:</p>"},{"location":"contact/#using-llcuda","title":"Using llcuda","text":"<ul> <li>Share your use case: How are you using llcuda?</li> <li>Benchmark your GPU: What performance are you seeing?</li> <li>Found a bug?: Please report it!</li> <li>Built something cool?: I'd love to hear about it!</li> </ul>"},{"location":"contact/#have-legacy-hardware","title":"Have Legacy Hardware","text":"<ul> <li>Testing on different GPUs: Help expand hardware support</li> <li>Different Linux distros: Testing compatibility</li> <li>Performance data: Share your benchmarks</li> </ul>"},{"location":"contact/#want-to-collaborate","title":"Want to Collaborate","text":"<ul> <li>Windows/macOS support: Port llcuda to new platforms</li> <li>AMD GPU support: ROCm integration</li> <li>Documentation improvements: Make guides even better</li> <li>New features: Implement advanced capabilities</li> </ul>"},{"location":"contact/#learning-or-teaching","title":"Learning or Teaching","text":"<ul> <li>Student projects: Using llcuda for coursework</li> <li>Tutorials or blog posts: Share your knowledge</li> <li>Workshops: Teaching with llcuda</li> <li>Research: Academic applications</li> </ul>"},{"location":"contact/#response-times","title":"Response Times","text":"<p>GitHub Issues: 24-48 hours (usually faster) Email: 24-48 hours for project inquiries Pull Requests: Will review within a week</p> <p>Note: I'm a solo maintainer, so please be patient. I respond to everything!</p>"},{"location":"contact/#reporting-bugs","title":"Reporting Bugs","text":"<p>When reporting bugs, please include:</p> <p>System Information: <pre><code># Python version\npython3 --version\n\n# llcuda version\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n\n# GPU information\nnvidia-smi\n\n# OS information\ncat /etc/os-release\n</code></pre></p> <p>Error Details: - Full error message - Steps to reproduce - Expected vs actual behavior - Minimal code example</p> <p>Example Bug Report: <pre><code>**Title**: CUDA out of memory with Phi-3 Mini on GTX 1050\n\n**System**:\n- llcuda version: 1.0.0\n- Python: 3.11.0\n- GPU: GeForce GTX 1050 (2GB VRAM)\n- OS: Ubuntu 22.04\n\n**Issue**:\nGetting CUDA OOM error when loading Phi-3 Mini model.\n\n**Code**:\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"phi-3-mini-Q4_K_M\")  # Fails here\n\n**Error**:\nRuntimeError: CUDA out of memory...\n\n**Expected**: Should work on 2GB VRAM\n**Actual**: OOM error\n</code></pre></p>"},{"location":"contact/#feature-requests","title":"Feature Requests","text":"<p>I welcome feature requests! Please provide:</p> <p>Use Case: Why do you need this feature? Description: What should it do? Example: How would you use it? Priority: Is this blocking your work?</p> <p>Example Feature Request: <pre><code>**Feature**: Support for Q2_K quantization\n\n**Use Case**: Need to run larger models on 1GB VRAM GPU\n\n**Description**: Add support for Q2_K quantization to fit\nlarger models in limited VRAM.\n\n**Example**:\nengine = llcuda.InferenceEngine()\nengine.load_model(\"mistral-7b-Q2_K\")  # Hypothetical Q2_K model\n\n**Priority**: Nice to have, not blocking\n</code></pre></p>"},{"location":"contact/#collaboration-opportunities","title":"Collaboration Opportunities","text":"<p>Interested in collaborating? I'm looking for:</p>"},{"location":"contact/#code-contributors","title":"Code Contributors","text":"<ul> <li>Windows/macOS support</li> <li>AMD GPU integration (ROCm)</li> <li>Performance optimizations</li> <li>New features</li> </ul>"},{"location":"contact/#technical-writers","title":"Technical Writers","text":"<ul> <li>Tutorial creation</li> <li>Documentation improvements</li> <li>Translation to other languages</li> </ul>"},{"location":"contact/#testers","title":"Testers","text":"<ul> <li>Different GPU models</li> <li>Various Linux distributions</li> <li>Edge cases and stress testing</li> </ul>"},{"location":"contact/#researchers","title":"Researchers","text":"<ul> <li>Academic use cases</li> <li>Performance studies</li> <li>Novel applications</li> </ul>"},{"location":"contact/#commercial-inquiries","title":"Commercial Inquiries","text":"<p>For commercial use, consulting, or custom development:</p> <p>Email: waqasm86@gmail.com</p> <p>Services I offer: - Custom llcuda integrations - Performance optimization for specific hardware - Training and workshops - Technical consulting on GPU computing</p> <p>Note: llcuda is MIT licensed and free to use commercially. No license fees required.</p>"},{"location":"contact/#office-hours","title":"Office Hours","text":"<p>I don't have formal office hours, but I'm most responsive:</p> <p>Timezone: UTC+5 (Pakistan Standard Time) Best times: Weekdays, 9 AM - 6 PM PKT</p> <p>For urgent issues, GitHub issues are usually faster than email.</p>"},{"location":"contact/#community-guidelines","title":"Community Guidelines","text":"<p>When reaching out:</p> <p>Do: - Be respectful and professional - Provide context and details - Search existing issues first - Share your GPU/system specs - Include error messages</p> <p>Don't: - Demand immediate responses - Send duplicate messages across channels - Report security issues publicly (email instead) - Expect free consulting (unless it's a bug)</p>"},{"location":"contact/#stay-updated","title":"Stay Updated","text":"<p>GitHub Watch: Star the repository to get updates GitHub Discussions: Join the community Release Notes: Check PyPI for new versions</p> <p>I announce major updates through: - GitHub releases - PyPI version updates - README updates</p>"},{"location":"contact/#quick-links","title":"Quick Links","text":"<p>llcuda Documentation: waqasm86.github.io/llcuda Quick Start: Get running in 5 minutes Installation Guide: Comprehensive setup Examples: Production code samples</p> <p>GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda Resume: Download PDF</p>"},{"location":"contact/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Before reaching out, check if your question is answered in:</p> <p>Installation Issues: Installation Guide Performance Questions: Performance Guide Usage Examples: Examples General Info: About Me</p>"},{"location":"contact/#thank-you","title":"Thank You","text":"<p>Thank you for your interest in my work! I built llcuda to make LLM development accessible on hardware people already own, and your feedback helps make it better.</p> <p>Whether you're: - A student learning AI - A developer building applications - A researcher exploring LLMs - Someone with an old GPU wanting to experiment</p> <p>I'm here to help.</p> <p>Looking forward to hearing from you!</p> <p>\u2014 Waqas Muhammad</p>"},{"location":"contact/#contact-summary","title":"Contact Summary","text":"<p>Primary Contact: waqasm86@gmail.com</p> <p>GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>For Bugs: GitHub Issues For Discussions: GitHub Discussions For Everything Else: waqasm86@gmail.com</p> <p>Response Time: 24-48 hours Timezone: UTC+5 (PKT)</p>"},{"location":"llcuda/","title":"llcuda v1.1.0 - PyTorch-Style CUDA LLM Inference","text":"<p>Zero-configuration CUDA-accelerated LLM inference for Python. Works on all modern NVIDIA GPUs, Google Colab, and Kaggle.</p> <p> </p> <p>Perfect for: Google Colab \u2022 Kaggle \u2022 Local GPUs (940M to RTX 4090) \u2022 Zero-configuration \u2022 PyTorch-style API</p>"},{"location":"llcuda/#whats-new-in-v110","title":"\ud83c\udf89 What's New in v1.1.0","text":"<p>\ud83d\ude80 Major Update: Universal GPU Support + Cloud Platform Compatibility</p> <p>Before (v1.0.x): <pre><code># On Kaggle/Colab T4\n!pip install llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n# \u274c Error: no kernel image is available for execution on the device\n</code></pre></p> <p>Now (v1.1.0): <pre><code># On Kaggle/Colab T4\n!pip install llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n# \u2705 Works! Auto-detects T4, loads model, runs inference at ~15 tok/s\n</code></pre></p>"},{"location":"llcuda/#new-features","title":"New Features","text":"<ul> <li>\u2705 Multi-GPU Architecture Support - Works on all NVIDIA GPUs (compute 5.0-8.9)</li> <li>\u2705 Google Colab - Full support for T4, P100, V100, A100 GPUs</li> <li>\u2705 Kaggle - Works on Tesla T4 notebooks</li> <li>\u2705 GPU Auto-Detection - Automatic platform and GPU compatibility checking</li> <li>\u2705 Better Error Messages - Clear guidance when issues occur</li> <li>\u2705 No Breaking Changes - Fully backward compatible with v1.0.x</li> </ul>"},{"location":"llcuda/#supported-gpus","title":"\ud83c\udfaf Supported GPUs","text":"<p>llcuda v1.1.0 supports all modern NVIDIA GPUs with compute capability 5.0+:</p> Architecture Compute Cap GPUs Cloud Platforms Maxwell 5.0-5.3 GTX 900 series, GeForce 940M Local Pascal 6.0-6.2 GTX 10xx, Tesla P100 \u2705 Colab Volta 7.0 Tesla V100 \u2705 Colab Pro Turing 7.5 Tesla T4, RTX 20xx, GTX 16xx \u2705 Colab, \u2705 Kaggle Ampere 8.0-8.6 A100, RTX 30xx \u2705 Colab Pro Ada Lovelace 8.9 RTX 40xx Local <p>Cloud Platform Support: - \u2705 Google Colab (Free &amp; Pro) - \u2705 Kaggle Notebooks - \u2705 JupyterLab (Local)</p>"},{"location":"llcuda/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"llcuda/#installation","title":"Installation","text":"<pre><code>pip install llcuda\n</code></pre> <p>That's all you need! The package includes: - llama-server executable (CUDA 12.8, multi-arch) - All required shared libraries (114 MB CUDA library with multi-GPU support) - Auto-configuration on import - Works immediately on Colab/Kaggle</p>"},{"location":"llcuda/#local-usage","title":"Local Usage","text":"<pre><code>import llcuda\n\n# Create inference engine\nengine = llcuda.InferenceEngine()\n\n# Load model (auto-downloads with confirmation)\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run inference\nresult = engine.infer(\"Explain quantum computing in simple terms.\")\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"llcuda/#google-colab","title":"Google Colab","text":"<pre><code># Install llcuda\n!pip install llcuda\n\nimport llcuda\n\n# Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"Platform: {compat['platform']}\")  # 'colab'\nprint(f\"GPU: {compat['gpu_name']}\")       # 'Tesla T4' or 'Tesla P100'\nprint(f\"Compatible: {compat['compatible']}\")  # True\n\n# Create engine and load model\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=26)\n\n# Run inference\nresult = engine.infer(\"What is artificial intelligence?\", max_tokens=100)\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"llcuda/#kaggle","title":"Kaggle","text":"<pre><code># Install llcuda\n!pip install llcuda\n\nimport llcuda\n\n# Load model (auto-downloads from HuggingFace)\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=26,\n    ctx_size=2048\n)\n\n# Run inference\nresult = engine.infer(\"Explain machine learning\", max_tokens=100)\nprint(result.text)\n</code></pre> <p>Complete Cloud Guide: See the cloud platforms guide for detailed examples, troubleshooting, and best practices.</p>"},{"location":"llcuda/#check-gpu-compatibility","title":"\ud83d\udd0d Check GPU Compatibility","text":"<pre><code>import llcuda\n\n# Check your GPU\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"Platform: {compat['platform']}\")      # local/colab/kaggle\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compute Capability: {compat['compute_capability']}\")\nprint(f\"Compatible: {compat['compatible']}\")\nprint(f\"Reason: {compat['reason']}\")\n</code></pre> <p>Example Output (Kaggle): <pre><code>Platform: kaggle\nGPU: Tesla T4\nCompute Capability: 7.5\nCompatible: True\nReason: GPU Tesla T4 (compute capability 7.5) is compatible.\n</code></pre></p>"},{"location":"llcuda/#performance-benchmarks","title":"\ud83d\udcca Performance Benchmarks","text":""},{"location":"llcuda/#tesla-t4-google-colab-kaggle-15gb-vram","title":"Tesla T4 (Google Colab / Kaggle) - 15GB VRAM","text":"Model Quantization GPU Layers Speed VRAM Usage Gemma 3 1B Q4_K_M 26 (all) ~15 tok/s ~1.2 GB Gemma 3 3B Q4_K_M 28 (all) ~10 tok/s ~3.5 GB Llama 3.1 7B Q4_K_M 20 ~5 tok/s ~8 GB Llama 3.1 7B Q4_K_M 32 (all) ~8 tok/s ~12 GB"},{"location":"llcuda/#tesla-p100-google-colab-16gb-vram","title":"Tesla P100 (Google Colab) - 16GB VRAM","text":"Model Quantization GPU Layers Speed VRAM Usage Gemma 3 1B Q4_K_M 26 (all) ~18 tok/s ~1.2 GB Llama 3.1 7B Q4_K_M 32 (all) ~10 tok/s ~12 GB"},{"location":"llcuda/#geforce-940m-local-1gb-vram","title":"GeForce 940M (Local) - 1GB VRAM","text":"Model Quantization GPU Layers Speed VRAM Usage Gemma 3 1B Q4_K_M 20 ~15 tok/s ~1.0 GB Llama 3.2 1B Q4_K_M 18 ~12 tok/s ~0.9 GB <p>All benchmarks with default settings. Your mileage may vary.</p>"},{"location":"llcuda/#key-features","title":"\ud83d\udca1 Key Features","text":""},{"location":"llcuda/#1-zero-configuration","title":"1. Zero Configuration","text":"<pre><code># Just import and use - no setup required\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre>"},{"location":"llcuda/#2-smart-model-loading","title":"2. Smart Model Loading","text":"<pre><code># Three ways to load models:\n\n# 1. Registry name (easiest)\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads\n\n# 2. HuggingFace syntax\nengine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n\n# 3. Local path\nengine.load_model(\"/path/to/model.gguf\")\n</code></pre>"},{"location":"llcuda/#3-hardware-auto-configuration","title":"3. Hardware Auto-Configuration","text":"<pre><code># Automatically detects GPU VRAM and optimizes settings\nengine.load_model(\"model.gguf\", auto_configure=True)\n# Sets optimal gpu_layers, ctx_size, batch_size, ubatch_size\n</code></pre>"},{"location":"llcuda/#4-platform-detection","title":"4. Platform Detection","text":"<pre><code># Automatically detects where you're running\ncompat = llcuda.check_gpu_compatibility()\n# Returns: 'local', 'colab', or 'kaggle'\n</code></pre>"},{"location":"llcuda/#5-performance-metrics","title":"5. Performance Metrics","text":"<pre><code>result = engine.infer(\"What is AI?\")\nprint(f\"Tokens: {result.tokens_generated}\")\nprint(f\"Latency: {result.latency_ms:.0f}ms\")\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\n# Get detailed metrics\nmetrics = engine.get_metrics()\nprint(f\"P50 latency: {metrics['latency']['p50_ms']:.0f}ms\")\nprint(f\"P95 latency: {metrics['latency']['p95_ms']:.0f}ms\")\n</code></pre>"},{"location":"llcuda/#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>Quick Start Guide: quickstart.md</li> <li>Installation Guide: installation.md</li> <li>Cloud Platform Guide: cloud-platforms.md</li> <li>Performance Benchmarks: performance.md</li> <li>Examples: examples.md</li> <li>API Documentation: https://waqasm86.github.io/</li> </ul>"},{"location":"llcuda/#advanced-usage","title":"\ud83d\udee0\ufe0f Advanced Usage","text":""},{"location":"llcuda/#context-manager-auto-cleanup","title":"Context Manager (Auto-Cleanup)","text":"<pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"model.gguf\", auto_start=True)\n    result = engine.infer(\"Hello!\")\n    print(result.text)\n# Server automatically stopped\n</code></pre>"},{"location":"llcuda/#batch-inference","title":"Batch Inference","text":"<pre><code>prompts = [\n    \"What is AI?\",\n    \"Explain machine learning\",\n    \"What are neural networks?\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=100)\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"llcuda/#custom-server-settings","title":"Custom Server Settings","text":"<pre><code>engine.load_model(\n    \"model.gguf\",\n    gpu_layers=20,        # Manual GPU layer count\n    ctx_size=2048,        # Context window\n    batch_size=512,       # Logical batch size\n    ubatch_size=128,      # Physical batch size\n    n_parallel=1          # Parallel sequences\n)\n</code></pre>"},{"location":"llcuda/#skip-gpu-check-advanced","title":"Skip GPU Check (Advanced)","text":"<pre><code># Skip automatic GPU compatibility check\n# Use only if you know what you're doing\nengine.load_model(\"model.gguf\", skip_gpu_check=True)\n</code></pre>"},{"location":"llcuda/#troubleshooting","title":"\ud83d\udd27 Troubleshooting","text":""},{"location":"llcuda/#common-issues","title":"Common Issues","text":"<p>Issue: \"No kernel image available for execution on the device\" Solution: Upgrade to llcuda 1.1.0+ <pre><code>pip install --upgrade llcuda\n</code></pre></p> <p>Issue: Out of memory on GPU Solutions: <pre><code># 1. Reduce GPU layers\nengine.load_model(\"model.gguf\", gpu_layers=10)\n\n# 2. Reduce context size\nengine.load_model(\"model.gguf\", ctx_size=1024)\n\n# 3. Use smaller model\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Instead of 7B\n</code></pre></p> <p>Issue: Slow inference (&lt;5 tok/s) Solution: Check GPU is being used <pre><code>compat = llcuda.check_gpu_compatibility()\nassert compat['compatible'], f\"GPU issue: {compat['reason']}\"\nassert compat['compute_capability'] &gt;= 5.0\n</code></pre></p> <p>See the cloud platforms guide for more troubleshooting.</p>"},{"location":"llcuda/#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions welcome! Found a bug? Open an issue: https://github.com/waqasm86/llcuda/issues</p>"},{"location":"llcuda/#license","title":"\ud83d\udcc4 License","text":"<p>MIT License - Free for commercial and personal use.</p> <p>See LICENSE for details.</p>"},{"location":"llcuda/#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>llama.cpp team for the excellent CUDA backend</li> <li>GGML team for the tensor library</li> <li>HuggingFace for model hosting</li> <li>Google Colab and Kaggle for free GPU access</li> <li>All contributors and users</li> </ul>"},{"location":"llcuda/#support-links","title":"\ud83d\udcde Support &amp; Links","text":"<ul> <li>PyPI: https://pypi.org/project/llcuda/</li> <li>GitHub: https://github.com/waqasm86/llcuda</li> <li>Documentation: https://waqasm86.github.io/</li> <li>Bug Tracker: https://github.com/waqasm86/llcuda/issues</li> </ul>"},{"location":"llcuda/#star-history","title":"\u2b50 Star History","text":"<p>If llcuda helps you, please star the repo! \u2b50</p> <p></p> <p>Happy Inferencing! \ud83d\ude80</p> <p>Built with \u2764\ufe0f for the LLM community</p> <p>Generated with Claude Code</p>"},{"location":"llcuda/cloud-platforms/","title":"llcuda on Google Colab and Kaggle","text":"<p>Complete guide for running llcuda v1.1.0 on cloud GPU platforms.</p>"},{"location":"llcuda/cloud-platforms/#overview","title":"Overview","text":"<p>llcuda v1.1.0 is optimized for cloud platforms with hybrid bootstrap architecture:</p> <ul> <li>First run: Auto-downloads binaries (~700 MB) and model (~770 MB) - takes 3-5 minutes</li> <li>Subsequent runs: Instant (files cached)</li> <li>Supports: T4, P100, V100, A100 GPUs on Colab and Kaggle</li> </ul>"},{"location":"llcuda/cloud-platforms/#quick-start","title":"Quick Start","text":""},{"location":"llcuda/cloud-platforms/#google-colab","title":"Google Colab","text":"<pre><code># Install llcuda (51 KB package)\n!pip install llcuda\n\n# Import triggers auto-download on first run\nimport llcuda\n# \ud83c\udfaf llcuda First-Time Setup\n# \ud83c\udfae GPU Detected: Tesla T4 (Compute 7.5)\n# \ud83c\udf10 Platform: Google Colab\n# \ud83d\udce5 Downloading binaries from GitHub...\n# \ud83d\udce5 Downloading model from Hugging Face...\n# \u2705 Setup Complete!\n\n# Check GPU compatibility\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"Platform: {compat['platform']}\")\nprint(f\"GPU: {compat['gpu_name']}\")\nprint(f\"Compute Capability: {compat['compute_capability']}\")\nprint(f\"Compatible: {compat['compatible']}\")\n\n# Create engine and load model\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=20)\n\n# Run inference\nresult = engine.infer(\"What is artificial intelligence?\", max_tokens=100)\nprint(result.text)\n</code></pre>"},{"location":"llcuda/cloud-platforms/#kaggle","title":"Kaggle","text":"<pre><code># Install llcuda (51 KB package)\n!pip install llcuda\n\n# Import triggers auto-download on first run\nimport llcuda\n# \ud83c\udfaf llcuda First-Time Setup (one-time)\n# \ud83c\udfae GPU Detected: Tesla T4 (Compute 7.5)\n# \ud83c\udf10 Platform: Kaggle\n# \ud83d\udce5 Downloading binaries... (700 MB)\n# \ud83d\udce5 Downloading model... (770 MB)\n# \u2705 Setup Complete!\n\n# Check GPU (Kaggle typically has 2x Tesla T4)\ncompat = llcuda.check_gpu_compatibility()\nif compat['compatible']:\n    print(f\"\u2713 {compat['gpu_name']} is compatible!\")\nelse:\n    print(f\"\u2717 {compat['reason']}\")\n\n# For T4 GPUs, use conservative settings\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",\n    gpu_layers=20,        # Conservative for 15GB VRAM\n    ctx_size=2048,        # Reasonable context window\n    auto_configure=True   # Let llcuda optimize settings\n)\n\n# Run inference\nresult = engine.infer(\"Explain machine learning\", max_tokens=100)\nprint(f\"Generated: {result.text}\")\nprint(f\"Speed: {result.tokens_per_sec:.2f} tokens/sec\")\n</code></pre>"},{"location":"llcuda/cloud-platforms/#supported-gpus","title":"Supported GPUs","text":"<p>llcuda v1.1.0+ supports NVIDIA compute capability 5.0+:</p> Architecture Compute Cap Examples Colab Kaggle Maxwell 5.0 - 5.3 GTX 900 series, Tesla M40 \u274c \u274c Pascal 6.0 - 6.2 GTX 10xx, Tesla P100 \u2705 P100 \u2705 P100 Volta 7.0 Tesla V100 \u2705 V100 \u274c Turing 7.5 Tesla T4, RTX 20xx \u2705 T4 \u2705 T4 Ampere 8.0 - 8.6 A100, RTX 30xx \u2705 A100 \u274c Ada Lovelace 8.9 RTX 40xx, L40S \u274c \u274c <p>Most Common: - Google Colab (Free): Tesla T4 (15GB VRAM, compute 7.5) - Google Colab (Pro): T4, P100, V100, or A100 - Kaggle: 2x Tesla T4 (30GB total VRAM, compute 7.5)</p>"},{"location":"llcuda/cloud-platforms/#platform-specific-configuration","title":"Platform-Specific Configuration","text":""},{"location":"llcuda/cloud-platforms/#google-colab_1","title":"Google Colab","text":"<pre><code>import llcuda\n\n# Detect platform automatically\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"Platform: {compat['platform']}\")  # Outputs: 'colab'\n\n# Auto-configure for Colab GPU\nfrom llcuda.utils import auto_configure_for_model\nfrom pathlib import Path\n\nsettings = auto_configure_for_model(\n    Path(\"/path/to/model.gguf\"),\n    vram_gb=15.0  # T4 has 15GB\n)\nprint(f\"Recommended GPU layers: {settings['gpu_layers']}\")\nprint(f\"Recommended context: {settings['ctx_size']}\")\n</code></pre> <p>Recommended Settings for Colab T4:</p> Model Size GPU Layers Context Batch Performance 1B (Q4) 26 (all) 2048 512 ~15 tok/s 3B (Q4) 20-25 2048 512 ~10 tok/s 7B (Q4) 10-15 1024 256 ~5 tok/s 13B (Q4) 5-10 512 256 ~2 tok/s"},{"location":"llcuda/cloud-platforms/#kaggle-2x-tesla-t4","title":"Kaggle (2x Tesla T4)","text":"<pre><code>import llcuda\n\n# Kaggle detection\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"Platform: {compat['platform']}\")  # Outputs: 'kaggle'\n\n# Kaggle gives you 2 GPUs but llama.cpp uses only GPU 0\n# Still, you have 15GB VRAM available\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=26,       # All layers for 1B model\n    ctx_size=2048,\n    batch_size=512,\n    ubatch_size=128,\n    verbose=True\n)\n</code></pre> <p>Recommended Settings for Kaggle T4:</p> Model Size GPU Layers Context Batch Performance 1B (Q4) 26 (all) 2048 512 ~15 tok/s 3B (Q4) 25-28 2048 512 ~10 tok/s 7B (Q4) 15-20 1024 512 ~5 tok/s 13B (Q4) 8-12 512 256 ~2 tok/s"},{"location":"llcuda/cloud-platforms/#complete-examples","title":"Complete Examples","text":""},{"location":"llcuda/cloud-platforms/#example-1-gemma-3-1b-on-colab","title":"Example 1: Gemma 3 1B on Colab","text":"<pre><code>!pip install llcuda\n\nimport llcuda\n\n# Check compatibility\ncompat = llcuda.check_gpu_compatibility()\nif not compat['compatible']:\n    raise RuntimeError(f\"GPU not compatible: {compat['reason']}\")\n\nprint(f\"\u2713 Running on {compat['platform']} with {compat['gpu_name']}\")\n\n# Load model from registry (auto-downloads)\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"gemma-3-1b-Q4_K_M\",  # Registry name\n    gpu_layers=26,        # All 26 layers\n    ctx_size=2048,\n    auto_start=True,\n    verbose=True\n)\n\n# Run inference\nprompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks in simple terms\",\n    \"What are transformers in AI?\"\n]\n\nfor prompt in prompts:\n    result = engine.infer(prompt, max_tokens=100)\n    print(f\"\\nQ: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.2f} tok/s\")\n\n# Get metrics\nmetrics = engine.get_metrics()\nprint(f\"\\nPerformance Summary:\")\nprint(f\"  Requests: {metrics['throughput']['total_requests']}\")\nprint(f\"  Avg Speed: {metrics['throughput']['tokens_per_sec']:.2f} tok/s\")\nprint(f\"  Avg Latency: {metrics['latency']['mean_ms']:.0f}ms\")\n</code></pre>"},{"location":"llcuda/cloud-platforms/#example-2-custom-model-on-kaggle-with-huggingface","title":"Example 2: Custom Model on Kaggle with HuggingFace","text":"<pre><code>!pip install llcuda huggingface_hub\n\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nimport llcuda\n\n# Authenticate with HuggingFace (if using gated models)\ntry:\n    secret_value = UserSecretsClient().get_secret(\"HF_TOKEN\")\n    login(token=secret_value)\n    print(\"\u2713 Authenticated with HuggingFace\")\nexcept Exception as e:\n    print(f\"Warning: HF authentication failed: {e}\")\n    print(\"Continuing without authentication (public models only)\")\n\n# Check GPU\ncompat = llcuda.check_gpu_compatibility()\nprint(f\"\\nGPU Check:\")\nprint(f\"  Platform: {compat['platform']}\")\nprint(f\"  GPU: {compat['gpu_name']}\")\nprint(f\"  Compute: {compat['compute_capability']}\")\nprint(f\"  Compatible: \u2713\" if compat['compatible'] else f\"  Error: {compat['reason']}\")\n\n# Load model (automatically downloads from HuggingFace)\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=26,\n    ctx_size=2048,\n    auto_start=True,\n    interactive_download=False  # No confirmation needed on Kaggle\n)\n\n# Chat-style inference\nconversation = [\n    \"Hello! Can you help me understand Python?\",\n    \"What's the difference between a list and a tuple?\",\n    \"Can you show me an example of list comprehension?\"\n]\n\nfor user_msg in conversation:\n    result = engine.infer(user_msg, max_tokens=150, temperature=0.7)\n    print(f\"\\nUser: {user_msg}\")\n    print(f\"Assistant: {result.text}\")\n</code></pre>"},{"location":"llcuda/cloud-platforms/#example-3-batch-processing-on-colab","title":"Example 3: Batch Processing on Colab","text":"<pre><code>!pip install llcuda pandas\n\nimport llcuda\nimport pandas as pd\n\n# Load model\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=26, ctx_size=1024)\n\n# Prepare batch data\nprompts = [\n    \"Summarize: Machine learning is a subset of AI...\",\n    \"Classify sentiment: This movie was terrible!\",\n    \"Extract keywords: Python is a programming language...\",\n    \"Translate to French: Hello, how are you?\",\n    \"Answer: What is 2+2?\"\n]\n\n# Batch inference\nresults = engine.batch_infer(prompts, max_tokens=50)\n\n# Create DataFrame\ndata = {\n    'Prompt': prompts,\n    'Response': [r.text for r in results],\n    'Tokens': [r.tokens_generated for r in results],\n    'Speed (tok/s)': [r.tokens_per_sec for r in results]\n}\n\ndf = pd.DataFrame(data)\nprint(df.to_string(index=False))\n\n# Save results\ndf.to_csv('inference_results.csv', index=False)\nprint(\"\\n\u2713 Results saved to inference_results.csv\")\n</code></pre>"},{"location":"llcuda/cloud-platforms/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llcuda/cloud-platforms/#issue-1-no-kernel-image-available-for-execution","title":"Issue 1: \"No kernel image available for execution\"","text":"<p>Cause: Binary compiled for wrong GPU architecture</p> <p>Solution: Upgrade to llcuda 1.1.0+ <pre><code>!pip install --upgrade llcuda\n</code></pre></p> <p>llcuda 1.1.0+ includes binaries for compute capability 5.0-8.9.</p>"},{"location":"llcuda/cloud-platforms/#issue-2-out-of-memory-oom","title":"Issue 2: Out of Memory (OOM)","text":"<p>Symptoms: <pre><code>CUDA error: out of memory\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Reduce GPU layers: <pre><code>engine.load_model(\"model.gguf\", gpu_layers=10)  # Instead of 99\n</code></pre></p> </li> <li> <p>Reduce context size: <pre><code>engine.load_model(\"model.gguf\", ctx_size=1024)  # Instead of 2048\n</code></pre></p> </li> <li> <p>Use smaller model: <pre><code># Use 1B instead of 3B/7B\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre></p> </li> <li> <p>Check VRAM usage: <pre><code>!nvidia-smi\n</code></pre></p> </li> </ol>"},{"location":"llcuda/cloud-platforms/#issue-3-model-download-fails","title":"Issue 3: Model Download Fails","text":"<p>Error: <pre><code>Repository not found\n</code></pre></p> <p>Solution: Use correct HuggingFace repo format: <pre><code># \u2717 Wrong\nengine.load_model(\"google/gemma-3-1b-it-GGUF\")\n\n# \u2713 Correct\nengine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n</code></pre></p>"},{"location":"llcuda/cloud-platforms/#issue-4-slow-inference","title":"Issue 4: Slow Inference","text":"<p>Symptoms: &lt; 5 tokens/second</p> <p>Solutions:</p> <ol> <li> <p>Check GPU is actually being used: <pre><code>compat = llcuda.check_gpu_compatibility()\nprint(f\"GPU: {compat['gpu_name']}\")  # Should show GPU name, not \"None\"\n</code></pre></p> </li> <li> <p>Verify GPU layers: <pre><code># Make sure gpu_layers &gt; 0\nengine.load_model(\"model.gguf\", gpu_layers=20, verbose=True)\n# Should print: \"GPU Layers: 20\"\n</code></pre></p> </li> <li> <p>Use auto-configuration: <pre><code>engine.load_model(\"model.gguf\", auto_configure=True)\n</code></pre></p> </li> </ol>"},{"location":"llcuda/cloud-platforms/#issue-5-server-wont-start","title":"Issue 5: Server Won't Start","text":"<p>Error: <pre><code>RuntimeError: Failed to start llama-server\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Check if port is already in use: <pre><code># Use different port\nengine = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n</code></pre></p> </li> <li> <p>Skip GPU check if needed: <pre><code># Only use if you know your GPU is compatible\nengine.load_model(\"model.gguf\", skip_gpu_check=True)\n</code></pre></p> </li> <li> <p>Check server logs: <pre><code>import subprocess\nresult = subprocess.run(['which', 'llama-server'], capture_output=True)\nprint(result.stdout.decode())\n</code></pre></p> </li> </ol>"},{"location":"llcuda/cloud-platforms/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"llcuda/cloud-platforms/#tesla-t4-15gb-vram-colabkaggle","title":"Tesla T4 (15GB VRAM) - Colab/Kaggle","text":"Model Quantization GPU Layers tok/s VRAM Usage Gemma 3 1B Q4_K_M 26 (all) ~15 ~1.2 GB Gemma 3 3B Q4_K_M 28 (all) ~10 ~3.5 GB Llama 3.2 3B Q4_K_M 28 (all) ~10 ~3.5 GB Llama 3.1 7B Q4_K_M 20 ~5 ~8 GB Llama 3.1 7B Q4_K_M 32 (all) ~8 ~12 GB"},{"location":"llcuda/cloud-platforms/#tesla-v100-16gb-vram-colab-pro","title":"Tesla V100 (16GB VRAM) - Colab Pro","text":"Model Quantization GPU Layers tok/s VRAM Usage Gemma 3 1B Q4_K_M 26 (all) ~20 ~1.2 GB Llama 3.1 7B Q4_K_M 32 (all) ~12 ~12 GB Llama 3.1 13B Q4_K_M 20 ~5 ~14 GB"},{"location":"llcuda/cloud-platforms/#best-practices","title":"Best Practices","text":""},{"location":"llcuda/cloud-platforms/#1-always-check-gpu-compatibility-first","title":"1. Always Check GPU Compatibility First","text":"<pre><code>import llcuda\n\ncompat = llcuda.check_gpu_compatibility()\nif not compat['compatible']:\n    print(f\"Error: {compat['reason']}\")\n    # Fall back to CPU or different model\nelse:\n    print(f\"\u2713 Compatible: {compat['gpu_name']}\")\n</code></pre>"},{"location":"llcuda/cloud-platforms/#2-use-auto-configuration","title":"2. Use Auto-Configuration","text":"<pre><code># Let llcuda optimize settings for your hardware\nengine.load_model(\"model.gguf\", auto_configure=True)\n</code></pre>"},{"location":"llcuda/cloud-platforms/#3-monitor-vram-usage","title":"3. Monitor VRAM Usage","text":"<pre><code># Check VRAM before loading large models\n!nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits\n</code></pre>"},{"location":"llcuda/cloud-platforms/#4-use-context-managers-for-cleanup","title":"4. Use Context Managers for Cleanup","text":"<pre><code>with llcuda.InferenceEngine() as engine:\n    engine.load_model(\"model.gguf\", auto_start=True)\n    result = engine.infer(\"Hello!\")\n    print(result.text)\n# Server automatically stopped\n</code></pre>"},{"location":"llcuda/cloud-platforms/#5-start-with-small-models","title":"5. Start with Small Models","text":"<pre><code># Test with 1B model first\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Then try larger models\n# engine.load_model(\"llama-3.1-8b-Q4_K_M\")\n</code></pre>"},{"location":"llcuda/cloud-platforms/#additional-resources","title":"Additional Resources","text":"<ul> <li>llcuda GitHub: https://github.com/waqasm86/llcuda</li> <li>llama.cpp: https://github.com/ggerganov/llama.cpp</li> <li>GGUF Models: https://huggingface.co/models?library=gguf</li> <li>Documentation: https://waqasm86.github.io/</li> </ul>"},{"location":"llcuda/cloud-platforms/#version-information","title":"Version Information","text":"<ul> <li>llcuda: 1.1.0+</li> <li>Required: Python 3.11+</li> <li>Supported Compute Capability: 5.0 - 8.9</li> <li>Platforms: JupyterLab, Google Colab, Kaggle, Local</li> </ul>"},{"location":"llcuda/cloud-platforms/#license","title":"License","text":"<p>MIT License - Free for commercial and personal use.</p>"},{"location":"llcuda/examples/","title":"Code Examples","text":"<p>Production-ready code samples for llcuda v1.1.0. All examples tested on GeForce 940M (1GB VRAM) and Tesla T4 (Colab/Kaggle).</p> <p>Note: On first import, llcuda will auto-download binaries and models (one-time setup taking 3-5 minutes).</p>"},{"location":"llcuda/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"llcuda/examples/#hello-world","title":"Hello World","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nresult = engine.infer(\"What is Python?\")\nprint(result.text)\n</code></pre>"},{"location":"llcuda/examples/#interactive-chat","title":"Interactive Chat","text":""},{"location":"llcuda/examples/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nconversation = [\n    \"What is machine learning?\",\n    \"How does it differ from AI?\",\n    \"Give me a practical example\"\n]\n\nfor prompt in conversation:\n    result = engine.infer(prompt, max_tokens=100)\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"llcuda/examples/#jupyterlab-integration","title":"JupyterLab Integration","text":""},{"location":"llcuda/examples/#data-analysis-with-llm","title":"Data Analysis with LLM","text":"<pre><code>import pandas as pd\nimport llcuda\n\n# Load data\ndf = pd.read_csv(\"sales_data.csv\")\n\n# Create engine\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Get data summary\nsummary = df.describe().to_string()\n\n# Analyze with LLM\nanalysis = engine.infer(\n    f\"Analyze this sales data and provide insights:\\n{summary}\",\n    max_tokens=200\n)\n\nprint(analysis.text)\n</code></pre>"},{"location":"llcuda/examples/#batch-processing","title":"Batch Processing","text":""},{"location":"llcuda/examples/#process-multiple-prompts","title":"Process Multiple Prompts","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompts = [\n    \"Explain neural networks\",\n    \"What is deep learning?\",\n    \"Describe NLP\"\n]\n\n# Batch inference (more efficient)\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre>"},{"location":"llcuda/examples/#code-generation","title":"Code Generation","text":""},{"location":"llcuda/examples/#generate-and-review-code","title":"Generate and Review Code","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Generate code\ncode_prompt = \"Write a Python function to calculate Fibonacci numbers\"\ncode_result = engine.infer(code_prompt, max_tokens=150)\nprint(\"Generated Code:\")\nprint(code_result.text)\n\n# Review code\nreview_prompt = f\"Review this code for improvements:\\n{code_result.text}\"\nreview_result = engine.infer(review_prompt, max_tokens=150)\nprint(\"\\nCode Review:\")\nprint(review_result.text)\n</code></pre>"},{"location":"llcuda/examples/#temperature-comparison","title":"Temperature Comparison","text":""},{"location":"llcuda/examples/#experiment-with-different-temperatures","title":"Experiment with Different Temperatures","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompt = \"Write a haiku about AI\"\ntemperatures = [0.3, 0.7, 1.2]\n\nfor temp in temperatures:\n    result = engine.infer(prompt, temperature=temp, max_tokens=50)\n    print(f\"\\nTemperature {temp}:\")\n    print(result.text)\n</code></pre>"},{"location":"llcuda/examples/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"llcuda/examples/#track-latency-and-throughput","title":"Track Latency and Throughput","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run some inferences\nfor i in range(20):\n    engine.infer(\"Hello, how are you?\", max_tokens=20)\n\n# Get performance metrics\nmetrics = engine.get_metrics()\n\nprint(\"Latency Statistics:\")\nprint(f\"  Mean: {metrics['latency']['mean_ms']:.2f} ms\")\nprint(f\"  p50:  {metrics['latency']['p50_ms']:.2f} ms\")\nprint(f\"  p95:  {metrics['latency']['p95_ms']:.2f} ms\")\nprint(f\"  p99:  {metrics['latency']['p99_ms']:.2f} ms\")\n\nprint(\"\\nThroughput:\")\nprint(f\"  Total Tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.2f}\")\n</code></pre>"},{"location":"llcuda/examples/#error-handling","title":"Error Handling","text":""},{"location":"llcuda/examples/#robust-production-code","title":"Robust Production Code","text":"<pre><code>import llcuda\n\ntry:\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n    result = engine.infer(\"What is AI?\", max_tokens=100)\n\n    if result.success:\n        print(result.text)\n    else:\n        print(f\"Inference failed: {result.error_message}\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    if 'engine' in locals():\n        engine.unload_model()\n</code></pre>"},{"location":"llcuda/examples/#context-manager-pattern","title":"Context Manager Pattern","text":""},{"location":"llcuda/examples/#automatic-resource-cleanup","title":"Automatic Resource Cleanup","text":"<pre><code>import llcuda\n\n# Use context manager for automatic cleanup\nwith llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n    result = engine.infer(\"Explain quantum computing\", max_tokens=100)\n    print(result.text)\n\n# Engine automatically cleaned up here\n</code></pre>"},{"location":"llcuda/examples/#using-local-gguf-files","title":"Using Local GGUF Files","text":""},{"location":"llcuda/examples/#load-custom-models","title":"Load Custom Models","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Find local GGUF models\nmodels = llcuda.find_gguf_models()\n\nif models:\n    # Use first model found\n    engine.load_model(str(models[0]))\nelse:\n    # Fall back to registry\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nresult = engine.infer(\"Hello!\", max_tokens=20)\nprint(result.text)\n</code></pre>"},{"location":"llcuda/examples/#production-pattern-api-wrapper","title":"Production Pattern: API Wrapper","text":""},{"location":"llcuda/examples/#build-a-simple-api","title":"Build a Simple API","text":"<pre><code>import llcuda\nfrom typing import Dict\n\nclass LLMService:\n    def __init__(self, model_name: str = \"gemma-3-1b-Q4_K_M\"):\n        self.engine = llcuda.InferenceEngine()\n        self.engine.load_model(model_name)\n\n    def generate(self, prompt: str, max_tokens: int = 100) -&gt; Dict:\n        result = self.engine.infer(prompt, max_tokens=max_tokens)\n\n        return {\n            \"text\": result.text,\n            \"tokens\": result.tokens_generated,\n            \"speed\": result.tokens_per_sec,\n            \"latency_ms\": result.latency_ms\n        }\n\n    def batch_generate(self, prompts: list, max_tokens: int = 100) -&gt; list:\n        results = self.engine.batch_infer(prompts, max_tokens=max_tokens)\n        return [\n            {\n                \"text\": r.text,\n                \"tokens\": r.tokens_generated,\n                \"speed\": r.tokens_per_sec\n            }\n            for r in results\n        ]\n\n    def get_stats(self) -&gt; Dict:\n        return self.engine.get_metrics()\n\n    def cleanup(self):\n        self.engine.unload_model()\n\n# Usage\nservice = LLMService()\nresponse = service.generate(\"What is AI?\")\nprint(response)\n</code></pre>"},{"location":"llcuda/examples/#complete-jupyterlab-example","title":"Complete JupyterLab Example","text":"<p>See the full JupyterLab notebook with:</p> <ul> <li>System info checks</li> <li>Model registry listing</li> <li>Batch inference</li> <li>Performance visualization</li> <li>Context manager usage</li> <li>Temperature comparisons</li> </ul>"},{"location":"llcuda/examples/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Getting started guide</li> <li>Performance - Optimization tips</li> <li>GitHub - Source code and more examples</li> </ul>"},{"location":"llcuda/installation/","title":"Installation Guide","text":"<p>Complete installation instructions for llcuda v1.1.0.</p>"},{"location":"llcuda/installation/#quick-install","title":"Quick Install","text":"<pre><code># Install or upgrade to latest version\npip install --upgrade llcuda\n\n# Or install specific version\npip install llcuda==1.1.0\n</code></pre> <p>First-time setup: On first import, llcuda will automatically download optimized binaries (~700 MB) and a default model (~770 MB) based on your GPU. This is a one-time download that takes 3-5 minutes depending on your internet connection.</p> <pre><code>import llcuda  # First import triggers automatic setup\n# \ud83c\udfaf llcuda First-Time Setup\n# \ud83c\udfae GPU Detected: [Your GPU] (Compute X.X)\n# \ud83d\udce5 Downloading binaries...\n# \ud83d\udce5 Downloading model...\n# \u2705 Setup Complete!\n</code></pre>"},{"location":"llcuda/installation/#system-requirements","title":"System Requirements","text":""},{"location":"llcuda/installation/#operating-system","title":"Operating System","text":"<ul> <li>Supported: Ubuntu 22.04 LTS (tested)</li> <li>Likely works: Ubuntu 20.04+, Debian 11+, other Linux distros</li> </ul>"},{"location":"llcuda/installation/#hardware","title":"Hardware","text":"<ul> <li>GPU: NVIDIA with compute capability 5.0+ (Maxwell architecture or later)</li> <li>VRAM: 1GB minimum (for 1B-2B models)</li> <li>CPU: Any modern x86_64 processor</li> <li>RAM: 4GB+ recommended</li> </ul>"},{"location":"llcuda/installation/#software","title":"Software","text":"<ul> <li>Python: 3.11 or later</li> <li>CUDA: Not required (bundled in package)</li> <li>GPU Drivers: NVIDIA drivers 535+ recommended</li> </ul>"},{"location":"llcuda/installation/#supported-gpus","title":"Supported GPUs","text":""},{"location":"llcuda/installation/#tested","title":"Tested","text":"<ul> <li>GeForce 940M (1GB VRAM) - Primary test platform</li> </ul>"},{"location":"llcuda/installation/#should-work-compute-capability-50","title":"Should Work (compute capability 5.0+)","text":"<ul> <li>GeForce 900 series: 940M, 950M, 960M, 970M, 980M</li> <li>GeForce 800 series: 840M, 850M, 860M</li> <li>GeForce GTX series: 750, 750 Ti, 950, 960, 970, 980 and newer</li> <li>GeForce RTX series: All models</li> <li>Quadro/Tesla: Maxwell generation and later</li> </ul>"},{"location":"llcuda/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"llcuda/installation/#1-check-gpu","title":"1. Check GPU","text":"<pre><code>nvidia-smi\n</code></pre> <p>You should see your GPU listed. If not, install NVIDIA drivers first.</p>"},{"location":"llcuda/installation/#2-install-llcuda","title":"2. Install llcuda","text":"<pre><code># Install latest version\npip install --upgrade llcuda\n</code></pre>"},{"location":"llcuda/installation/#3-verify-installation","title":"3. Verify Installation","text":"<pre><code>import llcuda\n\nllcuda.print_system_info()\n</code></pre> <p>You should see your GPU detected and llama-server auto-configured.</p>"},{"location":"llcuda/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llcuda/installation/#gpu-not-detected","title":"GPU Not Detected","text":"<p>Problem: <code>llcuda.check_cuda_available()</code> returns <code>False</code></p> <p>Solutions: 1. Check NVIDIA drivers: <code>nvidia-smi</code> 2. Reinstall drivers if needed 3. Verify compute capability \u2265 5.0</p>"},{"location":"llcuda/installation/#import-error","title":"Import Error","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'llcuda'</code></p> <p>Solution: <pre><code>pip install --upgrade llcuda\n</code></pre></p>"},{"location":"llcuda/installation/#model-download-fails","title":"Model Download Fails","text":"<p>Problem: HuggingFace download timeout or fails</p> <p>Solutions: 1. Check internet connection 2. Try again (HuggingFace may be temporarily down) 3. Use local GGUF file instead:    <pre><code>engine.load_model(\"/path/to/model.gguf\")\n</code></pre></p>"},{"location":"llcuda/installation/#out-of-memory","title":"Out of Memory","text":"<p>Problem: CUDA out of memory error</p> <p>Solutions: 1. Use smaller model (gemma-3-1b-Q4_K_M for 1GB VRAM) 2. Reduce GPU layers:    <pre><code>engine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=10)\n</code></pre> 3. Close other GPU applications</p>"},{"location":"llcuda/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import llcuda\n\n# System check\nllcuda.print_system_info()\n\n# Quick inference test\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\nresult = engine.infer(\"Hello!\", max_tokens=20)\nprint(result.text)\nprint(f\"\u2713 llcuda working! Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"llcuda/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Basic usage examples</li> <li>Performance - Benchmarks and optimization</li> <li>Examples - Production code samples</li> </ul>"},{"location":"llcuda/performance/","title":"Performance Benchmarks","text":"<p>Real-world benchmarks on actual hardware running llcuda v1.1.0.</p>"},{"location":"llcuda/performance/#cloud-platforms","title":"Cloud Platforms","text":"<p>llcuda v1.1.0 now supports Google Colab and Kaggle!</p>"},{"location":"llcuda/performance/#tesla-t4-google-colab-kaggle-15gb-vram","title":"Tesla T4 (Google Colab / Kaggle) - 15GB VRAM","text":"Model Quantization GPU Layers Speed VRAM Usage Gemma 3 1B Q4_K_M 26 (all) ~15 tok/s ~1.2 GB Llama 3.1 7B Q4_K_M 20 ~5-8 tok/s ~8 GB Llama 3.1 7B Q4_K_M 32 (all) ~8-10 tok/s ~12 GB <p>Platform: Google Colab Free / Kaggle (2x T4, 30GB total VRAM) llcuda: v1.1.0 Use case: Research, education, rapid prototyping</p>"},{"location":"llcuda/performance/#tesla-p100-google-colab-16gb-vram","title":"Tesla P100 (Google Colab) - 16GB VRAM","text":"Model Quantization GPU Layers Speed VRAM Usage Gemma 3 1B Q4_K_M 26 (all) ~18 tok/s ~1.2 GB Llama 3.1 7B Q4_K_M 32 (all) ~10 tok/s ~12 GB Llama 3.1 13B Q4_K_M 20 ~5-7 tok/s ~14 GB <p>Platform: Google Colab (varies by availability) llcuda: v1.1.0 Use case: Larger models, faster inference</p>"},{"location":"llcuda/performance/#local-gpus","title":"Local GPUs","text":""},{"location":"llcuda/performance/#geforce-940m-1gb-vram","title":"GeForce 940M - 1GB VRAM","text":"<p>Hardware: GeForce 940M (384 CUDA cores, Maxwell architecture) OS: Ubuntu 22.04 LTS llcuda: v1.1.0 CUDA: 12.8 (bundled)</p>"},{"location":"llcuda/performance/#gemma-3-1b-q4_k_m-recommended","title":"Gemma 3 1B Q4_K_M (Recommended)","text":"<pre><code>Model: google/gemma-3-1b-it (Q4_K_M quantization)\nSize: 700 MB\nVRAM Usage: ~800 MB\nPerformance: ~15 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\n</code></pre> <p>Use case: General chat, Q&amp;A, code assistance</p>"},{"location":"llcuda/performance/#tinyllama-11b-q5_k_m-fastest","title":"TinyLlama 1.1B Q5_K_M (Fastest)","text":"<pre><code>Model: TinyLlama-1.1B-Chat (Q5_K_M quantization)\nSize: 800 MB\nVRAM Usage: ~750 MB\nPerformance: ~18 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\n</code></pre> <p>Use case: Fast responses, simple tasks</p>"},{"location":"llcuda/performance/#llama-32-1b-q4_k_m-best-quality","title":"Llama 3.2 1B Q4_K_M (Best Quality)","text":"<pre><code>Model: meta-llama/Llama-3.2-1B-Instruct (Q4_K_M)\nSize: 750 MB\nVRAM Usage: ~800 MB\nPerformance: ~16 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\n</code></pre> <p>Use case: Best quality for 1GB VRAM</p>"},{"location":"llcuda/performance/#supported-gpu-architectures","title":"Supported GPU Architectures","text":"<p>llcuda v1.1.0 supports all modern NVIDIA GPUs with compute capability 5.0+:</p> Architecture Compute Cap Examples Platforms Maxwell 5.0-5.3 GTX 900, 940M Local Pascal 6.0-6.2 GTX 10xx, P100 Local, Colab Volta 7.0 V100 Colab Pro Turing 7.5 T4, RTX 20xx Colab, Kaggle Ampere 8.0-8.6 A100, RTX 30xx Colab Pro, Local Ada Lovelace 8.9 RTX 40xx Local"},{"location":"llcuda/performance/#full-model-registry","title":"Full Model Registry","text":"<p>llcuda includes 11 curated models optimized for different VRAM tiers:</p>"},{"location":"llcuda/performance/#1gb-vram-tier-geforce-940m","title":"1GB VRAM Tier (GeForce 940M)","text":"Model Size Performance (940M) Use Case tinyllama-1.1b-Q5_K_M 800 MB ~18 tok/s Fastest option gemma-3-1b-Q4_K_M 700 MB ~15 tok/s Recommended llama-3.2-1b-Q4_K_M 750 MB ~16 tok/s Best quality"},{"location":"llcuda/performance/#2gb-vram-tier","title":"2GB+ VRAM Tier","text":"Model Size Performance (T4) Use Case phi-3-mini-Q4_K_M 2.2 GB ~12 tok/s Code-focused gemma-2-2b-Q4_K_M 1.6 GB ~14 tok/s Balanced llama-3.2-3b-Q4_K_M 2.0 GB ~12 tok/s Quality"},{"location":"llcuda/performance/#4gb-vram-tier","title":"4GB+ VRAM Tier","text":"Model Size Performance (T4) Use Case mistral-7b-Q4_K_M 4.1 GB ~8 tok/s Highest quality llama-3.1-7b-Q4_K_M 4.3 GB ~5-8 tok/s Latest Llama phi-3-medium-Q4_K_M 8.0 GB ~5 tok/s Code expert"},{"location":"llcuda/performance/#8gb-vram-tier","title":"8GB+ VRAM Tier","text":"Model Size Performance (P100) Use Case llama-3.1-13b-Q4_K_M 7.8 GB ~5-7 tok/s Large context mixtral-8x7b-Q4_K_M 26 GB ~3-5 tok/s MoE architecture"},{"location":"llcuda/performance/#latency-metrics","title":"Latency Metrics","text":"<p>llcuda tracks P50/P95/P99 latencies:</p>"},{"location":"llcuda/performance/#geforce-940m-1gb-vram_1","title":"GeForce 940M (1GB VRAM)","text":"<pre><code>Model: gemma-3-1b-Q4_K_M\nGPU Layers: 20\nContext: 512 tokens\n\nP50 latency: 65 ms\nP95 latency: 72 ms\nP99 latency: 78 ms\n</code></pre>"},{"location":"llcuda/performance/#tesla-t4-colabkaggle","title":"Tesla T4 (Colab/Kaggle)","text":"<pre><code>Model: gemma-3-1b-Q4_K_M\nGPU Layers: 26\nContext: 2048 tokens\n\nP50 latency: 62 ms\nP95 latency: 68 ms\nP99 latency: 74 ms\n</code></pre>"},{"location":"llcuda/performance/#testing-methodology","title":"Testing Methodology","text":"<p>All benchmarks measured using:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Warmup\nfor _ in range(5):\n    engine.infer(\"Test\", max_tokens=50)\n\n# Measure\nresults = []\nfor _ in range(100):\n    result = engine.infer(\"Explain AI\", max_tokens=50)\n    results.append(result.tokens_per_sec)\n\nprint(f\"Mean: {sum(results)/len(results):.1f} tok/s\")\n</code></pre>"},{"location":"llcuda/performance/#tips-for-best-performance","title":"Tips for Best Performance","text":""},{"location":"llcuda/performance/#1-gpu-layer-tuning","title":"1. GPU Layer Tuning","text":"<pre><code># Start with auto-configuration\nengine.load_model(\"model.gguf\", auto_configure=True)\n\n# Then manually tune\nengine.load_model(\"model.gguf\", gpu_layers=20)  # Adjust based on VRAM\n</code></pre>"},{"location":"llcuda/performance/#2-context-size","title":"2. Context Size","text":"<pre><code># Smaller context = less VRAM\nengine.load_model(\"model.gguf\", ctx_size=512)   # 1GB VRAM\nengine.load_model(\"model.gguf\", ctx_size=2048)  # 4GB+ VRAM\n</code></pre>"},{"location":"llcuda/performance/#3-batch-size","title":"3. Batch Size","text":"<pre><code># Larger batch = better throughput (if VRAM allows)\nengine.load_model(\"model.gguf\", batch_size=512)\n</code></pre>"},{"location":"llcuda/performance/#4-quantization-selection","title":"4. Quantization Selection","text":"<p>Lower quantization = smaller model = faster loading, but slightly lower quality:</p> <ul> <li>Q4_K_M: Best balance (recommended)</li> <li>Q5_K_M: Higher quality, larger size</li> <li>Q6_K: Highest quality, biggest size</li> <li>Q3_K_M: Smallest, fastest, lower quality</li> </ul>"},{"location":"llcuda/performance/#cloud-platform-quick-start","title":"Cloud Platform Quick Start","text":""},{"location":"llcuda/performance/#google-colab","title":"Google Colab","text":"<pre><code>!pip install llcuda\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=26)\nresult = engine.infer(\"What is AI?\")\nprint(result.text)\n</code></pre>"},{"location":"llcuda/performance/#kaggle","title":"Kaggle","text":"<pre><code>!pip install llcuda\n\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\n    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n    gpu_layers=26\n)\nresult = engine.infer(\"Explain machine learning\")\nprint(result.text)\n</code></pre> <p>View complete cloud platform guide \u2192</p>"},{"location":"llcuda/performance/#comparison-v10x-vs-v110","title":"Comparison: v1.0.x vs v1.1.0","text":"Metric v1.0.x v1.1.0 Supported GPUs Compute 5.0 only Compute 5.0-8.9 (8 architectures) Cloud Platforms \u274c None \u2705 Colab, Kaggle PyPI Package Size 50 MB 51 KB (hybrid architecture) First Install Quick +3-5 min (downloads binaries/model) Subsequent Use ~1 sec ~1 sec (cached) Performance (940M) ~15 tok/s ~15 tok/s (same) T4 Support \u274c Failed \u2705 Works (~15 tok/s) P100 Support \u274c Failed \u2705 Works (~18 tok/s) <p>Hybrid Bootstrap Architecture: v1.1.0 uses a lightweight PyPI package (51 KB) that automatically downloads optimized binaries (~700 MB) and models (~770 MB) on first import based on your GPU. This solves PyPI's 100 MB file size limit while providing universal GPU support.</p> <p>Backward compatible: v1.1.0 works exactly the same on GeForce 940M as v1.0.x.</p>"},{"location":"llcuda/performance/#links","title":"Links","text":"<ul> <li>PyPI: pypi.org/project/llcuda</li> <li>GitHub: github.com/waqasm86/llcuda</li> <li>Latest Release: v1.1.0</li> <li>Cloud Guide: Cloud Platforms</li> </ul>"},{"location":"llcuda/quickstart/","title":"Quick Start Guide","text":"<p>Get llcuda v1.1.0 running in under 10 minutes.</p>"},{"location":"llcuda/quickstart/#installation-5-10-minutes-for-first-time-setup","title":"Installation (5-10 minutes for first-time setup)","text":"<pre><code># Install or upgrade to latest version\npip install --upgrade llcuda\n\n# Or install specific version\npip install llcuda==1.1.0\n</code></pre> <p>First-time Setup: When you first import llcuda, it will automatically download: - Binaries (~700 MB): Optimized for your GPU from GitHub Releases - Model (~770 MB): Default Gemma 3 1B from Hugging Face - Total: ~1.5 GB one-time download (3-5 minutes) - Subsequent uses: Instant (files cached locally)</p> <p>Why the download? llcuda v1.1.0 uses a hybrid bootstrap architecture: - PyPI package: Only 51 KB (Python code only) - Supports 8 GPU architectures (SM 5.0-8.9) - Works on Colab, Kaggle, and local GPUs</p>"},{"location":"llcuda/quickstart/#basic-usage-2-minutes","title":"Basic Usage (2 minutes)","text":"<pre><code>import llcuda\n# \ud83c\udfaf First import triggers automatic setup (one-time):\n#    - Detecting GPU: GeForce 940M (Compute 5.0)\n#    - Downloading binaries from GitHub...\n#    - Downloading model from Hugging Face...\n#    - \u2705 Setup Complete!\n\n# Create inference engine (auto-detects GPU)\nengine = llcuda.InferenceEngine()\n\n# Load model (already downloaded during first import)\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run inference\nresult = engine.infer(\"What is artificial intelligence?\", max_tokens=100)\n\n# Display results\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>Output: <pre><code>Artificial intelligence (AI) is the simulation of human intelligence\nin machines that are programmed to think and learn like humans...\n\nSpeed: 15.2 tok/s\n</code></pre></p>"},{"location":"llcuda/quickstart/#list-available-models","title":"List Available Models","text":"<p>llcuda v1.1.0 includes 11 curated models in the registry:</p> <pre><code>from llcuda.models import list_registry_models\n\nmodels = list_registry_models()\n\nfor name, info in models.items():\n    print(f\"{name}: {info['description']}\")\n    print(f\"  Size: {info['size_mb']} MB, Min VRAM: {info['min_vram_gb']} GB\\n\")\n</code></pre> <p>Output: <pre><code>tinyllama-1.1b-Q5_K_M: TinyLlama 1.1B Chat (fastest option)\n  Size: 800 MB, Min VRAM: 1 GB\n\ngemma-3-1b-Q4_K_M: Google Gemma 3 1B (recommended for 1GB VRAM)\n  Size: 700 MB, Min VRAM: 1 GB\n\nllama-3.2-1b-Q4_K_M: Meta Llama 3.2 1B Instruct\n  Size: 750 MB, Min VRAM: 1 GB\n\n...\n</code></pre></p>"},{"location":"llcuda/quickstart/#check-system-info","title":"Check System Info","text":"<pre><code>import llcuda\n\n# Print comprehensive system information\nllcuda.print_system_info()\n</code></pre> <p>Output: <pre><code>=== llcuda System Information ===\nllcuda version: 1.0.1\nPython version: 3.11.0\n\n=== CUDA Information ===\nCUDA Available: Yes\nCUDA Version: 12.8\n\nGPU 0: GeForce 940M\n  Memory: 1024 MB\n  Driver: 535.183.01\n  Compute Capability: 5.0\n\n=== llama-server ===\nPath: /home/user/.local/lib/python3.11/site-packages/llcuda/bin/llama-server\nStatus: Auto-configured (bundled)\n</code></pre></p>"},{"location":"llcuda/quickstart/#interactive-conversation","title":"Interactive Conversation","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Multi-turn conversation\nprompts = [\n    \"What is machine learning?\",\n    \"How does it differ from traditional programming?\",\n    \"Give me a practical example\"\n]\n\nfor prompt in prompts:\n    result = engine.infer(prompt, max_tokens=80)\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"llcuda/quickstart/#batch-inference","title":"Batch Inference","text":"<p>Process multiple prompts efficiently:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompts = [\n    \"Explain neural networks\",\n    \"What is deep learning?\",\n    \"Describe natural language processing\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=50)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre>"},{"location":"llcuda/quickstart/#performance-metrics","title":"Performance Metrics","text":"<p>Get detailed P50/P95/P99 latency statistics:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run some inferences\nfor i in range(10):\n    engine.infer(\"Hello, how are you?\", max_tokens=20)\n\n# Get metrics\nmetrics = engine.get_metrics()\n\nprint(\"Latency Statistics:\")\nprint(f\"  Mean: {metrics['latency']['mean_ms']:.2f} ms\")\nprint(f\"  p50:  {metrics['latency']['p50_ms']:.2f} ms\")\nprint(f\"  p95:  {metrics['latency']['p95_ms']:.2f} ms\")\nprint(f\"  p99:  {metrics['latency']['p99_ms']:.2f} ms\")\n\nprint(\"\\nThroughput:\")\nprint(f\"  Total Tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.2f}\")\n</code></pre>"},{"location":"llcuda/quickstart/#using-local-gguf-files","title":"Using Local GGUF Files","text":"<p>You can also use local GGUF model files:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Find local GGUF models\nmodels = llcuda.find_gguf_models()\n\nif models:\n    print(f\"Found {len(models)} local GGUF models\")\n    # Use first model found\n    engine.load_model(str(models[0]))\nelse:\n    # Fall back to registry\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre>"},{"location":"llcuda/quickstart/#context-manager-usage","title":"Context Manager Usage","text":"<p>Use llcuda with Python context managers for automatic cleanup:</p> <pre><code>import llcuda\n\n# Context manager handles cleanup automatically\nwith llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n    result = engine.infer(\"Explain quantum computing\", max_tokens=80)\n    print(result.text)\n\n# Engine automatically cleaned up after context exit\nprint(\"Resources cleaned up\")\n</code></pre>"},{"location":"llcuda/quickstart/#temperature-comparison","title":"Temperature Comparison","text":"<p>Compare outputs with different temperature settings:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompt = \"Write a creative opening for a science fiction story\"\ntemperatures = [0.3, 0.7, 1.2]\n\nfor temp in temperatures:\n    result = engine.infer(prompt, temperature=temp, max_tokens=60)\n    print(f\"\\nTemperature {temp}:\")\n    print(result.text)\n</code></pre>"},{"location":"llcuda/quickstart/#jupyterlab-integration","title":"JupyterLab Integration","text":"<p>llcuda works seamlessly in Jupyter notebooks:</p> <pre><code>import llcuda\nimport pandas as pd\n\n# Create engine\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Analyze data with LLM\ndf = pd.read_csv(\"data.csv\")\nsummary = df.describe().to_string()\n\nanalysis = engine.infer(f\"Analyze this data:\\n{summary}\", max_tokens=150)\nprint(analysis.text)\n</code></pre> <p>See the complete JupyterLab example notebook.</p>"},{"location":"llcuda/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed setup and troubleshooting</li> <li>Performance Guide - Benchmarks and optimization tips</li> <li>Examples - Production-ready code samples</li> <li>GitHub - Source code and issues</li> </ul>"},{"location":"llcuda/quickstart/#common-questions","title":"Common Questions","text":""},{"location":"llcuda/quickstart/#which-model-should-i-use","title":"Which model should I use?","text":"<p>For 1GB VRAM: <code>gemma-3-1b-Q4_K_M</code> (recommended) or <code>tinyllama-1.1b-Q5_K_M</code> (faster)</p> <p>For 2GB+ VRAM: <code>phi-3-mini-Q4_K_M</code> (best for code) or <code>llama-3.2-3b-Q4_K_M</code></p>"},{"location":"llcuda/quickstart/#how-do-i-change-gpu-layers","title":"How do I change GPU layers?","text":"<p>llcuda auto-configures based on your VRAM, but you can override:</p> <pre><code>engine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=30)  # More GPU offloading\n</code></pre>"},{"location":"llcuda/quickstart/#can-i-use-my-own-gguf-models","title":"Can I use my own GGUF models?","text":"<p>Yes, either use local files:</p> <pre><code>engine.load_model(\"/path/to/model.gguf\")\n</code></pre> <p>Or HuggingFace models:</p> <pre><code>engine.load_model(\"author/repo-name\", model_filename=\"model.gguf\")\n</code></pre>"},{"location":"llcuda/quickstart/#how-do-i-unload-a-model","title":"How do I unload a model?","text":"<pre><code>engine.unload_model()  # Stops server and frees resources\n</code></pre>"},{"location":"resume/","title":"Resume","text":"<p>Place your resume PDF file here as <code>Muhammad_Waqas_Resume_2025.pdf</code></p> <p>The mkdocs.yml navigation is configured to link to: - <code>resume/Muhammad_Waqas_Resume_2025.pdf</code></p> <p>Make sure to add your actual resume PDF to this directory.</p>"}]}