{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Mohammad Waqas (waqasm86)","text":"<p>Welcome to my personal engineering hub.</p> <p>Here I publish CUDA + C++ + LLM inference work, benchmarks, and architecture notes.</p>"},{"location":"#featured","title":"Featured","text":"<ul> <li>CUDA networking experiments for llama.cpp</li> <li>Multi-rank scheduling / throughput tests</li> <li>Storage pipeline experiments for LLM workloads</li> </ul>"},{"location":"about/","title":"About","text":"<p>I build GPU-focused C++ systems around LLM inference (llama.cpp), networking, and performance testing.</p> <ul> <li>GitHub: https://github.com/waqasm86</li> </ul>"},{"location":"projects/","title":"CUDA Projects","text":"<p>This section contains documentation, design notes, build steps, and results for my CUDA repositories.</p>"},{"location":"projects/cuda-llm-storage-pipeline/","title":"cuda-llm-storage-pipeline","text":"<p>A high-performance storage layer and pipeline orchestrator demonstrating datacenter-scale systems thinking for LLM inference infrastructure.</p> <p>:fontawesome-brands-github: View on GitHub{ .md-button }</p>"},{"location":"projects/cuda-llm-storage-pipeline/#overview","title":"Overview","text":"<p>This C++20 project showcases NVIDIA-scale infrastructure design for large language model inference by building a production-grade storage layer and pipeline orchestrator. It demonstrates how modern LLM systems require sophisticated artifact distribution, performance optimization, and end-to-end observability beyond basic model execution.</p> <p>Core Philosophy:</p> <p>Modern LLM inference at datacenter scale demands more than just running models\u2014it requires robust artifact management, content-addressed storage, integrity verification, and comprehensive performance monitoring.</p>"},{"location":"projects/cuda-llm-storage-pipeline/#architecture","title":"Architecture","text":"<p>The system separates concerns into two distinct planes:</p>"},{"location":"projects/cuda-llm-storage-pipeline/#data-plane","title":"Data Plane","text":"<p>Manages the actual bytes flowing through the system:</p> <ul> <li>GGUF Model Files: Quantized LLM weights and configurations</li> <li>Prompt Batches: Input data for inference workloads</li> <li>Inference Outputs: Generated completions and embeddings</li> <li>Storage Backend: SeaweedFS distributed object storage</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#control-plane","title":"Control Plane","text":"<p>Handles routing, metadata, and orchestration:</p> <ul> <li>Content Addressing: SHA256 hashing for immutable artifacts</li> <li>Manifest Sidecars: Provenance, size, timestamps, and integrity metadata</li> <li>Immutable Run Folders: Timestamped execution tracking with metrics</li> <li>Pipeline Orchestration: Stage coordination and dependency management</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#key-features","title":"Key Features","text":""},{"location":"projects/cuda-llm-storage-pipeline/#artifact-management","title":"Artifact Management","text":"<ul> <li>Content-Addressed Storage: SHA256-based immutable artifact addressing</li> <li>Manifest Files: Comprehensive metadata for each artifact (provenance, size, timestamps)</li> <li>Version Control: Immutable artifacts with cryptographic verification</li> <li>Local Caching: Avoid redundant downloads with hash-based cache validation</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#performance-reliability","title":"Performance &amp; Reliability","text":"<ul> <li>Failure-Aware Uploads: Checksum verification on upload completion</li> <li>Retry Logic: Automatic retry with exponential backoff</li> <li>Performance Benchmarking: p50/p95/p99 latency percentiles</li> <li>Stage-by-Stage Breakdown: Detailed latency analysis per pipeline stage</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#observability","title":"Observability","text":"<ul> <li>Comprehensive Logging: Structured logs for all operations</li> <li>Metric Collection: Latency, throughput, error rates</li> <li>Run Tracking: Immutable folders capturing full execution history</li> <li>Audit Trail: Content hashes provide tamper-proof artifact lineage</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#project-structure","title":"Project Structure","text":""},{"location":"projects/cuda-llm-storage-pipeline/#five-core-applications","title":"Five Core Applications","text":"<pre><code>cuda-llm-storage-pipeline/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 slp_put_model.cpp       # Upload GGUF models to storage\n\u2502   \u251c\u2500\u2500 slp_get_model.cpp       # Retrieve models by hash\n\u2502   \u251c\u2500\u2500 slp_put_prompts.cpp     # Upload prompt batches\n\u2502   \u251c\u2500\u2500 slp_run_infer.cpp       # Orchestrate inference pipeline\n\u2502   \u2514\u2500\u2500 slp_bench_storage.cpp   # Storage performance benchmarks\n\u251c\u2500\u2500 include/slp/\n\u2502   \u251c\u2500\u2500 storage_client.hpp      # SeaweedFS client abstraction\n\u2502   \u251c\u2500\u2500 manifest.hpp            # Metadata structures\n\u2502   \u2514\u2500\u2500 pipeline.hpp            # Pipeline orchestration\n\u251c\u2500\u2500 CMakeLists.txt\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#application-responsibilities","title":"Application Responsibilities","text":"<ol> <li>slp_put_model: Upload GGUF files with integrity verification</li> <li>slp_get_model: Content-addressed retrieval with local caching</li> <li>slp_put_prompts: Batch prompt uploads with manifest generation</li> <li>slp_run_infer: End-to-end pipeline orchestration</li> <li>slp_bench_storage: Performance measurement and analysis</li> </ol>"},{"location":"projects/cuda-llm-storage-pipeline/#dependencies","title":"Dependencies","text":"<ul> <li>C++20 Compiler: GCC 11+ or Clang 14+</li> <li>CMake: 3.22+</li> <li>libcurl: HTTP client for storage operations</li> <li>SeaweedFS: Distributed object storage backend</li> <li>nlohmann/json: JSON parsing (optional, for manifests)</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#setup","title":"Setup","text":""},{"location":"projects/cuda-llm-storage-pipeline/#1-install-seaweedfs","title":"1. Install SeaweedFS","text":"<pre><code># Download SeaweedFS\nwget https://github.com/seaweedfs/seaweedfs/releases/download/3.XX/linux_amd64.tar.gz\ntar -xzf linux_amd64.tar.gz\n\n# Start master server\n./weed master -port=9333\n\n# Start volume server\n./weed volume -port=8080 -mserver=localhost:9333\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#2-build-the-project","title":"2. Build the Project","text":"<pre><code># Configure with CMake\ncmake -S . -B build -DCMAKE_BUILD_TYPE=Release -G Ninja\n\n# Compile\ncmake --build build -j\n\n# Install (optional)\ncmake --install build --prefix /usr/local\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#3-verify-seaweedfs","title":"3. Verify SeaweedFS","text":"<pre><code># Check cluster status\ncurl http://localhost:9333/cluster/status\n\n# Test upload\necho \"test\" &gt; test.txt\ncurl -F file=@test.txt http://localhost:8080/submit\n\n# Test retrieval\ncurl http://localhost:8080/&lt;file_id&gt;\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#usage","title":"Usage","text":""},{"location":"projects/cuda-llm-storage-pipeline/#upload-a-model","title":"Upload a Model","text":"<pre><code>./build/slp_put_model \\\n    --model /path/to/gemma-3-1b-Q4_K_M.gguf \\\n    --storage http://localhost:8080\n</code></pre> <p>Output:</p> <pre><code>Uploading model: gemma-3-1b-Q4_K_M.gguf\nSize: 762 MiB\nSHA256: a1b2c3d4...\nUpload complete: 8.42s\nManifest: model-a1b2c3d4.json\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#retrieve-a-model","title":"Retrieve a Model","text":"<pre><code>./build/slp_get_model \\\n    --hash a1b2c3d4... \\\n    --storage http://localhost:8080 \\\n    --output ./models/\n</code></pre> <p>Features: - Checks local cache first - Verifies SHA256 after download - Updates manifest with retrieval metadata</p>"},{"location":"projects/cuda-llm-storage-pipeline/#upload-prompts","title":"Upload Prompts","text":"<pre><code>./build/slp_put_prompts \\\n    --prompts prompts.jsonl \\\n    --storage http://localhost:8080\n</code></pre> <p>Prompt Format (JSONL):</p> <pre><code>{\"prompt\": \"Explain CUDA in one sentence.\", \"max_tokens\": 50}\n{\"prompt\": \"What is quantization?\", \"max_tokens\": 100}\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#run-inference-pipeline","title":"Run Inference Pipeline","text":"<pre><code>./build/slp_run_infer \\\n    --model-hash a1b2c3d4... \\\n    --prompts-hash e5f6g7h8... \\\n    --storage http://localhost:8080 \\\n    --output ./runs/\n</code></pre> <p>Pipeline Stages: 1. Fetch model from storage (with caching) 2. Fetch prompts from storage 3. Load model into inference engine 4. Execute inference batch 5. Upload results to storage 6. Generate performance report</p>"},{"location":"projects/cuda-llm-storage-pipeline/#benchmark-storage","title":"Benchmark Storage","text":"<pre><code>./build/slp_bench_storage \\\n    --storage http://localhost:8080 \\\n    --size 100MB \\\n    --iterations 100\n</code></pre> <p>Metrics: - Upload latency (p50, p95, p99) - Download latency (p50, p95, p99) - Throughput (MB/s) - Error rates</p>"},{"location":"projects/cuda-llm-storage-pipeline/#performance","title":"Performance","text":""},{"location":"projects/cuda-llm-storage-pipeline/#storage-benchmarks","title":"Storage Benchmarks","text":"<p>Typical performance on local SeaweedFS:</p> Operation p50 p95 p99 Throughput Upload (100MB) 245ms 380ms 520ms 410 MB/s Download (100MB) 180ms 290ms 410ms 560 MB/s Upload (1GB) 2.4s 3.8s 5.1s 420 MB/s Download (1GB) 1.8s 2.9s 4.2s 550 MB/s"},{"location":"projects/cuda-llm-storage-pipeline/#pipeline-latency-breakdown","title":"Pipeline Latency Breakdown","text":"<p>Example end-to-end inference run:</p> <pre><code>Stage                 | Latency  | % Total\n----------------------|----------|--------\nFetch Model           | 1.8s     | 12%\nFetch Prompts         | 0.2s     | 1%\nLoad Model            | 3.5s     | 23%\nInference (50 prompts)| 8.2s     | 55%\nUpload Results        | 1.1s     | 7%\nManifest Generation   | 0.3s     | 2%\n----------------------|----------|--------\nTotal                 | 15.1s    | 100%\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#content-addressed-storage","title":"Content-Addressed Storage","text":""},{"location":"projects/cuda-llm-storage-pipeline/#sha256-hashing","title":"SHA256 Hashing","text":"<p>All artifacts are addressed by their SHA256 hash:</p> <pre><code>std::string compute_sha256(const std::string&amp; file_path) {\n    // Read file, compute SHA256\n    // Returns: \"a1b2c3d4e5f6...\"\n}\n</code></pre> <p>Benefits: - Immutability: Content cannot change without changing the hash - Deduplication: Identical files stored only once - Integrity: Automatic corruption detection - Cache Validation: Local cache verification without server roundtrip</p>"},{"location":"projects/cuda-llm-storage-pipeline/#manifest-files","title":"Manifest Files","text":"<p>Each artifact has an associated JSON manifest:</p> <pre><code>{\n  \"artifact_type\": \"model\",\n  \"file_name\": \"gemma-3-1b-Q4_K_M.gguf\",\n  \"sha256\": \"a1b2c3d4e5f6...\",\n  \"size_bytes\": 799408128,\n  \"upload_timestamp\": \"2024-12-24T10:30:45Z\",\n  \"provenance\": {\n    \"source\": \"huggingface.co/google/gemma-3-1b-it\",\n    \"quantization\": \"Q4_K_M\",\n    \"tool\": \"llama.cpp\"\n  }\n}\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#pipeline-orchestration","title":"Pipeline Orchestration","text":""},{"location":"projects/cuda-llm-storage-pipeline/#immutable-run-folders","title":"Immutable Run Folders","text":"<p>Each inference run creates a timestamped folder:</p> <pre><code>runs/\n\u2514\u2500\u2500 run-20241224-103045/\n    \u251c\u2500\u2500 manifest.json          # Run metadata\n    \u251c\u2500\u2500 model-hash.txt         # Model used\n    \u251c\u2500\u2500 prompts-hash.txt       # Prompts used\n    \u251c\u2500\u2500 results-hash.txt       # Results produced\n    \u251c\u2500\u2500 metrics.json           # Performance data\n    \u2514\u2500\u2500 logs/\n        \u251c\u2500\u2500 fetch.log\n        \u251c\u2500\u2500 inference.log\n        \u2514\u2500\u2500 upload.log\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#run-manifest","title":"Run Manifest","text":"<pre><code>{\n  \"run_id\": \"run-20241224-103045\",\n  \"model_hash\": \"a1b2c3d4...\",\n  \"prompts_hash\": \"e5f6g7h8...\",\n  \"results_hash\": \"i9j0k1l2...\",\n  \"start_time\": \"2024-12-24T10:30:45Z\",\n  \"end_time\": \"2024-12-24T10:31:00Z\",\n  \"total_latency_ms\": 15100,\n  \"stages\": {\n    \"fetch_model\": 1800,\n    \"fetch_prompts\": 200,\n    \"load_model\": 3500,\n    \"inference\": 8200,\n    \"upload_results\": 1100,\n    \"manifest\": 300\n  }\n}\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#design-patterns","title":"Design Patterns","text":""},{"location":"projects/cuda-llm-storage-pipeline/#1-separation-of-concerns","title":"1. Separation of Concerns","text":"<ul> <li>Data Plane: Byte movement and storage</li> <li>Control Plane: Metadata and orchestration</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#2-immutable-artifacts","title":"2. Immutable Artifacts","text":"<ul> <li>All artifacts content-addressed by SHA256</li> <li>No in-place modifications</li> <li>Version control through hashing</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#3-explicit-failure-handling","title":"3. Explicit Failure Handling","text":"<ul> <li>Checksum verification on all operations</li> <li>Retry logic with exponential backoff</li> <li>Detailed error reporting</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#4-comprehensive-observability","title":"4. Comprehensive Observability","text":"<ul> <li>Stage-by-stage latency tracking</li> <li>Structured logging</li> <li>Performance percentiles (p50/p95/p99)</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#use-cases","title":"Use Cases","text":"<ul> <li>LLM Infrastructure Research: Study datacenter-scale artifact management</li> <li>Performance Analysis: Benchmark storage layers for ML workloads</li> <li>Pipeline Development: Build reproducible inference pipelines</li> <li>Education: Learn systems design for AI infrastructure</li> <li>Prototyping: Test distributed storage strategies</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#troubleshooting","title":"Troubleshooting","text":""},{"location":"projects/cuda-llm-storage-pipeline/#seaweedfs-connection-failed","title":"SeaweedFS Connection Failed","text":"<pre><code># Verify services are running\ncurl http://localhost:9333/cluster/status\ncurl http://localhost:8080/status\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#sha256-mismatch-on-download","title":"SHA256 Mismatch on Download","text":"<p>Indicates corruption or tampering:</p> <pre><code>Error: SHA256 mismatch\nExpected: a1b2c3d4...\nGot:      z9y8x7w6...\n</code></pre> <p>Solution: Re-upload the artifact and verify storage integrity.</p>"},{"location":"projects/cuda-llm-storage-pipeline/#out-of-disk-space","title":"Out of Disk Space","text":"<pre><code># Check SeaweedFS volume usage\ncurl http://localhost:9333/vol/status\n\n# Garbage collection\n./weed shell\n&gt; volume.balance\n&gt; volume.deleteEmpty\n</code></pre>"},{"location":"projects/cuda-llm-storage-pipeline/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Multi-Region Replication: Geographic distribution for lower latency</li> <li>Smart Caching: Predictive pre-fetching based on access patterns</li> <li>Compression: Transparent artifact compression (ZSTD)</li> <li>Encryption: At-rest and in-transit encryption</li> <li>CDN Integration: CloudFlare/Fastly for global distribution</li> <li>RDMA Support: Ultra-low latency storage access</li> </ul>"},{"location":"projects/cuda-llm-storage-pipeline/#notes","title":"Notes","text":"<p>This project demonstrates that production LLM infrastructure extends far beyond model inference. The complete system requires:</p> <ul> <li>Robust artifact management with versioning and integrity</li> <li>Distributed storage for scalability and reliability</li> <li>Comprehensive observability for debugging and optimization</li> <li>Immutable pipelines for reproducibility</li> </ul> <p>The architecture patterns here mirror those used by major ML infrastructure teams (OpenAI, Anthropic, Google) to manage petabyte-scale model distributions and trillion-token inference workloads.</p> <p>Key Insight: The infrastructure surrounding the model is often more complex than the model itself.</p> <p>Author: Mohammad Waqas</p>"},{"location":"projects/cuda-mpi-llama-scheduler/","title":"cuda-mpi-llama-scheduler","text":"<p>A production-style GPU inference pipeline for running large language models on constrained hardware with CUDA acceleration and multi-rank scheduling.</p> <p>:fontawesome-brands-github: View on GitHub{ .md-button }</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#overview","title":"Overview","text":"<p>This project demonstrates a complete GPU inference pipeline that combines CUDA acceleration with llama.cpp and quantized models to run large language models efficiently on resource-constrained hardware. It features OpenAI-compatible API endpoints and multi-rank scheduling inspired by MPI-style load distribution.</p> <p>Key Achievement:</p> <p>Successfully running a 1B parameter LLM (Gemma-3-1B-IT) on a GPU with only 1GB VRAM while maintaining acceptable performance and providing production-ready HTTP endpoints.</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#features","title":"Features","text":"<ul> <li>llama.cpp HTTP Server: OpenAI-compatible <code>/v1/chat/completions</code> endpoint</li> <li>CUDA Acceleration: GPU-accelerated inference on low-VRAM devices</li> <li>Quantized Models: GGUF Q4_K_M format for extreme memory efficiency</li> <li>Multi-rank Scheduling: MPI-inspired load distribution across concurrent requests</li> <li>Performance Metrics: Detailed latency percentiles (p50, p95, p99) and throughput tracking</li> <li>Prompt Caching: Optimized handling of repeated requests</li> <li>Zero External Dependencies: No Docker, Python, or ML framework requirements (beyond build tools)</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#hardware-model-configuration","title":"Hardware &amp; Model Configuration","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#test-environment","title":"Test Environment","text":"<ul> <li>GPU: NVIDIA GeForce 940M</li> <li>Compute Capability 5.0</li> <li>VRAM: 1GB</li> <li>Model: Gemma-3-1B-IT</li> <li>Format: GGUF Q4_K_M quantization</li> <li>Size: ~762 MiB</li> <li>Context: 32k tokens</li> </ul> <p>This configuration demonstrates that modern LLM inference is possible even on older, resource-constrained hardware.</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#architecture","title":"Architecture","text":"<p>The system consists of three layers working together:</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#1-cuda-backed-inference-engine","title":"1. CUDA-backed Inference Engine","text":"<ul> <li>llama.cpp with CUDA support</li> <li>Quantized model loading (Q4_K_M)</li> <li>GPU memory management for constrained devices</li> <li>Kernel execution optimization</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#2-http-server-layer","title":"2. HTTP Server Layer","text":"<ul> <li>OpenAI-compatible REST API</li> <li>JSON request/response handling</li> <li>Connection management</li> <li>Error handling and logging</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#3-scheduler-load-distribution","title":"3. Scheduler &amp; Load Distribution","text":"<ul> <li>Multi-rank request distribution</li> <li>Concurrent request handling</li> <li>Load balancing across ranks</li> <li>Performance monitoring</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#project-structure","title":"Project Structure","text":"<pre><code>cuda-mpi-llama-scheduler/\n\u251c\u2500\u2500 src/                    # Scheduler and client implementations\n\u2502   \u251c\u2500\u2500 scheduler.cpp       # Multi-rank scheduling logic\n\u2502   \u2514\u2500\u2500 client.cpp          # HTTP client for testing\n\u251c\u2500\u2500 include/mls/            # Header files\n\u251c\u2500\u2500 scripts/                # Build and execution scripts\n\u2502   \u251c\u2500\u2500 build.sh           # Compilation script\n\u2502   \u2514\u2500\u2500 run_2ranks.sh      # 2-rank load test\n\u251c\u2500\u2500 docs/                   # Architecture and analysis guides\n\u2502   \u251c\u2500\u2500 architecture.md    # System design\n\u2502   \u251c\u2500\u2500 setup.md          # Installation guide\n\u2502   \u2514\u2500\u2500 analysis.md       # Performance analysis\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#dependencies","title":"Dependencies","text":"<ul> <li>llama.cpp: CUDA-enabled build</li> <li>CMake: Build system (3.15+)</li> <li>Ninja: Fast build tool</li> <li>nlohmann/json: JSON parsing library</li> <li>CUDA Toolkit: 12.x</li> <li>C++17 compiler: GCC 11+ or Clang 14+</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#build","title":"Build","text":"<pre><code># Run the build script\n./scripts/build.sh\n</code></pre> <p>The build script: 1. Configures CMake with CUDA support 2. Compiles llama.cpp with GPU acceleration 3. Builds the scheduler and client components 4. Links against required libraries</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#prerequisites","title":"Prerequisites","text":"<pre><code># Install build dependencies (Ubuntu/Debian)\nsudo apt-get install cmake ninja-build libcurl4-openssl-dev\n\n# Install nlohmann/json\nsudo apt-get install nlohmann-json3-dev\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#run","title":"Run","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#1-start-the-llamacpp-server","title":"1. Start the llama.cpp Server","text":"<pre><code>./llama-server \\\n    -m /path/to/gemma-3-1b-it-Q4_K_M.gguf \\\n    --port 8090 \\\n    --mlock\n</code></pre> <p>Parameters: - <code>-m</code>: Path to GGUF model file - <code>--port</code>: HTTP server port (default: 8090) - <code>--mlock</code>: Lock model in RAM to prevent swapping</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#2-run-load-tests","title":"2. Run Load Tests","text":"<pre><code># 2-rank concurrent test\n./scripts/run_2ranks.sh\n\n# Custom test with N ranks\n./build/scheduler --ranks N --endpoint http://localhost:8090\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#performance","title":"Performance","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#observed-metrics","title":"Observed Metrics","text":"<p>On NVIDIA GeForce 940M (1GB VRAM):</p> <ul> <li>Mean Latency: ~3342ms</li> <li>Throughput: 16 tokens/sec</li> <li>VRAM Usage: 850-900 MiB</li> <li>GPU Utilization: Stable during inference</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#latency-distribution","title":"Latency Distribution","text":"<ul> <li>p50: ~3200ms</li> <li>p95: ~4500ms</li> <li>p99: ~5200ms</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#memory-efficiency","title":"Memory Efficiency","text":"<p>The Q4_K_M quantization enables: - 4-bit weight quantization - ~762 MiB model footprint - ~100 MiB headroom for KV cache and activations</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#api-usage","title":"API Usage","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#openai-compatible-endpoint","title":"OpenAI-Compatible Endpoint","text":"<pre><code>curl -X POST http://localhost:8090/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Explain CUDA in one sentence.\"}\n    ],\n    \"max_tokens\": 50,\n    \"temperature\": 0.7\n  }'\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#response-format","title":"Response Format","text":"<pre><code>{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1234567890,\n  \"model\": \"gemma-3-1b-it\",\n  \"choices\": [{\n    \"index\": 0,\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"CUDA is NVIDIA's parallel computing platform...\"\n    },\n    \"finish_reason\": \"stop\"\n  }],\n  \"usage\": {\n    \"prompt_tokens\": 12,\n    \"completion_tokens\": 45,\n    \"total_tokens\": 57\n  }\n}\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#multi-rank-scheduling","title":"Multi-Rank Scheduling","text":"<p>The scheduler distributes requests across multiple \"ranks\" (concurrent workers), inspired by MPI programming models:</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#scheduling-strategy","title":"Scheduling Strategy","text":"<ol> <li>Round-Robin Distribution: Requests assigned to ranks in rotation</li> <li>Concurrent Execution: Each rank processes requests independently</li> <li>Aggregated Metrics: Combined latency and throughput statistics</li> </ol>"},{"location":"projects/cuda-mpi-llama-scheduler/#example-2-rank-test","title":"Example: 2-Rank Test","text":"<pre><code># Terminal 1: Start server\n./llama-server -m model.gguf --port 8090\n\n# Terminal 2: Run 2-rank scheduler\n./scripts/run_2ranks.sh\n</code></pre> <p>Output shows per-rank and aggregated performance metrics.</p>"},{"location":"projects/cuda-mpi-llama-scheduler/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#1-prompt-caching","title":"1. Prompt Caching","text":"<ul> <li>Cache repeated prompt prefixes</li> <li>Reduce redundant computation</li> <li>Improve throughput for similar requests</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#2-memory-management","title":"2. Memory Management","text":"<ul> <li>Model locking with <code>--mlock</code></li> <li>Careful KV cache sizing</li> <li>Dynamic batch sizing based on VRAM</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#3-quantization","title":"3. Quantization","text":"<ul> <li>Q4_K_M: 4-bit weights with optimized kernels</li> <li>Minimal accuracy loss</li> <li>4x memory reduction vs FP16</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#benchmarking","title":"Benchmarking","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code># Standard benchmark (10 requests)\n./build/client --endpoint http://localhost:8090 --requests 10\n\n# High-load test (100 requests, 4 ranks)\n./build/scheduler --ranks 4 --requests 100\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#metrics-collected","title":"Metrics Collected","text":"<ul> <li>Request latency (p50, p95, p99, max)</li> <li>Throughput (tokens/sec, requests/sec)</li> <li>GPU utilization</li> <li>Memory usage</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#use-cases","title":"Use Cases","text":"<ul> <li>Edge Inference: Running LLMs on resource-constrained devices</li> <li>Research: Studying performance characteristics of quantized models</li> <li>Education: Learning CUDA-accelerated inference pipelines</li> <li>Development: Prototyping LLM applications without cloud dependencies</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#troubleshooting","title":"Troubleshooting","text":""},{"location":"projects/cuda-mpi-llama-scheduler/#out-of-memory-errors","title":"Out of Memory Errors","text":"<p>Reduce context size or use more aggressive quantization:</p> <pre><code>./llama-server -m model.gguf --ctx-size 2048\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#slow-performance","title":"Slow Performance","text":"<p>Enable GPU layers:</p> <pre><code>./llama-server -m model.gguf --n-gpu-layers 99\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#connection-refused","title":"Connection Refused","text":"<p>Check server is running:</p> <pre><code>curl http://localhost:8090/health\n</code></pre>"},{"location":"projects/cuda-mpi-llama-scheduler/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Support for multi-GPU systems</li> <li>Advanced scheduling algorithms (priority queues, backpressure)</li> <li>Integration with NCCL for distributed inference</li> <li>Dynamic model swapping</li> <li>Quantization-aware fine-tuning support</li> </ul>"},{"location":"projects/cuda-mpi-llama-scheduler/#notes","title":"Notes","text":"<p>This project showcases that sophisticated LLM inference is achievable on modest hardware through careful optimization, quantization, and efficient resource management. The combination of CUDA acceleration, quantized models, and smart scheduling enables production-quality inference on GPUs that would typically be considered insufficient for modern LLM workloads.</p> <p>Key Insight: The bottleneck often isn't the GPU compute capability, but rather memory bandwidth and capacity. Quantization and efficient memory management unlock LLM inference on previously unsuitable hardware.</p> <p>Author: Mohammad Waqas</p>"},{"location":"projects/cuda-openmpi/","title":"cuda-openmpi","text":"<p>A comprehensive testing framework to validate NVIDIA CUDA and OpenMPI integration on Linux systems.</p> <p>:fontawesome-brands-github: View on GitHub{ .md-button }</p>"},{"location":"projects/cuda-openmpi/#overview","title":"Overview","text":"<p>This repository provides an integration test suite that verifies the proper configuration of GPU computing environments for parallel computing applications. It specifically tests CUDA-aware MPI capabilities, which enable direct GPU-to-GPU communication without intermediate host memory transfers.</p> <p>Purpose:</p> <p>Validate that your CUDA toolkit and OpenMPI installation are correctly configured and can work together for distributed GPU computing workloads.</p>"},{"location":"projects/cuda-openmpi/#test-coverage","title":"Test Coverage","text":"<p>The suite evaluates three core areas:</p>"},{"location":"projects/cuda-openmpi/#1-basic-cuda-operations","title":"1. Basic CUDA Operations","text":"<ul> <li>GPU device detection and enumeration</li> <li>Memory allocation and deallocation on device</li> <li>Kernel execution (vector addition test)</li> <li>Host-to-device and device-to-host data transfers</li> <li>Result validation</li> </ul>"},{"location":"projects/cuda-openmpi/#2-standard-mpi-communication","title":"2. Standard MPI Communication","text":"<ul> <li>Process initialization and rank assignment</li> <li>Host-to-host data transfers between MPI ranks</li> <li>Data integrity verification across multiple processes</li> <li>Inter-process communication patterns</li> </ul>"},{"location":"projects/cuda-openmpi/#3-cuda-aware-mpi","title":"3. CUDA-aware MPI","text":"<ul> <li>Direct GPU-to-GPU communication capabilities</li> <li>Automatic detection of CUDA support in OpenMPI</li> <li>Performance comparison with host-staged transfers</li> </ul>"},{"location":"projects/cuda-openmpi/#system-specifications","title":"System Specifications","text":"<p>The test suite has been validated on:</p> <ul> <li>CUDA Version: 12.8</li> <li>OpenMPI Version: 5.0.6</li> <li>Test GPU: NVIDIA GeForce 940M</li> <li>1GB VRAM</li> <li>Compute Capability 5.0</li> <li>Operating System: Xubuntu 22.04</li> </ul>"},{"location":"projects/cuda-openmpi/#project-structure","title":"Project Structure","text":"<pre><code>cuda-openmpi/\n\u251c\u2500\u2500 cuda_mpi_test.cu        # Main CUDA/MPI test program\n\u251c\u2500\u2500 CMakeLists.txt          # CMake build configuration\n\u251c\u2500\u2500 Makefile.direct         # Alternative direct build\n\u251c\u2500\u2500 build_and_run.sh        # Interactive build/test script\n\u251c\u2500\u2500 quick_test.sh           # Automated quick test\n\u251c\u2500\u2500 logs/                   # Test execution logs\n\u2514\u2500\u2500 README.md               # Documentation\n</code></pre>"},{"location":"projects/cuda-openmpi/#key-files","title":"Key Files","text":"<ul> <li>cuda_mpi_test.cu: Comprehensive test program covering CUDA operations, MPI communication, and CUDA-aware MPI</li> <li>build_and_run.sh: Interactive script for building and testing with multiple configurations</li> <li>quick_test.sh: Automated testing for CI/CD pipelines</li> </ul>"},{"location":"projects/cuda-openmpi/#dependencies","title":"Dependencies","text":"<ul> <li>NVIDIA CUDA Toolkit (12.8+)</li> <li>OpenMPI (5.0.6+)</li> <li>CMake (3.15+)</li> <li>C++ compiler with CUDA support (nvcc)</li> <li>Linux environment</li> </ul>"},{"location":"projects/cuda-openmpi/#build","title":"Build","text":""},{"location":"projects/cuda-openmpi/#using-cmake-recommended","title":"Using CMake (Recommended)","text":"<pre><code># Create build directory\nmkdir -p build &amp;&amp; cd build\n\n# Configure the project\ncmake ..\n\n# Compile\nmake\n</code></pre> <p>Expected build output confirms: - MPI detection and configuration - CUDA toolkit version (12.8) - Target GPU architecture (Compute Capability 5.0)</p>"},{"location":"projects/cuda-openmpi/#using-direct-makefile","title":"Using Direct Makefile","text":"<pre><code># Alternative build method\nmake -f Makefile.direct\n</code></pre>"},{"location":"projects/cuda-openmpi/#run","title":"Run","text":""},{"location":"projects/cuda-openmpi/#test-cuda-only-single-process","title":"Test CUDA Only (Single Process)","text":"<pre><code>./cuda_mpi_test\n</code></pre> <p>This runs basic CUDA operations without MPI communication.</p>"},{"location":"projects/cuda-openmpi/#test-cuda-mpi-multiple-processes","title":"Test CUDA + MPI (Multiple Processes)","text":"<pre><code># Run with 2 MPI ranks\nmpirun -np 2 ./cuda_mpi_test\n\n# Run with 4 MPI ranks\nmpirun -np 4 ./cuda_mpi_test\n</code></pre>"},{"location":"projects/cuda-openmpi/#check-cuda-aware-mpi-support","title":"Check CUDA-aware MPI Support","text":"<pre><code># Verify if OpenMPI was built with CUDA support\nompi_info --parsable --all | grep mpi_built_with_cuda_support\n</code></pre> <p>If the output shows <code>false</code>, your OpenMPI installation uses the standard data flow:</p> <pre><code>GPU \u2192 Host Memory \u2192 MPI Transfer \u2192 Host Memory \u2192 GPU\n</code></pre>"},{"location":"projects/cuda-openmpi/#test-output","title":"Test Output","text":"<p>Successful execution displays:</p> <pre><code>=== CUDA Basic Test ===\nFound 1 CUDA device(s)\nUsing device 0: GeForce 940M\nCUDA vector addition test PASSED\n\n=== MPI Communication Test ===\nRank 0/2: Host-to-host MPI test PASSED\nRank 1/2: Host-to-host MPI test PASSED\n\n=== CUDA-aware MPI Test ===\nCUDA-aware MPI: Not supported (using host staging)\nGPU-to-GPU communication test PASSED\n</code></pre>"},{"location":"projects/cuda-openmpi/#architecture-notes","title":"Architecture Notes","text":""},{"location":"projects/cuda-openmpi/#standard-vs-cuda-aware-mpi","title":"Standard vs CUDA-aware MPI","text":"<p>Standard MPI (most common):</p> <pre><code>Process A GPU \u2192 Host Memory \u2192 MPI Send \u2192 Host Memory \u2192 Process B GPU\n</code></pre> <p>CUDA-aware MPI (requires special OpenMPI build):</p> <pre><code>Process A GPU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MPI Send \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Process B GPU\n</code></pre> <p>CUDA-aware MPI eliminates intermediate host copies, reducing latency and improving throughput for GPU-intensive parallel applications.</p>"},{"location":"projects/cuda-openmpi/#memory-considerations","title":"Memory Considerations","text":"<p>With 1GB VRAM on the test GPU: - Safe allocation per process: &lt;400MB - Test suite memory usage: ~12KB per process - Suitable for resource-constrained hardware testing</p>"},{"location":"projects/cuda-openmpi/#performance","title":"Performance","text":"<p>The test suite uses minimal resources to ensure compatibility with constrained hardware:</p> <ul> <li>Vector Size: 3 elements (minimal test)</li> <li>Memory per Process: ~12KB GPU memory</li> <li>Scalability: Tested up to 4 MPI ranks on single GPU</li> </ul> <p>This conservative approach ensures the tests run on older or low-VRAM GPUs like the GeForce 940M.</p>"},{"location":"projects/cuda-openmpi/#troubleshooting","title":"Troubleshooting","text":""},{"location":"projects/cuda-openmpi/#mpi-not-found","title":"MPI Not Found","text":"<pre><code># Install OpenMPI on Ubuntu/Debian\nsudo apt-get install libopenmpi-dev openmpi-bin\n</code></pre>"},{"location":"projects/cuda-openmpi/#cuda-not-found","title":"CUDA Not Found","text":"<p>Ensure CUDA toolkit is installed and <code>nvcc</code> is in your PATH:</p> <pre><code>nvcc --version\n</code></pre>"},{"location":"projects/cuda-openmpi/#gpu-memory-errors","title":"GPU Memory Errors","text":"<p>Reduce the test vector size in <code>cuda_mpi_test.cu</code> if running on GPUs with &lt;1GB VRAM.</p>"},{"location":"projects/cuda-openmpi/#use-cases","title":"Use Cases","text":"<ul> <li>Environment Validation: Verify CUDA + MPI setup before running production workloads</li> <li>CI/CD Testing: Automated verification of GPU cluster configurations</li> <li>Educational: Learn CUDA-MPI integration patterns</li> <li>Benchmarking: Baseline performance testing for distributed GPU applications</li> </ul>"},{"location":"projects/cuda-openmpi/#future-enhancements","title":"Future Enhancements","text":"<ul> <li>Support for multi-GPU systems</li> <li>Performance benchmarking suite</li> <li>Advanced CUDA-aware MPI patterns</li> <li>Integration with NCCL for multi-GPU communication</li> </ul>"},{"location":"projects/cuda-openmpi/#notes","title":"Notes","text":"<p>This test suite is essential for validating distributed GPU computing environments. While most OpenMPI installations don't include CUDA-aware support by default, the test suite gracefully handles both scenarios and provides clear feedback on available capabilities.</p> <p>Author: Mohammad Waqas</p>"},{"location":"projects/cuda-tcp-llama/","title":"cuda-tcp-llama.cpp","text":"<p>A high-performance TCP-based inference server for CUDA-accelerated LLM inference using llama.cpp as the backend.</p> <p>:fontawesome-brands-github: View on GitHub{ .md-button }</p>"},{"location":"projects/cuda-tcp-llama/#overview","title":"Overview","text":"<p>This project implements a custom TCP-based inference data plane designed to serve CUDA-accelerated large language model (LLM) inference. It explores low-level networking, concurrency, and GPU integration challenges under tight hardware constraints like limited VRAM (\u22641 GB).</p> <p>Key Focus Areas:</p> <ul> <li>Custom binary TCP protocol optimized for minimal latency</li> <li>Non-blocking I/O with epoll-based concurrency</li> <li>Direct CUDA integration with llama.cpp</li> <li>Performance instrumentation (latency percentiles, throughput)</li> <li>Explicit control over data movement for predictable latency</li> </ul>"},{"location":"projects/cuda-tcp-llama/#architecture","title":"Architecture","text":"<p>The system is organized into three distinct layers:</p>"},{"location":"projects/cuda-tcp-llama/#1-control-plane","title":"1. Control Plane","text":"<ul> <li>Process lifecycle management</li> <li>Server configuration</li> <li>Backend selection</li> </ul>"},{"location":"projects/cuda-tcp-llama/#2-data-plane","title":"2. Data Plane","text":"<ul> <li>Custom TCP protocol with binary framing</li> <li>epoll-driven I/O for efficient multi-client handling</li> <li>Backpressure control mechanisms</li> <li>Minimal abstraction overhead</li> </ul>"},{"location":"projects/cuda-tcp-llama/#3-inference-runtime","title":"3. Inference Runtime","text":"<ul> <li>CUDA-backed llama.cpp engine</li> <li>GGUF quantized models</li> <li>Optimized for GPUs with \u22641 GB VRAM</li> </ul> <p>This clean separation enables future extensions to RDMA or GPU-aware transports without core modifications.</p>"},{"location":"projects/cuda-tcp-llama/#features","title":"Features","text":"<ul> <li>Custom TCP Protocol: Binary framing designed for minimal latency and overhead</li> <li>epoll-Based Concurrency: Non-blocking socket I/O handling multiple clients efficiently</li> <li>GPU-Aware Inference: Direct CUDA integration optimized for constrained GPUs</li> <li>Performance Metrics: Latency tracking (p50/p95/p99) and throughput measurement</li> <li>Low Abstraction: Emphasis on predictable latency and explicit control</li> </ul>"},{"location":"projects/cuda-tcp-llama/#technology-stack","title":"Technology Stack","text":"<ul> <li>Language: C++17/C++20</li> <li>GPU: CUDA 12.x</li> <li>Networking: epoll and POSIX sockets</li> <li>Inference Backend: llama.cpp (GGUF models)</li> <li>Build System: CMake</li> <li>Platform: Linux</li> </ul>"},{"location":"projects/cuda-tcp-llama/#build","title":"Build","text":"<pre><code># Configure the build\ncmake -S . -B build -DCMAKE_BUILD_TYPE=Release\n\n# Build the project\ncmake --build build -j\n</code></pre>"},{"location":"projects/cuda-tcp-llama/#prerequisites","title":"Prerequisites","text":"<ul> <li>CUDA Toolkit 12.x</li> <li>C++17/C++20 compatible compiler</li> <li>CMake 3.15+</li> <li>llama.cpp dependencies</li> </ul>"},{"location":"projects/cuda-tcp-llama/#run","title":"Run","text":""},{"location":"projects/cuda-tcp-llama/#starting-the-server","title":"Starting the Server","text":"<pre><code>./build/bin/llama_tcp_server \\\n    --model /path/to/model.gguf \\\n    --listen 0.0.0.0:8080\n</code></pre>"},{"location":"projects/cuda-tcp-llama/#configuration-options","title":"Configuration Options","text":"<ul> <li><code>--model</code>: Path to GGUF quantized model file</li> <li><code>--listen</code>: Address and port to bind the server (default: 0.0.0.0:8080)</li> </ul>"},{"location":"projects/cuda-tcp-llama/#performance","title":"Performance","text":"<p>The system prioritizes:</p> <ul> <li>Minimal Latency: Direct GPU memory management and non-blocking I/O</li> <li>Efficient Concurrency: epoll-based event loop for multiple concurrent clients</li> <li>Memory Efficiency: Optimized for GPUs with limited VRAM (\u22641 GB)</li> </ul> <p>Performance metrics include: - p50/p95/p99 latency percentiles - Tokens per second throughput - GPU memory utilization</p>"},{"location":"projects/cuda-tcp-llama/#design-philosophy","title":"Design Philosophy","text":"<p>\"Explicit control over data movement\"</p> <p>The project emphasizes:</p> <ol> <li>Clean Separation: Networking logic independent from inference runtime</li> <li>Low-Level Control: Direct management of sockets, memory, and GPU operations</li> <li>Predictable Performance: Minimal abstractions for consistent latency</li> <li>Extensibility: Architecture ready for RDMA or GPU-direct transports</li> </ol>"},{"location":"projects/cuda-tcp-llama/#use-cases","title":"Use Cases","text":"<ul> <li>Research into low-latency LLM serving</li> <li>GPU-constrained environments (edge devices, older GPUs)</li> <li>Custom networking protocols for inference workloads</li> <li>Educational exploration of systems programming with CUDA</li> </ul>"},{"location":"projects/cuda-tcp-llama/#notes","title":"Notes","text":"<p>This project demonstrates production-grade systems thinking for LLM infrastructure, focusing on the intersection of high-performance networking and GPU computing. It's particularly valuable for understanding how to build efficient inference services on resource-constrained hardware.</p> <p>Author: Mohammad Waqas</p>"}]}