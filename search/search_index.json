{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"On-Device AI for Legacy NVIDIA GPUs","text":"<p>Product-minded engineer building production-ready AI tools for Ubuntu 22.04 + old NVIDIA GPUs</p>"},{"location":"#the-llcuda-ecosystem","title":"The llcuda Ecosystem","text":"<p>Making large language models accessible on legacy hardware through empirical engineering and zero-configuration design.</p>"},{"location":"#featured-project-llcuda","title":"Featured Project: llcuda","text":"<p>llcuda on PyPI is a Python package that brings LLM inference to old NVIDIA GPUs with minimal setup. Built specifically for Ubuntu 22.04 and tested extensively on GeForce 940M (1GB VRAM).</p> <pre><code>pip install llcuda\npython -m llcuda\n</code></pre> <p>Key Features:</p> <ul> <li>Zero Configuration: Works out of the box on Ubuntu 22.04</li> <li>Legacy GPU Support: Optimized for GPUs with 1GB VRAM (GeForce 940M tested)</li> <li>Production Ready: Published to PyPI with comprehensive testing</li> <li>JupyterLab Integration: First-class support for notebook workflows</li> <li>Empirical Performance: ~15 tokens/second with Gemma 2 2B Q4_K_M on GeForce 940M</li> </ul>"},{"location":"#infrastructure-ubuntu-cuda-llamacpp-executable","title":"Infrastructure: Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>The foundation of the llcuda ecosystem is a pre-built, statically-linked llama.cpp binary compiled for Ubuntu 22.04 with CUDA 12.6 support. This eliminates the need for users to compile llama.cpp themselves.</p> <p>Why This Matters:</p> <ul> <li>No compilation required</li> <li>No CUDA toolkit installation needed</li> <li>No dependency hell</li> <li>Just download and run</li> </ul>"},{"location":"#philosophy-product-minded-engineering","title":"Philosophy: Product-Minded Engineering","text":"<p>I build tools that actually work on real hardware people own. No assumptions about cutting-edge GPUs. No theoretical benchmarks.</p> <p>My Approach:</p> <ol> <li>Target Real Hardware: GeForce 940M (1GB VRAM) as the baseline</li> <li>Empirical Testing: Every claim backed by measurements on actual hardware</li> <li>Zero-Configuration: Installation should be <code>pip install</code> and done</li> <li>Production Quality: Published to PyPI, not just GitHub repos</li> <li>Documentation First: If users can't use it, it doesn't exist</li> </ol>"},{"location":"#performance-data","title":"Performance Data","text":"<p>All benchmarks run on GeForce 940M (1GB VRAM, 384 CUDA cores, Maxwell architecture) on Ubuntu 22.04.</p>"},{"location":"#gemma-2-2b-q4_k_m","title":"Gemma 2 2B Q4_K_M","text":"<pre><code>Model: google/gemma-2-2b-it (Q4_K_M quantization)\nHardware: GeForce 940M (1GB VRAM)\nPerformance: ~15 tokens/second\nContext: 2048 tokens\nMemory Usage: ~950MB VRAM\n</code></pre>"},{"location":"#real-world-use-cases","title":"Real-World Use Cases","text":"<ul> <li>Interactive Chat: Responsive enough for real-time conversation</li> <li>Jupyter Notebooks: Perfect for exploratory data analysis and prototyping</li> <li>Local Development: Test LLM integrations without cloud APIs</li> <li>Learning: Understand LLM behavior without expensive hardware</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get up and running in under 5 minutes:</p> <pre><code># Install llcuda\npip install llcuda\n\n# Run interactive chat (downloads model automatically)\npython -m llcuda\n\n# Or use in Python\nfrom llcuda import LLM\n\nllm = LLM()\nresponse = llm.chat(\"Explain quantum computing in simple terms.\")\nprint(response)\n</code></pre> <p>For JupyterLab integration:</p> <pre><code>from llcuda import LLM\n\nllm = LLM(model=\"gemma-2-2b-it\")\n\n# Interactive chat with context\nconversation = [\n    \"What is machine learning?\",\n    \"How does it differ from traditional programming?\",\n    \"Give me a practical example\"\n]\n\nfor message in conversation:\n    response = llm.chat(message)\n    print(f\"User: {message}\")\n    print(f\"AI: {response}\\n\")\n</code></pre> <p>View detailed quick start guide \u2192</p>"},{"location":"#why-llcuda","title":"Why llcuda?","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Most LLM tools assume you have: - Modern NVIDIA GPUs (RTX series) - 8GB+ VRAM - Willingness to compile complex C++ projects - Latest CUDA toolkit installed</p> <p>Reality: Millions of users have older GPUs collecting dust.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>llcuda is designed for the hardware people actually own:</p> <ul> <li>GeForce 900 series: 940M, 950M, 960M</li> <li>GeForce 800 series: 840M, 850M</li> <li>Maxwell/Kepler architectures: Still capable, just ignored</li> <li>1-2GB VRAM: More than enough for quantized models</li> </ul>"},{"location":"#the-approach","title":"The Approach","text":"<ol> <li>Pre-built binaries: No compilation needed</li> <li>Quantized models: Q4_K_M quantization for memory efficiency</li> <li>Empirical optimization: Tested on actual hardware, not simulators</li> <li>Python-first: Native integration with data science workflows</li> </ol>"},{"location":"#projects","title":"Projects","text":""},{"location":"#core-infrastructure","title":"Core Infrastructure","text":""},{"location":"#llcuda","title":"llcuda","text":"<p>PyPI Package | GitHub</p> <p>Python package for LLM inference on legacy NVIDIA GPUs. Zero-configuration installation, JupyterLab integration, empirically tested on GeForce 940M.</p> <p>Explore llcuda documentation \u2192</p>"},{"location":"#ubuntu-cuda-llamacpp-executable","title":"Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>GitHub</p> <p>Pre-built llama.cpp binary for Ubuntu 22.04 with CUDA 12.6 support. The foundation that makes llcuda possible.</p> <p>View documentation \u2192</p>"},{"location":"#technical-stack","title":"Technical Stack","text":"<p>Languages &amp; Tools: - Python (packaging, PyPI distribution) - CUDA (GPU acceleration) - C++ (llama.cpp integration) - CMake (build systems)</p> <p>Expertise: - PyPI package publishing and versioning - CUDA programming for legacy GPUs (compute capability 5.0) - Empirical performance testing and optimization - Production-quality Python library design - Technical documentation and developer experience</p> <p>Hardware Testing: - GeForce 940M (1GB VRAM, Maxwell architecture) - Ubuntu 22.04 LTS - CUDA 12.6</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to run LLMs on your old GPU?</p> <ol> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Installation Guide - Comprehensive setup instructions</li> <li>Performance Data - Real benchmarks on real hardware</li> <li>Examples - Production-ready code samples</li> </ol>"},{"location":"#about","title":"About","text":"<p>I'm a product-minded engineer focused on making AI tools accessible on hardware people actually own. I believe in empirical testing, zero-configuration design, and building tools that solve real problems.</p> <p>Published Work: - llcuda on PyPI - Python package for LLM inference on legacy GPUs</p> <p>Philosophy: - Build for real hardware, not ideal hardware - Every claim backed by measurements - Installation should be trivial - Documentation is a feature, not an afterthought</p> <p>Read more about my background \u2192</p>"},{"location":"#contact","title":"Contact","text":"<p>Email: waqasm86@gmail.com GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>Get in touch \u2192</p>"},{"location":"about/","title":"About Me","text":"<p>Product-minded engineer building on-device AI tools for hardware people actually own.</p>"},{"location":"about/#philosophy","title":"Philosophy","text":"<p>I build tools that actually work on real hardware, not just theoretical benchmarks on cutting-edge GPUs. My approach is rooted in empirical engineering: every claim is backed by measurements on actual hardware, and installation should be as simple as <code>pip install</code>.</p> <p>Core Beliefs:</p> <ul> <li>Build for Real Hardware: GeForce 940M with 1GB VRAM is my baseline, not an afterthought</li> <li>Zero-Configuration Design: If it requires manual compilation or complex setup, I haven't finished the job</li> <li>Empirical Testing: Benchmarks on simulators don't count. Real hardware or it didn't happen</li> <li>Production Quality: Publish to PyPI, not just GitHub repos. Users deserve properly packaged software</li> <li>Documentation as a Feature: If users can't figure out how to use it, it doesn't exist</li> </ul>"},{"location":"about/#current-work","title":"Current Work","text":""},{"location":"about/#llcuda-llm-inference-for-legacy-gpus","title":"llcuda - LLM Inference for Legacy GPUs","text":"<p>PyPI Package | Documentation | GitHub</p> <p>Python package that brings LLM inference to old NVIDIA GPUs with zero configuration. Published to PyPI, tested extensively on GeForce 940M (1GB VRAM).</p> <p>Key Achievements: - Published production-ready package to PyPI - Achieved ~15 tokens/second on GeForce 940M - Zero-configuration installation (no CUDA toolkit, no compilation) - JupyterLab-first design for data science workflows - Comprehensive documentation and examples</p> <p>Technical Stack: - Python packaging and PyPI distribution - CUDA programming for legacy GPUs (compute capability 5.0) - CMake build systems and static linking - Empirical performance testing and optimization</p>"},{"location":"about/#ubuntu-cuda-llamacpp-executable","title":"Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>GitHub | Documentation</p> <p>Pre-built llama.cpp binary with CUDA 12.6 support for Ubuntu 22.04. The infrastructure that makes llcuda possible.</p> <p>Key Achievements: - Eliminated compilation requirement for llama.cpp - Static linking removes all external dependencies - Optimized for Maxwell architecture (compute 5.0) - Verified on multiple legacy GPU models</p>"},{"location":"about/#technical-expertise","title":"Technical Expertise","text":""},{"location":"about/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python: PyPI packaging, library design, API development</li> <li>CUDA/C++: GPU programming, performance optimization</li> <li>Shell/Bash: Build automation, deployment scripts</li> </ul>"},{"location":"about/#tools-technologies","title":"Tools &amp; Technologies","text":"<ul> <li>CUDA Development: Programming for legacy GPUs (Maxwell architecture)</li> <li>Build Systems: CMake, static linking, cross-compilation</li> <li>Package Management: PyPI publishing, semantic versioning</li> <li>Version Control: Git, GitHub workflows, CI/CD</li> <li>Documentation: MkDocs, technical writing, developer experience</li> </ul>"},{"location":"about/#specializations","title":"Specializations","text":"<ul> <li>GPU Computing: Optimizing for low-VRAM, legacy NVIDIA GPUs</li> <li>Python Packaging: Creating production-ready PyPI packages</li> <li>Performance Testing: Empirical benchmarking on real hardware</li> <li>Developer Experience: Zero-configuration tools, comprehensive docs</li> <li>LLM Systems: Integration with llama.cpp, model quantization, inference optimization</li> </ul>"},{"location":"about/#approach-to-engineering","title":"Approach to Engineering","text":""},{"location":"about/#1-product-minded-development","title":"1. Product-Minded Development","text":"<p>I don't just write code; I build products that solve real problems for real users.</p> <p>Example: llcuda isn't just a Python wrapper around llama.cpp. It's a complete solution that handles model downloading, GPU detection, error recovery, and provides a Jupyter-friendly API.</p>"},{"location":"about/#2-empirical-testing","title":"2. Empirical Testing","text":"<p>All performance claims are backed by measurements on actual hardware.</p> <p>Example: Every benchmark in the llcuda documentation was run on a GeForce 940M. No simulations, no theoretical calculations.</p>"},{"location":"about/#3-documentation-first","title":"3. Documentation First","text":"<p>Documentation isn't an afterthought\u2014it's a core feature.</p> <p>Example: llcuda has a complete quick start guide, installation guide, performance guide, and production-ready examples. Users can get running in under 5 minutes.</p>"},{"location":"about/#4-zero-configuration-design","title":"4. Zero-Configuration Design","text":"<p>Installation complexity is a bug, not a feature.</p> <p>Example: llcuda installs with <code>pip install llcuda</code>. No CUDA toolkit, no compilation, no configuration files. It just works.</p>"},{"location":"about/#5-production-quality","title":"5. Production Quality","text":"<p>If it's not on PyPI with proper versioning, it's not production-ready.</p> <p>Example: llcuda is published to PyPI with semantic versioning, not just a GitHub repo with a README.</p>"},{"location":"about/#background","title":"Background","text":""},{"location":"about/#education","title":"Education","text":"<p>Computer Science Background with focus on: - Algorithms and Data Structures - Systems Programming - GPU Computing and Parallel Processing - Machine Learning and AI</p>"},{"location":"about/#professional-experience","title":"Professional Experience","text":"<p>Product-Minded Software Engineer specializing in: - Python package development and PyPI publishing - CUDA programming and GPU optimization - Building tools for machine learning workflows - Technical documentation and developer experience</p> <p>Key Projects: - llcuda: PyPI package for LLM inference on legacy GPUs - Ubuntu-Cuda-Llama.cpp-Executable: Pre-built llama.cpp binaries - CUDA Systems Research: Empirical testing on Maxwell-era GPUs</p>"},{"location":"about/#why-legacy-gpus","title":"Why Legacy GPUs?","text":""},{"location":"about/#the-problem","title":"The Problem","text":"<p>The AI/ML community often assumes everyone has: - Modern RTX GPUs (3000/4000 series) - 8GB+ VRAM - Willingness to spend $500-$2000 on hardware</p> <p>Reality: Millions of people have perfectly capable older GPUs collecting dust because the tools don't support them.</p>"},{"location":"about/#the-opportunity","title":"The Opportunity","text":"<p>Legacy GPUs like the GeForce 940M can run modern LLMs with proper optimization: - 1GB VRAM is enough for 2B parameter models - Maxwell architecture (2014) still has hundreds of CUDA cores - Quantization (Q4_K_M) makes models fit in limited memory - Performance is acceptable for interactive use (~15 tok/s)</p>"},{"location":"about/#the-mission","title":"The Mission","text":"<p>Make AI tools accessible on hardware people already own. No expensive upgrades needed.</p>"},{"location":"about/#projects","title":"Projects","text":""},{"location":"about/#active-projects","title":"Active Projects","text":"<p>llcuda - LLM inference for legacy NVIDIA GPUs - Status: Published to PyPI, actively maintained - Focus: Python API, JupyterLab integration, documentation - Links: PyPI | Docs | GitHub</p> <p>Ubuntu-Cuda-Llama.cpp-Executable - Pre-built llama.cpp binaries - Status: Released, stable - Focus: Build optimization, platform support - Links: GitHub | Docs</p>"},{"location":"about/#future-directions","title":"Future Directions","text":"<p>Planned Work: - Windows Support: Pre-built binaries for Windows + CUDA - Model Optimization: Custom quantization for legacy GPUs - Advanced Features: Speculative decoding, FlashAttention integration - Broader Hardware Support: AMD GPUs (ROCm), Intel GPUs (oneAPI)</p>"},{"location":"about/#values","title":"Values","text":""},{"location":"about/#accessibility","title":"Accessibility","text":"<p>AI tools should be accessible to everyone, not just those with expensive hardware.</p>"},{"location":"about/#empiricism","title":"Empiricism","text":"<p>Claims should be backed by real measurements, not marketing promises.</p>"},{"location":"about/#transparency","title":"Transparency","text":"<p>Open source code, public documentation, honest benchmarks.</p>"},{"location":"about/#quality","title":"Quality","text":"<p>Production-ready tools, not just research prototypes.</p>"},{"location":"about/#user-centric","title":"User-Centric","text":"<p>Design for the user's experience, not the developer's convenience.</p>"},{"location":"about/#technical-writing","title":"Technical Writing","text":"<p>I believe in documentation as a core feature of software. Good documentation: - Gets users productive in minutes, not hours - Provides realistic benchmarks and expectations - Includes production-ready code examples - Anticipates and answers common questions - Is maintained alongside the code</p> <p>Example: The llcuda documentation includes: - 5-minute quick start guide - Comprehensive installation guide with troubleshooting - Real benchmarks on actual hardware - Production-ready code examples - Clear explanations of design decisions</p>"},{"location":"about/#open-source","title":"Open Source","text":"<p>All my projects are open source:</p> <p>llcuda - License: MIT - Repository: github.com/waqasm86/llcuda - Contributions: Bug reports, feature requests, and PRs welcome</p> <p>Ubuntu-Cuda-Llama.cpp-Executable - License: MIT - Repository: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable - Contributions: Build configurations, platform support, optimizations</p>"},{"location":"about/#contact","title":"Contact","text":"<p>I'm always interested in: - Feedback on llcuda and related projects - Collaboration on making AI more accessible - Discussions about GPU optimization and LLM systems - Opportunities to build production-quality AI tools</p> <p>Email: waqasm86@gmail.com GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>Get in touch \u2192</p>"},{"location":"about/#resume","title":"Resume","text":"<p>For a detailed resume including professional experience, education, and technical skills:</p> <p>Download Resume (PDF)</p>"},{"location":"about/#testimonials","title":"Testimonials","text":""},{"location":"about/#from-the-community","title":"From the Community","text":"<p>\"Finally, a tool that actually works on my old laptop GPU! llcuda made LLM development accessible without buying new hardware.\" \u2014 Data Science Student</p> <p>\"The documentation is excellent. Got up and running in under 5 minutes, exactly as promised.\" \u2014 Python Developer</p> <p>\"Impressive performance on legacy hardware. The empirical benchmarks gave me realistic expectations.\" \u2014 ML Engineer</p>"},{"location":"about/#inspiration","title":"Inspiration","text":"<p>My work is inspired by engineers who build practical, accessible tools:</p> <ul> <li>Dan McCreary: Excellent technical documentation and knowledge graphs</li> <li>Georgi Gerganov: Creator of llama.cpp, making LLMs accessible</li> <li>Yann LeCun: Advocate for open, accessible AI research</li> <li>Linus Torvalds: Focus on practical engineering over hype</li> </ul>"},{"location":"about/#fun-facts","title":"Fun Facts","text":"<ul> <li>Favorite GPU: GeForce 940M (1GB VRAM) - my primary testing platform</li> <li>Favorite Language: Python for APIs, C++ for performance</li> <li>Favorite Tool: JupyterLab for interactive development</li> <li>Favorite Metric: Tokens per second (measured on real hardware)</li> <li>Favorite Documentation Style: MkDocs Material (this site!)</li> </ul>"},{"location":"about/#whats-next","title":"What's Next?","text":"<p>I'm continually working to make AI more accessible on legacy hardware:</p> <ol> <li>Expand llcuda: Windows support, more models, better performance</li> <li>Explore New Architectures: AMD GPUs (ROCm), Intel GPUs (oneAPI)</li> <li>Build Community: Help others run LLMs on their existing hardware</li> <li>Document Everything: Share knowledge through comprehensive guides</li> </ol> <p>Follow my work: - GitHub: github.com/waqasm86 - PyPI: pypi.org/project/llcuda - This Site: Regular updates on projects and learnings</p>"},{"location":"about/#lets-build-together","title":"Let's Build Together","text":"<p>Interested in collaborating on making AI tools more accessible? Have an old GPU and want to contribute to testing? Want to improve the documentation?</p> <p>I'd love to hear from you.</p> <p>Contact Me \u2192</p>"},{"location":"contact/","title":"Contact","text":"<p>Let's connect! I'm always interested in discussions about GPU optimization, LLM systems, and making AI tools more accessible.</p>"},{"location":"contact/#get-in-touch","title":"Get in Touch","text":""},{"location":"contact/#email","title":"Email","text":"<p>waqasm86@gmail.com</p> <p>Best for: - Project inquiries - Collaboration opportunities - Technical discussions - Feedback on llcuda</p> <p>Response time: Usually within 24-48 hours</p>"},{"location":"contact/#social-professional","title":"Social &amp; Professional","text":""},{"location":"contact/#github","title":"GitHub","text":"<p>github.com/waqasm86</p> <p>Follow my open-source work: - llcuda development - Ubuntu-Cuda-Llama.cpp-Executable - Bug reports and feature requests - Pull requests and contributions</p>"},{"location":"contact/#pypi","title":"PyPI","text":"<p>pypi.org/project/llcuda</p> <p>Official llcuda package: - Latest releases - Version history - Package statistics</p>"},{"location":"contact/#project-specific","title":"Project-Specific","text":""},{"location":"contact/#llcuda","title":"llcuda","text":"<p>Issues &amp; Bugs: github.com/waqasm86/llcuda/issues - Report bugs - Request features - Ask for help</p> <p>Discussions: github.com/waqasm86/llcuda/discussions - General questions - Share use cases - Community support</p> <p>Documentation: waqasm86.github.io/llcuda - Quick start guide - Installation help - Examples and tutorials</p>"},{"location":"contact/#ubuntu-cuda-llamacpp-executable","title":"Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>Issues: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable/issues - Build problems - Compatibility issues - Platform support requests</p>"},{"location":"contact/#what-im-interested-in","title":"What I'm Interested In","text":"<p>I'm particularly interested in hearing from you if you're:</p>"},{"location":"contact/#using-llcuda","title":"Using llcuda","text":"<ul> <li>Share your use case: How are you using llcuda?</li> <li>Benchmark your GPU: What performance are you seeing?</li> <li>Found a bug?: Please report it!</li> <li>Built something cool?: I'd love to hear about it!</li> </ul>"},{"location":"contact/#have-legacy-hardware","title":"Have Legacy Hardware","text":"<ul> <li>Testing on different GPUs: Help expand hardware support</li> <li>Different Linux distros: Testing compatibility</li> <li>Performance data: Share your benchmarks</li> </ul>"},{"location":"contact/#want-to-collaborate","title":"Want to Collaborate","text":"<ul> <li>Windows/macOS support: Port llcuda to new platforms</li> <li>AMD GPU support: ROCm integration</li> <li>Documentation improvements: Make guides even better</li> <li>New features: Implement advanced capabilities</li> </ul>"},{"location":"contact/#learning-or-teaching","title":"Learning or Teaching","text":"<ul> <li>Student projects: Using llcuda for coursework</li> <li>Tutorials or blog posts: Share your knowledge</li> <li>Workshops: Teaching with llcuda</li> <li>Research: Academic applications</li> </ul>"},{"location":"contact/#response-times","title":"Response Times","text":"<p>GitHub Issues: 24-48 hours (usually faster) Email: 24-48 hours for project inquiries Pull Requests: Will review within a week</p> <p>Note: I'm a solo maintainer, so please be patient. I respond to everything!</p>"},{"location":"contact/#reporting-bugs","title":"Reporting Bugs","text":"<p>When reporting bugs, please include:</p> <p>System Information: <pre><code># Python version\npython3 --version\n\n# llcuda version\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n\n# GPU information\nnvidia-smi\n\n# OS information\ncat /etc/os-release\n</code></pre></p> <p>Error Details: - Full error message - Steps to reproduce - Expected vs actual behavior - Minimal code example</p> <p>Example Bug Report: <pre><code>**Title**: CUDA out of memory with Gemma 2 2B on GTX 1050\n\n**System**:\n- llcuda version: 0.1.5\n- Python: 3.10.12\n- GPU: GeForce GTX 1050 (2GB VRAM)\n- OS: Ubuntu 22.04\n\n**Issue**:\nGetting CUDA OOM error when loading Gemma 2 2B model.\n\n**Code**:\nfrom llcuda import LLM\nllm = LLM(model=\"gemma-2-2b-it\")  # Fails here\n\n**Error**:\nRuntimeError: CUDA out of memory...\n\n**Expected**: Should work on 2GB VRAM\n**Actual**: OOM error\n</code></pre></p>"},{"location":"contact/#feature-requests","title":"Feature Requests","text":"<p>I welcome feature requests! Please provide:</p> <p>Use Case: Why do you need this feature? Description: What should it do? Example: How would you use it? Priority: Is this blocking your work?</p> <p>Example Feature Request: <pre><code>**Feature**: Support for Q2_K quantization\n\n**Use Case**: Need to run larger models on 1GB VRAM GPU\n\n**Description**: Add support for Q2_K quantization to fit\nlarger models in limited VRAM.\n\n**Example**:\nllm = LLM(model=\"llama-7b\", quantization=\"Q2_K\")\n\n**Priority**: Nice to have, not blocking\n</code></pre></p>"},{"location":"contact/#collaboration-opportunities","title":"Collaboration Opportunities","text":"<p>Interested in collaborating? I'm looking for:</p>"},{"location":"contact/#code-contributors","title":"Code Contributors","text":"<ul> <li>Windows/macOS support</li> <li>AMD GPU integration (ROCm)</li> <li>Performance optimizations</li> <li>New features</li> </ul>"},{"location":"contact/#technical-writers","title":"Technical Writers","text":"<ul> <li>Tutorial creation</li> <li>Documentation improvements</li> <li>Translation to other languages</li> </ul>"},{"location":"contact/#testers","title":"Testers","text":"<ul> <li>Different GPU models</li> <li>Various Linux distributions</li> <li>Edge cases and stress testing</li> </ul>"},{"location":"contact/#researchers","title":"Researchers","text":"<ul> <li>Academic use cases</li> <li>Performance studies</li> <li>Novel applications</li> </ul>"},{"location":"contact/#commercial-inquiries","title":"Commercial Inquiries","text":"<p>For commercial use, consulting, or custom development:</p> <p>Email: waqasm86@gmail.com</p> <p>Services I offer: - Custom llcuda integrations - Performance optimization for specific hardware - Training and workshops - Technical consulting on GPU computing</p> <p>Note: llcuda is MIT licensed and free to use commercially. No license fees required.</p>"},{"location":"contact/#office-hours","title":"Office Hours","text":"<p>I don't have formal office hours, but I'm most responsive:</p> <p>Timezone: UTC+5 (Pakistan Standard Time) Best times: Weekdays, 9 AM - 6 PM PKT</p> <p>For urgent issues, GitHub issues are usually faster than email.</p>"},{"location":"contact/#community-guidelines","title":"Community Guidelines","text":"<p>When reaching out:</p> <p>Do: - Be respectful and professional - Provide context and details - Search existing issues first - Share your GPU/system specs - Include error messages</p> <p>Don't: - Demand immediate responses - Send duplicate messages across channels - Report security issues publicly (email instead) - Expect free consulting (unless it's a bug)</p>"},{"location":"contact/#stay-updated","title":"Stay Updated","text":"<p>GitHub Watch: Star the repository to get updates GitHub Discussions: Join the community Release Notes: Check PyPI for new versions</p> <p>I announce major updates through: - GitHub releases - PyPI version updates - README updates</p>"},{"location":"contact/#quick-links","title":"Quick Links","text":"<p>llcuda Documentation: waqasm86.github.io/llcuda Quick Start: Get running in 5 minutes Installation Guide: Comprehensive setup Examples: Production code samples</p> <p>GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda Resume: Download PDF</p>"},{"location":"contact/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Before reaching out, check if your question is answered in:</p> <p>Installation Issues: Installation Guide Performance Questions: Performance Guide Usage Examples: Examples General Info: About Me</p>"},{"location":"contact/#thank-you","title":"Thank You","text":"<p>Thank you for your interest in my work! I built llcuda to make LLM development accessible on hardware people already own, and your feedback helps make it better.</p> <p>Whether you're: - A student learning AI - A developer building applications - A researcher exploring LLMs - Someone with an old GPU wanting to experiment</p> <p>I'm here to help.</p> <p>Looking forward to hearing from you!</p> <p>\u2014 Waqas Muhammad</p>"},{"location":"contact/#contact-summary","title":"Contact Summary","text":"<p>Primary Contact: waqasm86@gmail.com</p> <p>GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>For Bugs: GitHub Issues For Discussions: GitHub Discussions For Everything Else: waqasm86@gmail.com</p> <p>Response Time: 24-48 hours Timezone: UTC+5 (PKT)</p>"},{"location":"llcuda/","title":"llcuda: LLM Inference for Legacy NVIDIA GPUs","text":"<p>A Python package that brings large language model inference to old NVIDIA GPUs with zero configuration.</p> <p> </p>"},{"location":"llcuda/#overview","title":"Overview","text":"<p>llcuda is designed to make LLM inference accessible on hardware you already own. No expensive GPU upgrades. No complex compilation. No CUDA toolkit installation. Just <code>pip install llcuda</code> and start running models.</p>"},{"location":"llcuda/#the-challenge","title":"The Challenge","text":"<p>Running large language models typically requires: - Modern NVIDIA RTX GPUs - 8GB+ VRAM - Complex compilation of llama.cpp with CUDA support - CUDA toolkit installation and configuration - Deep understanding of model quantization</p> <p>This creates a huge barrier for: - Students learning AI/ML - Developers with older laptops - Researchers in resource-constrained environments - Anyone with legacy NVIDIA GPUs (GeForce 900/800 series)</p>"},{"location":"llcuda/#the-solution","title":"The Solution","text":"<p>llcuda removes all these barriers:</p> <pre><code>pip install llcuda\npython -m llcuda\n</code></pre> <p>That's it. No compilation. No CUDA toolkit. No configuration. It just works.</p> <p>How? - Pre-built llama.cpp binaries with CUDA 12.6 support - Automatic model downloading from Hugging Face - Optimized for low-VRAM GPUs (tested on 1GB) - Intelligent quantization selection (Q4_K_M by default) - JupyterLab-first design for data science workflows</p>"},{"location":"llcuda/#key-features","title":"Key Features","text":""},{"location":"llcuda/#zero-configuration","title":"Zero Configuration","text":"<p>Install via pip and run immediately. No manual steps, no dependencies, no compilation.</p> <pre><code>pip install llcuda\npython -m llcuda  # Interactive chat\n</code></pre>"},{"location":"llcuda/#legacy-gpu-support","title":"Legacy GPU Support","text":"<p>Tested extensively on GeForce 940M (1GB VRAM, Maxwell architecture). If you have a CUDA-capable NVIDIA GPU from 2014 or later, llcuda will work.</p> <p>Supported GPUs: - GeForce 900 series (940M, 950M, 960M, 970M, 980M) - GeForce 800 series (840M, 850M, 860M) - GeForce GTX 750/750 Ti and newer - Any GPU with compute capability 5.0+ (Maxwell architecture and later)</p>"},{"location":"llcuda/#production-ready","title":"Production Ready","text":"<p>Published to PyPI with semantic versioning. Not a GitHub experiment, but a maintained package you can depend on.</p> <ul> <li>PyPI: pypi.org/project/llcuda</li> <li>GitHub: github.com/waqasm86/llcuda</li> <li>Version: 0.1.x (actively maintained)</li> </ul>"},{"location":"llcuda/#jupyterlab-integration","title":"JupyterLab Integration","text":"<p>First-class support for Jupyter notebooks. Perfect for: - Exploratory data analysis with LLM assistance - Prototyping LLM-powered features - Interactive learning and experimentation - Documentation generation</p> <pre><code>from llcuda import LLM\n\nllm = LLM()\nresponse = llm.chat(\"Explain gradient descent\")\nprint(response)\n</code></pre>"},{"location":"llcuda/#empirical-performance","title":"Empirical Performance","text":"<p>Every performance claim is backed by real measurements on real hardware.</p> <p>GeForce 940M (1GB VRAM) - Gemma 2 2B Q4_K_M: - Speed: ~15 tokens/second - Memory: ~950MB VRAM usage - Context: 2048 tokens - Quality: Coherent, context-aware responses</p> <p>View detailed benchmarks \u2192</p>"},{"location":"llcuda/#how-it-works","title":"How It Works","text":""},{"location":"llcuda/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Your Python Code / Jupyter      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            llcuda (Python)              \u2502\n\u2502  - Model management                     \u2502\n\u2502  - API interface                        \u2502\n\u2502  - Context handling                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Ubuntu-Cuda-Llama.cpp-Executable       \u2502\n\u2502  - Pre-built binary                     \u2502\n\u2502  - CUDA 12.6 support                    \u2502\n\u2502  - Optimized for legacy GPUs            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         NVIDIA GPU (CUDA)               \u2502\n\u2502  - GeForce 940M tested                  \u2502\n\u2502  - 1GB VRAM minimum                     \u2502\n\u2502  - Compute capability 5.0+              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"llcuda/#component-breakdown","title":"Component Breakdown","text":"<p>1. llcuda (Python Package) - High-level Python API for LLM inference - Automatic model downloading from Hugging Face - Context window management - Conversation history tracking - Error handling and recovery</p> <p>2. Ubuntu-Cuda-Llama.cpp-Executable - Pre-compiled llama.cpp with CUDA support - Statically linked (no external dependencies) - Compiled with CUDA 12.6 for Ubuntu 22.04 - Optimized build flags for legacy GPUs</p> <p>3. Model Management - Automatic GGUF model downloading - Smart quantization selection (Q4_K_M default) - Model caching to avoid re-downloads - Support for custom models via Hugging Face IDs</p>"},{"location":"llcuda/#supported-models","title":"Supported Models","text":"<p>llcuda works with any GGUF-format model from Hugging Face. Default recommendations for 1GB VRAM:</p>"},{"location":"llcuda/#recommended-models","title":"Recommended Models","text":"<p>Gemma 2 2B (Default) <pre><code>llm = LLM(model=\"gemma-2-2b-it\")\n</code></pre> - Size: ~1.4GB (Q4_K_M quantization) - Performance: ~15 tok/s on GeForce 940M - Use Case: General chat, Q&amp;A, code assistance - Context: 2048 tokens</p> <p>Llama 3.2 1B <pre><code>llm = LLM(model=\"llama-3.2-1b-instruct\")\n</code></pre> - Size: ~900MB (Q4_K_M quantization) - Performance: ~18 tok/s on GeForce 940M - Use Case: Fast responses, simple tasks - Context: 2048 tokens</p> <p>Qwen 2.5 0.5B <pre><code>llm = LLM(model=\"qwen-2.5-0.5b-instruct\")\n</code></pre> - Size: ~400MB (Q4_K_M quantization) - Performance: ~25 tok/s on GeForce 940M - Use Case: Ultra-fast, basic chat - Context: 2048 tokens</p>"},{"location":"llcuda/#custom-models","title":"Custom Models","text":"<p>Use any GGUF model from Hugging Face:</p> <pre><code>llm = LLM(\n    model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n)\n</code></pre> <p>VRAM Considerations</p> <p>For 1GB VRAM GPUs, stick to models \u22642B parameters with Q4_K_M quantization. Larger models require more VRAM or CPU offloading (which is much slower).</p>"},{"location":"llcuda/#use-cases","title":"Use Cases","text":""},{"location":"llcuda/#interactive-development","title":"Interactive Development","text":"<p>Run LLMs locally during development without cloud API costs:</p> <pre><code>from llcuda import LLM\n\nllm = LLM()\n\n# Generate code\ncode = llm.chat(\"Write a Python function to calculate Fibonacci numbers\")\nprint(code)\n\n# Review code\nreview = llm.chat(f\"Review this code for improvements: {code}\")\nprint(review)\n</code></pre>"},{"location":"llcuda/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>Perfect for data science workflows:</p> <pre><code>import pandas as pd\nfrom llcuda import LLM\n\nllm = LLM()\n\n# Analyze data with LLM assistance\ndf = pd.read_csv(\"sales_data.csv\")\nsummary = df.describe().to_string()\n\nanalysis = llm.chat(f\"Analyze this sales data:\\n{summary}\")\nprint(analysis)\n</code></pre>"},{"location":"llcuda/#learning-experimentation","title":"Learning &amp; Experimentation","text":"<p>Understand LLM behavior without cloud costs:</p> <pre><code>from llcuda import LLM\n\nllm = LLM()\n\n# Test different prompting strategies\nprompts = [\n    \"What is photosynthesis?\",\n    \"Explain photosynthesis to a 5-year-old\",\n    \"You are a biology professor. Explain photosynthesis.\",\n]\n\nfor prompt in prompts:\n    response = llm.chat(prompt)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Response: {response}\\n\")\n</code></pre>"},{"location":"llcuda/#offline-applications","title":"Offline Applications","text":"<p>Build applications that work without internet:</p> <pre><code>from llcuda import LLM\n\nclass OfflineAssistant:\n    def __init__(self):\n        self.llm = LLM(model=\"gemma-2-2b-it\")\n\n    def answer_question(self, question):\n        return self.llm.chat(question)\n\n    def summarize_text(self, text):\n        prompt = f\"Summarize this text:\\n\\n{text}\"\n        return self.llm.chat(prompt)\n\nassistant = OfflineAssistant()\nanswer = assistant.answer_question(\"What is quantum computing?\")\n</code></pre>"},{"location":"llcuda/#installation","title":"Installation","text":"<p>Quick installation:</p> <pre><code>pip install llcuda\n</code></pre> <p>For detailed installation instructions, including system requirements and troubleshooting:</p> <p>View Installation Guide \u2192</p>"},{"location":"llcuda/#quick-start","title":"Quick Start","text":"<p>Get up and running in under 5 minutes:</p> <pre><code># Install\npip install llcuda\n\n# Run interactive chat\npython -m llcuda\n</code></pre> <p>For detailed tutorials and examples:</p> <p>View Quick Start Guide \u2192</p>"},{"location":"llcuda/#performance","title":"Performance","text":"<p>Real-world benchmarks on actual hardware (GeForce 940M, 1GB VRAM):</p> Model Quantization Speed VRAM Context Gemma 2 2B Q4_K_M ~15 tok/s 950MB 2048 Llama 3.2 1B Q4_K_M ~18 tok/s 750MB 2048 Qwen 2.5 0.5B Q4_K_M ~25 tok/s 450MB 2048 <p>For comprehensive benchmarks and optimization tips:</p> <p>View Performance Guide \u2192</p>"},{"location":"llcuda/#examples","title":"Examples","text":"<p>Production-ready code samples for common use cases:</p> <ul> <li>Basic Chat: Simple question-answering</li> <li>Context Management: Multi-turn conversations</li> <li>Custom Models: Loading specific GGUF files</li> <li>JupyterLab Integration: Notebook workflows</li> <li>Batch Processing: Processing multiple inputs</li> <li>Error Handling: Robust production code</li> </ul> <p>View Examples \u2192</p>"},{"location":"llcuda/#philosophy","title":"Philosophy","text":"<p>llcuda is built on these principles:</p>"},{"location":"llcuda/#1-zero-configuration","title":"1. Zero Configuration","text":"<p>Installation should be <code>pip install</code> and done. No manual steps, no compilation, no configuration files.</p>"},{"location":"llcuda/#2-real-hardware-testing","title":"2. Real Hardware Testing","text":"<p>Every performance claim is tested on GeForce 940M (1GB VRAM). No theoretical benchmarks.</p>"},{"location":"llcuda/#3-production-quality","title":"3. Production Quality","text":"<p>Published to PyPI, not just GitHub. Semantic versioning, proper packaging, comprehensive documentation.</p>"},{"location":"llcuda/#4-python-first","title":"4. Python-First","text":"<p>Designed for Python developers and data scientists. Native Jupyter support, Pythonic API.</p>"},{"location":"llcuda/#5-empirical-optimization","title":"5. Empirical Optimization","text":"<p>Optimize based on measurements, not assumptions. Profile on real hardware, fix real bottlenecks.</p>"},{"location":"llcuda/#technical-details","title":"Technical Details","text":""},{"location":"llcuda/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS (tested), likely works on other Linux distros</li> <li>GPU: NVIDIA GPU with compute capability 5.0+ (Maxwell architecture or later)</li> <li>VRAM: 1GB minimum (for 2B models with Q4_K_M quantization)</li> <li>Python: 3.8+</li> <li>CUDA: Not required (pre-built binaries include CUDA runtime)</li> </ul>"},{"location":"llcuda/#dependencies","title":"Dependencies","text":"<ul> <li>Minimal Python dependencies (requests, tqdm)</li> <li>Pre-built llama.cpp binary (no compilation required)</li> <li>No CUDA toolkit installation needed</li> </ul>"},{"location":"llcuda/#package-structure","title":"Package Structure","text":"<pre><code>llcuda/\n\u251c\u2500\u2500 __init__.py          # Public API\n\u251c\u2500\u2500 llm.py              # Main LLM class\n\u251c\u2500\u2500 model_manager.py    # Model downloading/caching\n\u251c\u2500\u2500 llama_wrapper.py    # llama.cpp interface\n\u2514\u2500\u2500 binaries/           # Pre-built executables\n    \u2514\u2500\u2500 llama-cli       # CUDA-enabled llama.cpp\n</code></pre>"},{"location":"llcuda/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Tool Compilation CUDA Required PyPI Legacy GPU Support llcuda No No Yes Excellent llama.cpp Yes Yes No Good llama-cpp-python Yes Yes Yes Good Ollama Yes Yes No Limited vLLM No Yes Yes None <p>llcuda's Advantage: Only solution with zero-configuration setup AND excellent legacy GPU support.</p>"},{"location":"llcuda/#roadmap","title":"Roadmap","text":"<p>Current: v0.1.x - Core functionality, PyPI publication</p> <p>Planned Features: - Support for more quantization formats (Q2_K, Q5_K_M, Q6_K) - Multi-GPU support for older multi-GPU setups - Windows support (pre-built binaries for Windows + CUDA) - Model compression utilities - Fine-tuning helpers for custom models - Advanced context window management (sliding window, RAG integration)</p>"},{"location":"llcuda/#contributing","title":"Contributing","text":"<p>llcuda is open source and welcomes contributions:</p> <ul> <li>Bug Reports: GitHub Issues</li> <li>Feature Requests: GitHub Discussions</li> <li>Pull Requests: GitHub PRs</li> </ul> <p>Areas for Contribution: - Testing on different GPU models - Windows/MacOS support - Additional model integrations - Documentation improvements - Performance optimization</p>"},{"location":"llcuda/#support","title":"Support","text":"<p>Documentation: waqasm86.github.io GitHub: github.com/waqasm86/llcuda PyPI: pypi.org/project/llcuda Email: waqasm86@gmail.com</p>"},{"location":"llcuda/#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"llcuda/#next-steps","title":"Next Steps","text":"<p>Ready to get started?</p> <ol> <li>Quick Start - Get running in 5 minutes</li> <li>Installation - Detailed setup guide</li> <li>Performance - Benchmarks and optimization</li> <li>Examples - Production code samples</li> </ol>"},{"location":"llcuda/examples/","title":"Code Examples","text":"<p>Production-ready code samples for common llcuda use cases. All examples tested on GeForce 940M (1GB VRAM) with Ubuntu 22.04.</p> <p>Table of Contents:</p> <ol> <li>Basic Usage</li> <li>Interactive Chat</li> <li>Context Management</li> <li>Custom Models</li> <li>JupyterLab Integration</li> <li>Batch Processing</li> <li>Error Handling</li> <li>Code Generation</li> <li>Data Analysis</li> <li>Production Patterns</li> </ol>"},{"location":"llcuda/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"llcuda/examples/#hello-world","title":"Hello World","text":"<p>Simplest possible llcuda usage:</p> <pre><code>from llcuda import LLM\n\n# Initialize with defaults\nllm = LLM()\n\n# Ask a question\nresponse = llm.chat(\"What is Python?\")\nprint(response)\n</code></pre> <p>Expected output: <pre><code>Python is a high-level, interpreted programming language known for its\nclear syntax and readability. It's widely used for web development, data\nanalysis, artificial intelligence, automation, and more.\n</code></pre></p>"},{"location":"llcuda/examples/#custom-configuration","title":"Custom Configuration","text":"<p>Full control over model parameters:</p> <pre><code>from llcuda import LLM\n\nllm = LLM(\n    model=\"gemma-2-2b-it\",        # Model selection\n    max_tokens=512,                # Maximum response length\n    temperature=0.7,               # Randomness (0-1)\n    top_p=0.9,                     # Nucleus sampling\n    top_k=40,                      # Top-K sampling\n    repeat_penalty=1.1,            # Penalize repetition\n    context_length=2048,           # Context window\n    verbose=True                   # Show debug info\n)\n\nresponse = llm.chat(\"Tell me a story\")\nprint(response)\n</code></pre>"},{"location":"llcuda/examples/#interactive-chat","title":"Interactive Chat","text":""},{"location":"llcuda/examples/#simple-cli-chat","title":"Simple CLI Chat","text":"<p>Build a basic command-line chat interface:</p> <pre><code>from llcuda import LLM\n\ndef main():\n    print(\"llcuda Chat Interface\")\n    print(\"Type 'quit' to exit\\n\")\n\n    llm = LLM()\n\n    while True:\n        user_input = input(\"You: \").strip()\n\n        if user_input.lower() in ['quit', 'exit', 'q']:\n            print(\"Goodbye!\")\n            break\n\n        if not user_input:\n            continue\n\n        response = llm.chat(user_input)\n        print(f\"AI: {response}\\n\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Usage: <pre><code>python chat.py\n</code></pre></p>"},{"location":"llcuda/examples/#chat-with-history-display","title":"Chat with History Display","text":"<p>Show conversation history:</p> <pre><code>from llcuda import LLM\n\nclass ChatBot:\n    def __init__(self):\n        self.llm = LLM()\n        self.history = []\n\n    def chat(self, message):\n        # Store user message\n        self.history.append({\"role\": \"user\", \"content\": message})\n\n        # Get response\n        response = self.llm.chat(message)\n\n        # Store AI response\n        self.history.append({\"role\": \"assistant\", \"content\": response})\n\n        return response\n\n    def show_history(self):\n        print(\"\\n--- Conversation History ---\")\n        for i, msg in enumerate(self.history, 1):\n            role = msg[\"role\"].upper()\n            content = msg[\"content\"]\n            print(f\"{i}. [{role}]: {content}\\n\")\n\n# Usage\nbot = ChatBot()\n\nbot.chat(\"Hi, my name is Alice\")\nbot.chat(\"What should I learn about Python?\")\nbot.chat(\"What was my name?\")\n\nbot.show_history()\n</code></pre>"},{"location":"llcuda/examples/#context-management","title":"Context Management","text":""},{"location":"llcuda/examples/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<p>llcuda automatically maintains context:</p> <pre><code>from llcuda import LLM\n\nllm = LLM()\n\n# First message sets context\nllm.chat(\"I'm working on a Python project to analyze sales data.\")\n\n# Subsequent messages reference previous context\nresponse1 = llm.chat(\"What libraries should I use?\")\nprint(response1)\n# Mentions pandas, matplotlib, etc. (context-aware)\n\nresponse2 = llm.chat(\"Show me example code\")\nprint(response2)\n# Provides code for sales analysis (remembers topic)\n\nresponse3 = llm.chat(\"How can I optimize it?\")\nprint(response3)\n# Gives optimization tips for sales analysis (full context)\n</code></pre>"},{"location":"llcuda/examples/#manual-context-reset","title":"Manual Context Reset","text":"<p>Clear conversation history when starting a new topic:</p> <pre><code>from llcuda import LLM\n\nllm = LLM()\n\n# First conversation\nllm.chat(\"Tell me about Python\")\nllm.chat(\"What about its history?\")\n\n# Reset for new topic\nllm.reset_conversation()\n\n# New conversation (no memory of Python discussion)\nllm.chat(\"Tell me about JavaScript\")\n</code></pre>"},{"location":"llcuda/examples/#context-window-management","title":"Context Window Management","text":"<p>Handle long conversations that exceed context length:</p> <pre><code>from llcuda import LLM\n\nclass ContextAwareLLM:\n    def __init__(self, context_length=2048):\n        self.llm = LLM(context_length=context_length)\n        self.message_count = 0\n        self.max_messages = 10  # Reset after 10 messages\n\n    def chat(self, message):\n        self.message_count += 1\n\n        # Auto-reset if approaching context limit\n        if self.message_count &gt;= self.max_messages:\n            print(\"[Context reset - starting fresh conversation]\")\n            self.llm.reset_conversation()\n            self.message_count = 0\n\n        return self.llm.chat(message)\n\n# Usage\nllm = ContextAwareLLM()\n\nfor i in range(15):\n    response = llm.chat(f\"Tell me fact #{i} about space\")\n    print(f\"{i}: {response[:50]}...\")\n</code></pre>"},{"location":"llcuda/examples/#custom-models","title":"Custom Models","text":""},{"location":"llcuda/examples/#use-different-models","title":"Use Different Models","text":"<p>Switch between models for different use cases:</p> <pre><code>from llcuda import LLM\n\n# Fast model for quick responses\nfast_llm = LLM(model=\"llama-3.2-1b-instruct\")\nquick_answer = fast_llm.chat(\"What is 2+2?\")\nprint(f\"Fast: {quick_answer}\")\n\n# Quality model for complex tasks\nquality_llm = LLM(model=\"gemma-2-2b-it\")\ndetailed_answer = quality_llm.chat(\"Explain quantum entanglement\")\nprint(f\"Quality: {detailed_answer}\")\n</code></pre>"},{"location":"llcuda/examples/#load-custom-gguf-model","title":"Load Custom GGUF Model","text":"<p>Use any GGUF model from Hugging Face:</p> <pre><code>from llcuda import LLM\n\n# Load custom model\nllm = LLM(\n    model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n)\n\nresponse = llm.chat(\"Hello!\")\nprint(response)\n</code></pre> <p>VRAM Requirements</p> <p>7B models require ~4GB VRAM. Won't fit on 1GB GPUs like GeForce 940M.</p>"},{"location":"llcuda/examples/#model-comparison-script","title":"Model Comparison Script","text":"<p>Compare outputs from different models:</p> <pre><code>from llcuda import LLM\n\nmodels = [\n    \"gemma-2-2b-it\",\n    \"llama-3.2-1b-instruct\",\n    \"qwen-2.5-0.5b-instruct\"\n]\n\nprompt = \"Explain machine learning in one sentence\"\n\nprint(f\"Prompt: {prompt}\\n\")\n\nfor model_name in models:\n    llm = LLM(model=model_name)\n    response = llm.chat(prompt)\n    print(f\"{model_name}:\")\n    print(f\"  {response}\\n\")\n</code></pre>"},{"location":"llcuda/examples/#jupyterlab-integration","title":"JupyterLab Integration","text":""},{"location":"llcuda/examples/#basic-jupyter-usage","title":"Basic Jupyter Usage","text":"<p>Perfect for exploratory data analysis:</p> <pre><code># Cell 1: Setup\nfrom llcuda import LLM\nllm = LLM(model=\"gemma-2-2b-it\")\n\n# Cell 2: Ask questions interactively\nresponse = llm.chat(\"What is gradient descent?\")\nprint(response)\n\n# Cell 3: Follow-up (maintains context)\nresponse = llm.chat(\"Give me a Python example\")\nprint(response)\n\n# Cell 4: Test the code\nexec(response.split(\"```python\")[1].split(\"```\")[0])\n</code></pre>"},{"location":"llcuda/examples/#data-analysis-in-jupyter","title":"Data Analysis in Jupyter","text":"<p>Integrate with pandas workflows:</p> <pre><code>import pandas as pd\nfrom llcuda import LLM\n\n# Load data\ndf = pd.read_csv(\"sales_data.csv\")\n\n# Initialize LLM\nllm = LLM()\n\n# Get LLM insights on data\nsummary = df.describe().to_string()\ninsights = llm.chat(f\"Analyze this sales data:\\n{summary}\")\nprint(insights)\n\n# Follow-up questions\nquestion = \"What trends do you see?\"\nresponse = llm.chat(question)\nprint(response)\n</code></pre>"},{"location":"llcuda/examples/#generate-visualization-code","title":"Generate Visualization Code","text":"<p>Let LLM help with plotting:</p> <pre><code>import pandas as pd\nfrom llcuda import LLM\n\ndf = pd.read_csv(\"sales.csv\")\nllm = LLM()\n\n# Ask for visualization code\ncode = llm.chat(f\"\"\"\nGiven this DataFrame with columns {list(df.columns)},\nwrite Python code using matplotlib to create a bar chart\nof sales by category.\n\"\"\")\n\nprint(code)\n\n# Execute the code\nexec(code.split(\"```python\")[1].split(\"```\")[0])\n</code></pre>"},{"location":"llcuda/examples/#jupyter-magic-command-advanced","title":"Jupyter Magic Command (Advanced)","text":"<p>Create a custom Jupyter magic for llcuda:</p> <pre><code># In a notebook cell\nfrom IPython.core.magic import register_line_magic\nfrom llcuda import LLM\n\n# Initialize LLM once\n_llm = LLM()\n\n@register_line_magic\ndef ask(line):\n    \"\"\"Magic command: %ask &lt;your question&gt;\"\"\"\n    response = _llm.chat(line)\n    print(response)\n    return response\n\n# Usage in subsequent cells:\n# %ask What is machine learning?\n# %ask Explain neural networks\n</code></pre>"},{"location":"llcuda/examples/#batch-processing","title":"Batch Processing","text":""},{"location":"llcuda/examples/#process-multiple-inputs","title":"Process Multiple Inputs","text":"<p>Efficient batch processing pattern:</p> <pre><code>from llcuda import LLM\n\ninputs = [\n    \"Summarize: Python is a programming language...\",\n    \"Summarize: Machine learning is a subset of AI...\",\n    \"Summarize: Data science involves analyzing data...\",\n]\n\nllm = LLM(model=\"llama-3.2-1b-instruct\")  # Use fast model\n\nresults = []\nfor i, input_text in enumerate(inputs, 1):\n    print(f\"Processing {i}/{len(inputs)}...\")\n    result = llm.chat(input_text)\n    results.append(result)\n\n# Display results\nfor i, result in enumerate(results, 1):\n    print(f\"\\nResult {i}:\")\n    print(result)\n</code></pre>"},{"location":"llcuda/examples/#parallel-processing-multi-model","title":"Parallel Processing (Multi-Model)","text":"<p>Use multiple model instances for parallelization:</p> <pre><code>from llcuda import LLM\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef process_with_llm(text):\n    \"\"\"Each thread gets its own LLM instance\"\"\"\n    llm = LLM(model=\"llama-3.2-1b-instruct\")\n    return llm.chat(f\"Summarize: {text}\")\n\ntexts = [\n    \"Python is a programming language...\",\n    \"Machine learning is a subset of AI...\",\n    \"Data science involves analyzing data...\",\n]\n\n# Process in parallel (if you have VRAM for multiple models)\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = list(executor.map(process_with_llm, texts))\n\nfor i, result in enumerate(results, 1):\n    print(f\"\\nResult {i}: {result}\")\n</code></pre> <p>VRAM Limitations</p> <p>Multiple LLM instances require VRAM for each. On 1GB GPUs, use sequential processing instead.</p>"},{"location":"llcuda/examples/#error-handling","title":"Error Handling","text":""},{"location":"llcuda/examples/#robust-error-handling","title":"Robust Error Handling","text":"<p>Production-ready error handling:</p> <pre><code>from llcuda import LLM\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef safe_chat(prompt, max_retries=3):\n    \"\"\"Chat with retry logic\"\"\"\n    llm = None\n\n    for attempt in range(max_retries):\n        try:\n            if llm is None:\n                llm = LLM()\n\n            response = llm.chat(prompt)\n            return response\n\n        except MemoryError:\n            logger.error(\"CUDA out of memory. Try smaller model.\")\n            return None\n\n        except Exception as e:\n            logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt &lt; max_retries - 1:\n                logger.info(\"Retrying...\")\n                llm = None  # Reinitialize\n            else:\n                logger.error(\"Max retries reached\")\n                return None\n\n# Usage\nresponse = safe_chat(\"What is Python?\")\nif response:\n    print(response)\nelse:\n    print(\"Failed to get response\")\n</code></pre>"},{"location":"llcuda/examples/#timeout-handling","title":"Timeout Handling","text":"<p>Add timeout for long-running generations:</p> <pre><code>from llcuda import LLM\nimport signal\nfrom contextlib import contextmanager\n\nclass TimeoutError(Exception):\n    pass\n\n@contextmanager\ndef timeout(seconds):\n    def timeout_handler(signum, frame):\n        raise TimeoutError(f\"Timed out after {seconds} seconds\")\n\n    signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(seconds)\n    try:\n        yield\n    finally:\n        signal.alarm(0)\n\n# Usage\nllm = LLM()\n\ntry:\n    with timeout(30):  # 30 second timeout\n        response = llm.chat(\"Write a very long essay...\")\n        print(response)\nexcept TimeoutError as e:\n    print(f\"Generation timed out: {e}\")\n</code></pre>"},{"location":"llcuda/examples/#code-generation","title":"Code Generation","text":""},{"location":"llcuda/examples/#generate-and-execute-code","title":"Generate and Execute Code","text":"<p>Safe code generation with execution:</p> <pre><code>from llcuda import LLM\n\ndef generate_and_test_code(task):\n    llm = LLM()\n\n    # Generate code\n    prompt = f\"Write a Python function to {task}. Only output the code, no explanations.\"\n    response = llm.chat(prompt)\n\n    # Extract code block\n    if \"```python\" in response:\n        code = response.split(\"```python\")[1].split(\"```\")[0]\n    else:\n        code = response\n\n    print(\"Generated code:\")\n    print(code)\n\n    # Test execution (be careful in production!)\n    try:\n        exec(code, globals())\n        print(\"\\nCode executed successfully!\")\n        return code\n    except Exception as e:\n        print(f\"\\nExecution error: {e}\")\n        return None\n\n# Usage\ngenerate_and_test_code(\"calculate fibonacci numbers\")\n</code></pre>"},{"location":"llcuda/examples/#code-review-assistant","title":"Code Review Assistant","text":"<p>Get LLM feedback on code:</p> <pre><code>from llcuda import LLM\n\ndef review_code(code):\n    llm = LLM(model=\"gemma-2-2b-it\")\n\n    prompt = f\"\"\"\nReview this Python code for:\n1. Correctness\n2. Performance\n3. Best practices\n4. Potential bugs\n\nCode:\n{code}\n\"\"\"\n\n    review = llm.chat(prompt)\n    return review\n\n# Usage\nmy_code = \"\"\"\ndef fibonacci(n):\n    if n &lt;= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\"\"\"\n\nreview = review_code(my_code)\nprint(review)\n</code></pre>"},{"location":"llcuda/examples/#data-analysis","title":"Data Analysis","text":""},{"location":"llcuda/examples/#automated-data-insights","title":"Automated Data Insights","text":"<p>Generate insights from pandas DataFrames:</p> <pre><code>import pandas as pd\nfrom llcuda import LLM\n\ndef analyze_dataframe(df, question=None):\n    llm = LLM()\n\n    # Generate summary\n    summary = f\"\"\"\nDataset Shape: {df.shape}\nColumns: {list(df.columns)}\nSummary Statistics:\n{df.describe().to_string()}\n\nFirst few rows:\n{df.head().to_string()}\n\"\"\"\n\n    # Ask LLM for insights\n    if question:\n        prompt = f\"{summary}\\n\\nQuestion: {question}\"\n    else:\n        prompt = f\"{summary}\\n\\nProvide key insights and recommendations.\"\n\n    insights = llm.chat(prompt)\n    return insights\n\n# Usage\ndf = pd.read_csv(\"sales_data.csv\")\ninsights = analyze_dataframe(df, \"What are the main trends?\")\nprint(insights)\n</code></pre>"},{"location":"llcuda/examples/#generate-sql-queries","title":"Generate SQL Queries","text":"<p>Natural language to SQL:</p> <pre><code>from llcuda import LLM\n\ndef nl_to_sql(natural_language, schema):\n    llm = LLM()\n\n    prompt = f\"\"\"\nGiven this database schema:\n{schema}\n\nConvert this natural language query to SQL:\n\"{natural_language}\"\n\nOutput only the SQL query, no explanations.\n\"\"\"\n\n    sql = llm.chat(prompt)\n    return sql.strip()\n\n# Usage\nschema = \"\"\"\nTable: employees\nColumns: id, name, department, salary, hire_date\n\"\"\"\n\nquery = nl_to_sql(\"Find all employees hired after 2020 with salary &gt; 50000\", schema)\nprint(query)\n</code></pre>"},{"location":"llcuda/examples/#production-patterns","title":"Production Patterns","text":""},{"location":"llcuda/examples/#singleton-llm-instance","title":"Singleton LLM Instance","text":"<p>Reuse LLM instance across application:</p> <pre><code>from llcuda import LLM\n\nclass LLMSingleton:\n    _instance = None\n\n    @classmethod\n    def get_instance(cls):\n        if cls._instance is None:\n            cls._instance = LLM(model=\"gemma-2-2b-it\")\n        return cls._instance\n\n# Usage across multiple modules\nllm = LLMSingleton.get_instance()\nresponse = llm.chat(\"Hello\")\n</code></pre>"},{"location":"llcuda/examples/#cached-responses","title":"Cached Responses","text":"<p>Cache LLM responses to avoid redundant generation:</p> <pre><code>from llcuda import LLM\nfrom functools import lru_cache\n\nclass CachedLLM:\n    def __init__(self):\n        self.llm = LLM()\n\n    @lru_cache(maxsize=128)\n    def chat(self, message):\n        \"\"\"Cached chat - same input returns cached response\"\"\"\n        return self.llm.chat(message)\n\n# Usage\nllm = CachedLLM()\n\n# First call: generates response\nresponse1 = llm.chat(\"What is Python?\")\n\n# Second call: returns cached response (instant)\nresponse2 = llm.chat(\"What is Python?\")\n\nassert response1 == response2\n</code></pre>"},{"location":"llcuda/examples/#rate-limiting","title":"Rate Limiting","text":"<p>Limit requests per minute:</p> <pre><code>from llcuda import LLM\nimport time\nfrom collections import deque\n\nclass RateLimitedLLM:\n    def __init__(self, max_requests_per_minute=10):\n        self.llm = LLM()\n        self.max_requests = max_requests_per_minute\n        self.request_times = deque()\n\n    def chat(self, message):\n        # Remove requests older than 1 minute\n        current_time = time.time()\n        while self.request_times and current_time - self.request_times[0] &gt; 60:\n            self.request_times.popleft()\n\n        # Check rate limit\n        if len(self.request_times) &gt;= self.max_requests:\n            wait_time = 60 - (current_time - self.request_times[0])\n            print(f\"Rate limit reached. Waiting {wait_time:.1f}s...\")\n            time.sleep(wait_time)\n            return self.chat(message)\n\n        # Process request\n        self.request_times.append(current_time)\n        return self.llm.chat(message)\n\n# Usage\nllm = RateLimitedLLM(max_requests_per_minute=5)\n\nfor i in range(10):\n    response = llm.chat(f\"Request #{i}\")\n    print(f\"{i}: {response[:50]}...\")\n</code></pre>"},{"location":"llcuda/examples/#logging-and-monitoring","title":"Logging and Monitoring","text":"<p>Track LLM usage and performance:</p> <pre><code>from llcuda import LLM\nimport time\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\nclass MonitoredLLM:\n    def __init__(self):\n        self.llm = LLM()\n        self.request_count = 0\n        self.total_time = 0\n\n    def chat(self, message):\n        self.request_count += 1\n        logger.info(f\"Request #{self.request_count}: {message[:50]}...\")\n\n        start = time.time()\n        response = self.llm.chat(message)\n        elapsed = time.time() - start\n\n        self.total_time += elapsed\n\n        logger.info(f\"Response generated in {elapsed:.2f}s\")\n        logger.info(f\"Average time: {self.total_time / self.request_count:.2f}s\")\n\n        return response\n\n# Usage\nllm = MonitoredLLM()\nresponse = llm.chat(\"What is machine learning?\")\n</code></pre>"},{"location":"llcuda/examples/#complete-application-example","title":"Complete Application Example","text":""},{"location":"llcuda/examples/#chatbot-with-all-best-practices","title":"Chatbot with All Best Practices","text":"<p>Full production-ready chatbot:</p> <pre><code>from llcuda import LLM\nimport logging\nimport time\nfrom functools import lru_cache\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ProductionChatbot:\n    def __init__(self, model=\"gemma-2-2b-it\"):\n        self.llm = None\n        self.model = model\n        self.conversation_count = 0\n        self.initialize()\n\n    def initialize(self):\n        \"\"\"Initialize LLM with error handling\"\"\"\n        try:\n            logger.info(f\"Initializing {self.model}...\")\n            self.llm = LLM(model=self.model)\n            logger.info(\"LLM initialized successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize LLM: {e}\")\n            raise\n\n    def chat(self, message, max_retries=3):\n        \"\"\"Chat with retry logic and monitoring\"\"\"\n        self.conversation_count += 1\n\n        for attempt in range(max_retries):\n            try:\n                logger.info(f\"Processing message #{self.conversation_count}\")\n                start = time.time()\n\n                response = self.llm.chat(message)\n\n                elapsed = time.time() - start\n                tokens = len(response.split())\n                speed = tokens / elapsed\n\n                logger.info(f\"Response: {tokens} tokens in {elapsed:.1f}s ({speed:.1f} tok/s)\")\n\n                return response\n\n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n                if attempt &lt; max_retries - 1:\n                    logger.info(\"Retrying...\")\n                    time.sleep(1)\n                else:\n                    logger.error(\"Max retries reached\")\n                    return None\n\n    def reset(self):\n        \"\"\"Reset conversation\"\"\"\n        logger.info(\"Resetting conversation\")\n        self.llm.reset_conversation()\n        self.conversation_count = 0\n\n# Usage\nif __name__ == \"__main__\":\n    bot = ProductionChatbot()\n\n    print(\"Chatbot ready! Type 'quit' to exit.\\n\")\n\n    while True:\n        user_input = input(\"You: \").strip()\n\n        if user_input.lower() in ['quit', 'exit']:\n            print(\"Goodbye!\")\n            break\n\n        if user_input.lower() == 'reset':\n            bot.reset()\n            continue\n\n        response = bot.chat(user_input)\n\n        if response:\n            print(f\"Bot: {response}\\n\")\n        else:\n            print(\"Sorry, I encountered an error. Please try again.\\n\")\n</code></pre>"},{"location":"llcuda/examples/#next-steps","title":"Next Steps","text":"<p>Explore more:</p> <ol> <li>Performance Guide - Optimize for your GPU</li> <li>Installation Guide - Advanced setup</li> <li>Main Documentation - Full API reference</li> </ol> <p>Happy coding with llcuda!</p>"},{"location":"llcuda/installation/","title":"Installation Guide","text":"<p>Comprehensive installation instructions for llcuda, including system requirements, installation methods, verification, and troubleshooting.</p>"},{"location":"llcuda/installation/#system-requirements","title":"System Requirements","text":""},{"location":"llcuda/installation/#operating-system","title":"Operating System","text":"<p>Supported: - Ubuntu 22.04 LTS (tested and recommended) - Ubuntu 20.04 LTS (should work) - Debian 11+ (should work) - Other Linux distributions (may work, not tested)</p> <p>Not Currently Supported: - Windows (planned for future release) - macOS (not planned - limited CUDA support)</p>"},{"location":"llcuda/installation/#hardware-requirements","title":"Hardware Requirements","text":"<p>GPU Requirements: - NVIDIA GPU with CUDA support - Compute capability 5.0 or higher (Maxwell architecture or later) - Minimum 1GB VRAM (for 2B parameter models with Q4_K_M quantization)</p> <p>Tested GPUs: - GeForce 940M (1GB VRAM) - Primary test platform - GeForce GTX 1050 (2GB VRAM) - GeForce GTX 1650 (4GB VRAM)</p> <p>Supported GPU Families: - GeForce 900 series (940M, 950M, 960M, 970M, 980M, GTX 950, GTX 960, GTX 970, GTX 980, GTX 980 Ti) - GeForce 800 series (840M, 850M, 860M, 870M, 880M) - GeForce GTX 750/750 Ti and all newer models - Quadro K-series (K620, K1200, K2200, etc.) and newer - Tesla K-series and newer</p> <p>Check Your GPU: <pre><code># List NVIDIA GPUs\nlspci | grep -i nvidia\n\n# Check compute capability\nnvidia-smi --query-gpu=compute_cap --format=csv\n</code></pre></p>"},{"location":"llcuda/installation/#software-requirements","title":"Software Requirements","text":"<p>Python: - Python 3.8 or newer - pip (Python package manager)</p> <p>NVIDIA Drivers: - NVIDIA driver version 450.80.02 or newer - No CUDA toolkit installation required (llcuda includes pre-built binaries)</p> <p>Check Your Python Version: <pre><code>python3 --version\npip3 --version\n</code></pre></p> <p>Check Your NVIDIA Driver: <pre><code>nvidia-smi\n</code></pre></p>"},{"location":"llcuda/installation/#disk-space","title":"Disk Space","text":"<ul> <li>Package: ~50MB</li> <li>Models: 400MB - 2GB per model (cached in <code>~/.cache/llcuda/</code>)</li> <li>Recommended free space: 5GB minimum</li> </ul>"},{"location":"llcuda/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"llcuda/installation/#method-1-pip-recommended","title":"Method 1: pip (Recommended)","text":"<p>The easiest and recommended way to install llcuda:</p> <pre><code># Install from PyPI\npip install llcuda\n\n# Or with pip3 explicitly\npip3 install llcuda\n\n# Install for current user only (no sudo needed)\npip install --user llcuda\n</code></pre> <p>Verify Installation: <pre><code>python -c \"import llcuda; print(llcuda.__version__)\"\n</code></pre></p>"},{"location":"llcuda/installation/#method-2-pip-with-virtual-environment","title":"Method 2: pip with Virtual Environment","text":"<p>For isolated installation (recommended for development):</p> <pre><code># Create virtual environment\npython3 -m venv llcuda-env\n\n# Activate virtual environment\nsource llcuda-env/bin/activate\n\n# Install llcuda\npip install llcuda\n\n# Verify\npython -c \"import llcuda; print(llcuda.__version__)\"\n</code></pre> <p>To use llcuda later: <pre><code>source llcuda-env/bin/activate\npython -m llcuda\n</code></pre></p>"},{"location":"llcuda/installation/#method-3-install-from-github-development","title":"Method 3: Install from GitHub (Development)","text":"<p>For the latest development version:</p> <pre><code># Clone repository\ngit clone https://github.com/waqasm86/llcuda.git\ncd llcuda\n\n# Install in development mode\npip install -e .\n\n# Verify\npython -c \"import llcuda; print(llcuda.__version__)\"\n</code></pre>"},{"location":"llcuda/installation/#method-4-jupytercolab-installation","title":"Method 4: Jupyter/Colab Installation","text":"<p>For use in Jupyter notebooks:</p> <pre><code># In a Jupyter notebook cell\n!pip install llcuda\n\n# Verify\nimport llcuda\nprint(llcuda.__version__)\n</code></pre>"},{"location":"llcuda/installation/#post-installation-setup","title":"Post-Installation Setup","text":""},{"location":"llcuda/installation/#verify-gpu-detection","title":"Verify GPU Detection","text":"<p>Check that llcuda can detect your GPU:</p> <pre><code>from llcuda import LLM\n\nllm = LLM(verbose=True)\n# Should print: \"Using GPU: [Your GPU Name]\"\n</code></pre>"},{"location":"llcuda/installation/#download-default-model","title":"Download Default Model","text":"<p>llcuda downloads models automatically on first use, but you can pre-download:</p> <pre><code>from llcuda import LLM\n\n# This will download Gemma 2 2B (~1.4GB)\nllm = LLM(model=\"gemma-2-2b-it\")\nprint(\"Model downloaded and cached\")\n</code></pre> <p>Model cache location: <code>~/.cache/llcuda/models/</code></p>"},{"location":"llcuda/installation/#verify-installation","title":"Verify Installation","text":"<p>Run the comprehensive verification script:</p> <pre><code>from llcuda import LLM\nimport time\n\nprint(\"llcuda Installation Verification\")\nprint(\"=\" * 50)\n\n# 1. Check version\nimport llcuda\nprint(f\"\u2713 llcuda version: {llcuda.__version__}\")\n\n# 2. Check GPU\nimport subprocess\nresult = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'],\n                       capture_output=True, text=True)\nprint(f\"\u2713 GPU: {result.stdout.strip()}\")\n\n# 3. Initialize LLM\nprint(\"\\nInitializing LLM...\")\nstart = time.time()\nllm = LLM(model=\"gemma-2-2b-it\", verbose=False)\nprint(f\"\u2713 Model loaded in {time.time() - start:.1f}s\")\n\n# 4. Test inference\nprint(\"\\nTesting inference...\")\nstart = time.time()\nresponse = llm.chat(\"Say 'Hello World'\")\nelapsed = time.time() - start\ntokens = len(response.split())\nspeed = tokens / elapsed\nprint(f\"\u2713 Generated response in {elapsed:.1f}s\")\nprint(f\"\u2713 Speed: {speed:.1f} tokens/second\")\n\n# 5. Test context\nprint(\"\\nTesting context retention...\")\nllm.chat(\"My favorite number is 42\")\nresponse = llm.chat(\"What is my favorite number?\")\nif \"42\" in response:\n    print(\"\u2713 Context retention working\")\nelse:\n    print(\"\u2717 Context retention issue\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Installation verified successfully!\")\nprint(\"\\nNext steps:\")\nprint(\"1. Run interactive chat: python -m llcuda\")\nprint(\"2. View examples: https://waqasm86.github.io/llcuda/examples/\")\n</code></pre>"},{"location":"llcuda/installation/#configuration","title":"Configuration","text":"<p>llcuda works with zero configuration, but you can customize behavior:</p>"},{"location":"llcuda/installation/#environment-variables","title":"Environment Variables","text":"<pre><code># Set custom model cache directory\nexport LLCUDA_MODEL_DIR=/path/to/models\n\n# Disable GPU (CPU only, very slow)\nexport LLCUDA_USE_GPU=0\n\n# Set default model\nexport LLCUDA_DEFAULT_MODEL=llama-3.2-1b-instruct\n</code></pre>"},{"location":"llcuda/installation/#config-file-optional","title":"Config File (Optional)","text":"<p>Create <code>~/.llcuda/config.json</code>:</p> <pre><code>{\n  \"default_model\": \"gemma-2-2b-it\",\n  \"model_dir\": \"~/.cache/llcuda/models\",\n  \"max_tokens\": 512,\n  \"temperature\": 0.7,\n  \"context_length\": 2048,\n  \"gpu_layers\": -1\n}\n</code></pre>"},{"location":"llcuda/installation/#upgrading","title":"Upgrading","text":""},{"location":"llcuda/installation/#upgrade-to-latest-version","title":"Upgrade to Latest Version","text":"<pre><code>pip install --upgrade llcuda\n</code></pre>"},{"location":"llcuda/installation/#upgrade-to-specific-version","title":"Upgrade to Specific Version","text":"<pre><code>pip install llcuda==0.1.5\n</code></pre>"},{"location":"llcuda/installation/#check-current-version","title":"Check Current Version","text":"<pre><code>pip show llcuda\n</code></pre> <p>Or in Python: <pre><code>import llcuda\nprint(llcuda.__version__)\n</code></pre></p>"},{"location":"llcuda/installation/#uninstalling","title":"Uninstalling","text":""},{"location":"llcuda/installation/#remove-package","title":"Remove Package","text":"<pre><code>pip uninstall llcuda\n</code></pre>"},{"location":"llcuda/installation/#remove-cached-models","title":"Remove Cached Models","text":"<pre><code>rm -rf ~/.cache/llcuda\n</code></pre>"},{"location":"llcuda/installation/#remove-configuration","title":"Remove Configuration","text":"<pre><code>rm -rf ~/.llcuda\n</code></pre>"},{"location":"llcuda/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llcuda/installation/#issue-no-module-named-llcuda","title":"Issue: \"No module named 'llcuda'\"","text":"<p>Cause: llcuda not installed or wrong Python environment</p> <p>Solution: <pre><code># Reinstall\npip install llcuda\n\n# Or check which Python you're using\nwhich python\nwhich pip\n\n# Use explicit pip3/python3\npip3 install llcuda\npython3 -c \"import llcuda\"\n</code></pre></p>"},{"location":"llcuda/installation/#issue-cuda-out-of-memory","title":"Issue: \"CUDA out of memory\"","text":"<p>Cause: Model too large for your GPU's VRAM</p> <p>Solutions:</p> <ol> <li> <p>Use a smaller model: <pre><code># Instead of Gemma 2 2B\nllm = LLM(model=\"gemma-2-2b-it\")\n\n# Try Llama 3.2 1B\nllm = LLM(model=\"llama-3.2-1b-instruct\")\n\n# Or Qwen 2.5 0.5B\nllm = LLM(model=\"qwen-2.5-0.5b-instruct\")\n</code></pre></p> </li> <li> <p>Reduce context length: <pre><code>llm = LLM(context_length=1024)  # Default is 2048\n</code></pre></p> </li> <li> <p>Offload layers to CPU (slower): <pre><code>llm = LLM(gpu_layers=16)  # Default is -1 (all on GPU)\n</code></pre></p> </li> </ol>"},{"location":"llcuda/installation/#issue-nvidia-driver-not-found","title":"Issue: \"NVIDIA driver not found\"","text":"<p>Cause: NVIDIA driver not installed or too old</p> <p>Solution: <pre><code># Check if driver is installed\nnvidia-smi\n\n# If not installed, install NVIDIA driver\nsudo ubuntu-drivers autoinstall\n\n# Or install specific version\nsudo apt install nvidia-driver-535\n\n# Reboot\nsudo reboot\n</code></pre></p>"},{"location":"llcuda/installation/#issue-model-download-failed","title":"Issue: \"Model download failed\"","text":"<p>Cause: Network issue or Hugging Face API limit</p> <p>Solutions:</p> <ol> <li> <p>Retry with force download: <pre><code>llm = LLM(model=\"gemma-2-2b-it\", force_download=True)\n</code></pre></p> </li> <li> <p>Manual download: <pre><code># Download model manually\ncd ~/.cache/llcuda/models\nwget https://huggingface.co/google/gemma-2-2b-it-GGUF/resolve/main/gemma-2-2b-it.Q4_K_M.gguf\n</code></pre></p> </li> <li> <p>Check internet connection and Hugging Face status: <pre><code>curl -I https://huggingface.co\n</code></pre></p> </li> </ol>"},{"location":"llcuda/installation/#issue-slow-generation-5-tokenssecond","title":"Issue: \"Slow generation (&lt; 5 tokens/second)\"","text":"<p>Cause: Model running on CPU instead of GPU</p> <p>Diagnosis: <pre><code>from llcuda import LLM\n\nllm = LLM(verbose=True)\n# Should show \"Using GPU: [Your GPU Name]\"\n# If shows \"Using CPU\", GPU not detected\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Verify NVIDIA driver: <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Check GPU compute capability: <pre><code>nvidia-smi --query-gpu=compute_cap --format=csv\n# Should be &gt;= 5.0\n</code></pre></p> </li> <li> <p>Reinstall llcuda: <pre><code>pip uninstall llcuda\npip install --no-cache-dir llcuda\n</code></pre></p> </li> </ol>"},{"location":"llcuda/installation/#issue-permission-denied","title":"Issue: \"Permission denied\"","text":"<p>Cause: No write permission for model cache directory</p> <p>Solution: <pre><code># Fix permissions\nmkdir -p ~/.cache/llcuda\nchmod -R u+w ~/.cache/llcuda\n\n# Or install to custom directory\nexport LLCUDA_MODEL_DIR=/tmp/llcuda_models\n</code></pre></p>"},{"location":"llcuda/installation/#issue-segmentation-fault","title":"Issue: \"Segmentation fault\"","text":"<p>Cause: Incompatible CUDA version or corrupted binary</p> <p>Solutions:</p> <ol> <li> <p>Check CUDA version: <pre><code>nvidia-smi | grep \"CUDA Version\"\n# Should be 12.0 or higher\n</code></pre></p> </li> <li> <p>Update NVIDIA driver: <pre><code>sudo ubuntu-drivers autoinstall\nsudo reboot\n</code></pre></p> </li> <li> <p>Reinstall llcuda: <pre><code>pip uninstall llcuda\npip install --no-cache-dir llcuda\n</code></pre></p> </li> </ol>"},{"location":"llcuda/installation/#issue-importerror-cannot-import-name-llm","title":"Issue: \"ImportError: cannot import name 'LLM'\"","text":"<p>Cause: Old version of llcuda or conflicting package</p> <p>Solution: <pre><code># Uninstall all versions\npip uninstall llcuda -y\n\n# Clear pip cache\npip cache purge\n\n# Reinstall latest\npip install llcuda\n\n# Verify\npython -c \"from llcuda import LLM; print('Success')\"\n</code></pre></p>"},{"location":"llcuda/installation/#issue-response-is-gibberish-or-nonsensical","title":"Issue: \"Response is gibberish or nonsensical\"","text":"<p>Cause: Corrupted model file or wrong quantization</p> <p>Solution: <pre><code># Remove corrupted model cache\nrm -rf ~/.cache/llcuda/models/*\n\n# Re-download\npython -c \"from llcuda import LLM; llm = LLM(force_download=True)\"\n</code></pre></p>"},{"location":"llcuda/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"llcuda/installation/#ubuntu-2204-recommended","title":"Ubuntu 22.04 (Recommended)","text":"<p>Fully supported and tested. No special configuration needed.</p> <pre><code># Install dependencies (if needed)\nsudo apt update\nsudo apt install python3 python3-pip\n\n# Install llcuda\npip3 install llcuda\n</code></pre>"},{"location":"llcuda/installation/#ubuntu-2004","title":"Ubuntu 20.04","text":"<p>Should work with default settings. If issues:</p> <pre><code># Update Python to 3.8+\nsudo apt update\nsudo apt install python3.8 python3-pip\n</code></pre>"},{"location":"llcuda/installation/#other-linux-distributions","title":"Other Linux Distributions","text":"<p>May work but not officially tested. Requirements: - NVIDIA driver 450.80.02+ - Python 3.8+ - GLIBC 2.31+ (check with <code>ldd --version</code>)</p>"},{"location":"llcuda/installation/#wsl2-windows-subsystem-for-linux","title":"WSL2 (Windows Subsystem for Linux)","text":"<p>Not officially supported. CUDA support in WSL2 is experimental.</p>"},{"location":"llcuda/installation/#docker","title":"Docker","text":"<p>Not yet officially supported. Coming in future release.</p>"},{"location":"llcuda/installation/#developer-installation","title":"Developer Installation","text":"<p>For contributing to llcuda:</p> <pre><code># Clone repository\ngit clone https://github.com/waqasm86/llcuda.git\ncd llcuda\n\n# Create virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n\n# Install development dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest tests/\n\n# Run linting\nflake8 llcuda/\nblack llcuda/\n</code></pre>"},{"location":"llcuda/installation/#getting-help","title":"Getting Help","text":"<p>If you're still having issues:</p> <ol> <li>Check existing issues: GitHub Issues</li> <li>Search documentation: waqasm86.github.io</li> <li>Ask the community: GitHub Discussions</li> <li>Contact directly: waqasm86@gmail.com</li> </ol> <p>When reporting issues, include: - llcuda version (<code>python -c \"import llcuda; print(llcuda.__version__)\"</code>) - Python version (<code>python --version</code>) - GPU model (<code>nvidia-smi</code>) - Operating system (<code>cat /etc/os-release</code>) - Complete error message</p>"},{"location":"llcuda/installation/#next-steps","title":"Next Steps","text":"<p>Installation complete! Now:</p> <ol> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Performance Guide - Optimize for your GPU</li> <li>Examples - Production code samples</li> </ol>"},{"location":"llcuda/installation/#quick-reference","title":"Quick Reference","text":""},{"location":"llcuda/installation/#installation","title":"Installation","text":"<pre><code>pip install llcuda\n</code></pre>"},{"location":"llcuda/installation/#verification","title":"Verification","text":"<pre><code>python -c \"import llcuda; print(llcuda.__version__)\"\n</code></pre>"},{"location":"llcuda/installation/#first-run","title":"First Run","text":"<pre><code>python -m llcuda\n</code></pre>"},{"location":"llcuda/installation/#upgrade","title":"Upgrade","text":"<pre><code>pip install --upgrade llcuda\n</code></pre>"},{"location":"llcuda/installation/#uninstall","title":"Uninstall","text":"<pre><code>pip uninstall llcuda\nrm -rf ~/.cache/llcuda\n</code></pre>"},{"location":"llcuda/performance/","title":"Performance Guide","text":"<p>Empirical performance data for llcuda on real hardware. All benchmarks conducted on GeForce 940M (1GB VRAM) unless otherwise noted.</p>"},{"location":"llcuda/performance/#executive-summary","title":"Executive Summary","text":"<p>GeForce 940M Performance: - Gemma 2 2B Q4_K_M: ~15 tokens/second, 950MB VRAM - Llama 3.2 1B Q4_K_M: ~18 tokens/second, 750MB VRAM - Qwen 2.5 0.5B Q4_K_M: ~25 tokens/second, 450MB VRAM</p> <p>Usability: Fast enough for real-time interactive chat. Comparable to human reading speed (250-300 words/minute).</p>"},{"location":"llcuda/performance/#test-hardware","title":"Test Hardware","text":""},{"location":"llcuda/performance/#primary-test-system","title":"Primary Test System","text":"<p>GPU: NVIDIA GeForce 940M - VRAM: 1GB GDDR3 - CUDA Cores: 384 - Architecture: Maxwell (compute capability 5.0) - Base Clock: 1072 MHz - Boost Clock: 1176 MHz - Memory Bus: 64-bit - Memory Bandwidth: 16 GB/s</p> <p>CPU: Intel Core i5-5200U - Cores: 2 (4 threads) - Base: 2.2 GHz, Boost: 2.7 GHz - Cache: 3MB L3</p> <p>System: - RAM: 8GB DDR3 - OS: Ubuntu 22.04 LTS - Kernel: 5.15.0 - NVIDIA Driver: 535.183.01 - CUDA: 12.6 (via pre-built binary)</p>"},{"location":"llcuda/performance/#secondary-test-systems","title":"Secondary Test Systems","text":"<p>GeForce GTX 1050 (comparison data) - VRAM: 2GB GDDR5 - CUDA Cores: 640 - Architecture: Pascal (compute capability 6.1)</p> <p>GeForce GTX 1650 (comparison data) - VRAM: 4GB GDDR5 - CUDA Cores: 896 - Architecture: Turing (compute capability 7.5)</p>"},{"location":"llcuda/performance/#benchmark-methodology","title":"Benchmark Methodology","text":""},{"location":"llcuda/performance/#testing-procedure","title":"Testing Procedure","text":"<ol> <li>Fresh system boot to avoid thermal throttling</li> <li>No other GPU processes running (verified with <code>nvidia-smi</code>)</li> <li>Model pre-loaded (exclude download time)</li> <li>Warmup run (first inference often slower)</li> <li>5 test runs per configuration, report median</li> <li>Consistent prompts across all tests</li> <li>Monitor GPU utilization and memory usage</li> </ol>"},{"location":"llcuda/performance/#measurement-tools","title":"Measurement Tools","text":"<pre><code>import time\nfrom llcuda import LLM\n\n# Benchmark function\ndef benchmark(model, prompt, runs=5):\n    llm = LLM(model=model)\n\n    # Warmup\n    llm.chat(prompt)\n\n    times = []\n    token_counts = []\n\n    for _ in range(runs):\n        start = time.time()\n        response = llm.chat(prompt)\n        elapsed = time.time() - start\n\n        tokens = len(response.split())\n        times.append(elapsed)\n        token_counts.append(tokens)\n\n    # Return median\n    median_time = sorted(times)[len(times)//2]\n    median_tokens = sorted(token_counts)[len(token_counts)//2]\n\n    return median_tokens / median_time\n\n# Example usage\nspeed = benchmark(\"gemma-2-2b-it\", \"Explain quantum physics in 100 words\")\nprint(f\"Speed: {speed:.1f} tokens/second\")\n</code></pre>"},{"location":"llcuda/performance/#gpu-monitoring","title":"GPU Monitoring","text":"<pre><code># Monitor GPU during inference\nwatch -n 0.5 nvidia-smi\n</code></pre>"},{"location":"llcuda/performance/#detailed-benchmarks","title":"Detailed Benchmarks","text":""},{"location":"llcuda/performance/#gemma-2-2b-q4_k_m-default-model","title":"Gemma 2 2B Q4_K_M (Default Model)","text":"<p>Configuration: - Model: google/gemma-2-2b-it - Quantization: Q4_K_M - Context Length: 2048 tokens - Temperature: 0.7 - Model Size: ~1.4GB on disk</p> <p>Performance:</p> Metric Value Tokens/Second 14.8 (median) Range 13.2 - 16.5 tok/s VRAM Usage 948 MB GPU Utilization 95-100% Time to First Token 180ms 100-word response ~8 seconds 500-word response ~40 seconds <p>Quality Assessment: - Coherent multi-paragraph responses - Good context retention (8k context window) - Suitable for code generation, Q&amp;A, summarization - Occasional repetition with very long generation</p> <p>Use Cases: - General-purpose chat - Code assistance - Technical documentation - Data analysis helper</p>"},{"location":"llcuda/performance/#llama-32-1b-q4_k_m-faster-alternative","title":"Llama 3.2 1B Q4_K_M (Faster Alternative)","text":"<p>Configuration: - Model: meta-llama/llama-3.2-1b-instruct - Quantization: Q4_K_M - Context Length: 2048 tokens - Model Size: ~900MB on disk</p> <p>Performance:</p> Metric Value Tokens/Second 18.3 (median) Range 16.8 - 20.1 tok/s VRAM Usage 732 MB GPU Utilization 92-98% Time to First Token 150ms 100-word response ~6.5 seconds 500-word response ~32 seconds <p>Quality Assessment: - Good for simple tasks and quick responses - Adequate context retention - Slightly less coherent than Gemma 2 2B for complex tasks - Better for factual Q&amp;A than creative writing</p> <p>Use Cases: - Quick lookups - Simple code generation - Fast prototyping - Interactive development</p>"},{"location":"llcuda/performance/#qwen-25-05b-q4_k_m-ultra-fast","title":"Qwen 2.5 0.5B Q4_K_M (Ultra-Fast)","text":"<p>Configuration: - Model: Qwen/qwen-2.5-0.5b-instruct - Quantization: Q4_K_M - Context Length: 2048 tokens - Model Size: ~400MB on disk</p> <p>Performance:</p> Metric Value Tokens/Second 25.7 (median) Range 23.5 - 28.2 tok/s VRAM Usage 438 MB GPU Utilization 85-95% Time to First Token 100ms 100-word response ~4.5 seconds 500-word response ~23 seconds <p>Quality Assessment: - Basic conversational ability - Good for simple, factual queries - Limited reasoning capability - Best for speed over quality</p> <p>Use Cases: - Ultra-fast responses - Simple Q&amp;A - Basic text completion - Learning/experimentation</p>"},{"location":"llcuda/performance/#comparison-across-gpus","title":"Comparison Across GPUs","text":""},{"location":"llcuda/performance/#gemma-2-2b-q4_k_m-performance","title":"Gemma 2 2B Q4_K_M Performance","text":"GPU VRAM Tokens/Second Relative Speed GeForce 940M 1GB 14.8 1.0x (baseline) GeForce GTX 1050 2GB 28.3 1.9x GeForce GTX 1650 4GB 45.6 3.1x RTX 3060 (reference) 12GB 127.4 8.6x <p>Insight: Even old GPUs like 940M provide acceptable performance. The 940M at 15 tok/s is faster than most people read.</p>"},{"location":"llcuda/performance/#vram-usage-by-model-size","title":"VRAM Usage by Model Size","text":"Model Parameters Quantization VRAM (940M) Qwen 2.5 0.5B 0.5B Q4_K_M 438 MB Llama 3.2 1B 1B Q4_K_M 732 MB Gemma 2 2B 2B Q4_K_M 948 MB Llama 3 3B 3B Q4_K_M ~1.2 GB (won't fit) Mistral 7B 7B Q4_K_M ~4 GB (won't fit) <p>Recommendation for 1GB VRAM: Stick to \u22642B parameter models with Q4_K_M quantization.</p>"},{"location":"llcuda/performance/#quantization-impact","title":"Quantization Impact","text":"<p>Quantization reduces model size and VRAM usage at the cost of some quality.</p>"},{"location":"llcuda/performance/#gemma-2-2b-different-quantizations","title":"Gemma 2 2B - Different Quantizations","text":"Quantization Size VRAM Speed (940M) Quality Q2_K 800MB 650MB ~18 tok/s Low (not recommended) Q4_K_M 1.4GB 948MB ~15 tok/s Good (recommended) Q5_K_M 1.7GB 1.1GB ~13 tok/s Better (won't fit on 1GB) Q6_K 2.0GB 1.3GB ~11 tok/s Excellent (won't fit) F16 4.5GB 3.8GB ~7 tok/s Perfect (won't fit) <p>Recommendation: Q4_K_M offers the best balance of quality and efficiency for legacy GPUs.</p>"},{"location":"llcuda/performance/#context-length-impact","title":"Context Length Impact","text":"<p>Context length affects VRAM usage and generation speed.</p>"},{"location":"llcuda/performance/#gemma-2-2b-q4_k_m-variable-context","title":"Gemma 2 2B Q4_K_M - Variable Context","text":"Context Length VRAM Usage Speed (940M) Use Case 512 tokens 820 MB ~16.5 tok/s Short Q&amp;A 1024 tokens 880 MB ~15.8 tok/s Single-turn chat 2048 tokens 948 MB ~14.8 tok/s Multi-turn (default) 4096 tokens 1.05 GB Won't fit Long documents <p>Recommendation for 1GB VRAM: 2048 token context is optimal.</p>"},{"location":"llcuda/performance/#real-world-scenarios","title":"Real-World Scenarios","text":""},{"location":"llcuda/performance/#scenario-1-interactive-chat","title":"Scenario 1: Interactive Chat","text":"<p>Task: Multi-turn conversation (5 exchanges)</p> <p>Setup: <pre><code>from llcuda import LLM\nllm = LLM(model=\"gemma-2-2b-it\")\n\nconversation = [\n    \"What is machine learning?\",\n    \"How is it different from traditional programming?\",\n    \"Give me an example\",\n    \"Explain neural networks\",\n    \"How do I get started?\"\n]\n\nfor message in conversation:\n    response = llm.chat(message)\n</code></pre></p> <p>Results (GeForce 940M): - Total time: 52 seconds - Average response: ~120 words - Average time per response: ~10 seconds - User experience: Responsive, no noticeable lag</p> <p>Verdict: Excellent for interactive development and learning.</p>"},{"location":"llcuda/performance/#scenario-2-code-generation","title":"Scenario 2: Code Generation","text":"<p>Task: Generate Python function</p> <p>Setup: <pre><code>from llcuda import LLM\nllm = LLM(model=\"gemma-2-2b-it\")\n\nresponse = llm.chat(\"Write a Python function to implement binary search\")\n</code></pre></p> <p>Results (GeForce 940M): - Generated code: 35 lines - Time: 18 seconds - Quality: Correct implementation with comments - Average speed: ~15 tok/s</p> <p>Verdict: Fast enough for interactive coding assistance.</p>"},{"location":"llcuda/performance/#scenario-3-data-analysis-in-jupyter","title":"Scenario 3: Data Analysis in Jupyter","text":"<p>Task: Analyze pandas DataFrame with LLM</p> <p>Setup: <pre><code>import pandas as pd\nfrom llcuda import LLM\n\ndf = pd.read_csv(\"sales.csv\")\nsummary = df.describe().to_string()\n\nllm = LLM(model=\"gemma-2-2b-it\")\nanalysis = llm.chat(f\"Analyze this sales data and suggest insights:\\n{summary}\")\n</code></pre></p> <p>Results (GeForce 940M): - Analysis length: ~200 words - Time: 15 seconds - Quality: Meaningful insights, actionable recommendations - Workflow: Seamless integration with data science workflow</p> <p>Verdict: Perfect for exploratory data analysis.</p>"},{"location":"llcuda/performance/#scenario-4-batch-processing","title":"Scenario 4: Batch Processing","text":"<p>Task: Summarize 10 product reviews</p> <p>Setup: <pre><code>from llcuda import LLM\n\nreviews = [...]  # 10 reviews, ~100 words each\n\nllm = LLM(model=\"llama-3.2-1b-instruct\")  # Faster model\n\nsummaries = []\nfor review in reviews:\n    summary = llm.chat(f\"Summarize this review in one sentence: {review}\")\n    summaries.append(summary)\n</code></pre></p> <p>Results (GeForce 940M): - Total time: 45 seconds - Average per summary: 4.5 seconds - Quality: Concise, captures main points - Throughput: ~13 summaries/minute</p> <p>Verdict: Viable for moderate batch processing tasks.</p>"},{"location":"llcuda/performance/#optimization-tips","title":"Optimization Tips","text":""},{"location":"llcuda/performance/#1-choose-the-right-model","title":"1. Choose the Right Model","text":"<p>For speed: Use Llama 3.2 1B or Qwen 2.5 0.5B <pre><code>llm = LLM(model=\"llama-3.2-1b-instruct\")  # 18 tok/s\n</code></pre></p> <p>For quality: Use Gemma 2 2B (default) <pre><code>llm = LLM(model=\"gemma-2-2b-it\")  # 15 tok/s, better quality\n</code></pre></p>"},{"location":"llcuda/performance/#2-adjust-context-length","title":"2. Adjust Context Length","text":"<p>For short interactions, reduce context to save VRAM: <pre><code>llm = LLM(context_length=1024)  # Saves ~70MB VRAM\n</code></pre></p>"},{"location":"llcuda/performance/#3-control-response-length","title":"3. Control Response Length","text":"<p>Limit max tokens for faster responses: <pre><code>llm = LLM(max_tokens=256)  # Shorter responses\n</code></pre></p>"},{"location":"llcuda/performance/#4-batch-similar-queries","title":"4. Batch Similar Queries","text":"<p>Reuse LLM instance to avoid reloading model: <pre><code>llm = LLM()\n\nfor query in queries:\n    response = llm.chat(query)\n</code></pre></p>"},{"location":"llcuda/performance/#5-monitor-gpu-temperature","title":"5. Monitor GPU Temperature","text":"<p>Thermal throttling can reduce performance: <pre><code>watch -n 1 nvidia-smi --query-gpu=temperature.gpu --format=csv\n</code></pre></p> <p>Keep GPU &lt; 80\u00b0C for optimal performance.</p>"},{"location":"llcuda/performance/#6-close-other-gpu-applications","title":"6. Close Other GPU Applications","text":"<p>Free up VRAM by closing other GPU processes: <pre><code># Check GPU processes\nnvidia-smi\n\n# Kill process if needed (use with caution)\nkill -9 &lt;PID&gt;\n</code></pre></p>"},{"location":"llcuda/performance/#performance-comparisons","title":"Performance Comparisons","text":""},{"location":"llcuda/performance/#llcuda-vs-cloud-apis-latency","title":"llcuda vs Cloud APIs (Latency)","text":"<p>Task: Generate 100-word response</p> Method Latency (GeForce 940M) Cost llcuda (local) ~8 seconds $0 OpenAI GPT-3.5 ~3 seconds $0.002 OpenAI GPT-4 ~12 seconds $0.03 Anthropic Claude ~5 seconds $0.008 <p>Insight: llcuda is cost-free and competitive in speed. Trade-offs are model quality and no internet required.</p>"},{"location":"llcuda/performance/#llcuda-vs-cpu-inference","title":"llcuda vs CPU Inference","text":"<p>Task: Gemma 2 2B, 100-word response</p> Hardware Speed Relative GeForce 940M (GPU) ~15 tok/s 7.5x faster Intel Core i5-5200U (CPU) ~2 tok/s 1.0x baseline <p>Insight: Even a low-end GPU is 7.5x faster than CPU. GPU acceleration is critical.</p>"},{"location":"llcuda/performance/#limitations","title":"Limitations","text":""},{"location":"llcuda/performance/#vram-constraints","title":"VRAM Constraints","text":"<p>GeForce 940M (1GB VRAM) limitations: - Cannot run models &gt; 2B parameters (Q4_K_M) - Cannot use context &gt; 2048 tokens reliably - Cannot load multiple models simultaneously</p> <p>Workarounds: - Use smaller models (Llama 3.2 1B, Qwen 2.5 0.5B) - Reduce context length (<code>context_length=1024</code>) - Offload layers to CPU (<code>gpu_layers=20</code>, slower)</p>"},{"location":"llcuda/performance/#generation-speed","title":"Generation Speed","text":"<p>15 tok/s means: - 100-word response: ~8 seconds - 500-word response: ~40 seconds - Not suitable for real-time streaming applications - Acceptable for interactive chat and development</p>"},{"location":"llcuda/performance/#model-quality","title":"Model Quality","text":"<p>Q4_K_M quantization trade-offs: - Reduced precision vs full-precision models - Occasional minor errors in math/reasoning - Still excellent for most tasks</p>"},{"location":"llcuda/performance/#future-optimizations","title":"Future Optimizations","text":"<p>Planned improvements for future llcuda releases:</p> <ol> <li>FlashAttention integration - 20-30% speedup expected</li> <li>Speculative decoding - 2x speedup for certain prompts</li> <li>Quantized KV cache - Reduce VRAM usage by 30%</li> <li>Multi-GPU support - Aggregate VRAM across GPUs</li> <li>Metal support (macOS) - Extend to Apple Silicon</li> </ol>"},{"location":"llcuda/performance/#benchmarking-your-system","title":"Benchmarking Your System","text":"<p>Run this script to benchmark your own hardware:</p> <pre><code>from llcuda import LLM\nimport time\n\nmodels = [\n    \"gemma-2-2b-it\",\n    \"llama-3.2-1b-instruct\",\n    \"qwen-2.5-0.5b-instruct\"\n]\n\nprompt = \"Explain the theory of relativity in 100 words\"\n\nprint(\"llcuda Benchmark Report\")\nprint(\"=\" * 60)\n\nfor model in models:\n    print(f\"\\nModel: {model}\")\n    print(\"-\" * 60)\n\n    llm = LLM(model=model)\n\n    # Warmup\n    llm.chat(prompt)\n\n    # Benchmark\n    times = []\n    token_counts = []\n\n    for i in range(5):\n        start = time.time()\n        response = llm.chat(prompt)\n        elapsed = time.time() - start\n\n        tokens = len(response.split())\n        times.append(elapsed)\n        token_counts.append(tokens)\n\n        print(f\"  Run {i+1}: {tokens} tokens in {elapsed:.1f}s ({tokens/elapsed:.1f} tok/s)\")\n\n    median_time = sorted(times)[2]\n    median_tokens = sorted(token_counts)[2]\n    median_speed = median_tokens / median_time\n\n    print(f\"\\n  Median: {median_tokens} tokens in {median_time:.1f}s\")\n    print(f\"  Speed: {median_speed:.1f} tokens/second\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Benchmark complete!\")\n</code></pre> <p>Share your results at GitHub Discussions!</p>"},{"location":"llcuda/performance/#summary","title":"Summary","text":"<p>GeForce 940M (1GB VRAM) is viable for LLM inference: - 15 tok/s is fast enough for interactive chat - Zero cost compared to cloud APIs - Perfect for learning, development, and experimentation - Works offline with no internet dependency</p> <p>Key Takeaway: You don't need expensive hardware to run modern LLMs. llcuda makes it accessible on hardware you already own.</p> <p>Next: View production-ready examples \u2192</p>"},{"location":"llcuda/quickstart/","title":"Quick Start Guide","text":"<p>Get llcuda running in under 5 minutes. This guide assumes you have Ubuntu 22.04 and an NVIDIA GPU.</p>"},{"location":"llcuda/quickstart/#prerequisites-check","title":"Prerequisites Check","text":"<p>Before installation, verify your system meets the requirements:</p> <pre><code># Check if you have an NVIDIA GPU\nlspci | grep -i nvidia\n\n# Check CUDA compute capability (should be 5.0+)\nnvidia-smi\n\n# Check Python version (need 3.8+)\npython3 --version\n</code></pre> <p>System Requirements Met?</p> <p>If all commands above work, you're ready to install llcuda!</p>"},{"location":"llcuda/quickstart/#installation-30-seconds","title":"Installation (30 seconds)","text":"<p>Install llcuda via pip:</p> <pre><code>pip install llcuda\n</code></pre> <p>That's it. No compilation, no CUDA toolkit, no configuration files.</p> Installation Failed? <p>See the Installation Guide for troubleshooting.</p>"},{"location":"llcuda/quickstart/#first-run-interactive-chat-2-minutes","title":"First Run: Interactive Chat (2 minutes)","text":"<p>Launch the interactive chat interface:</p> <pre><code>python -m llcuda\n</code></pre> <p>What happens on first run: 1. llcuda detects your GPU 2. Downloads Gemma 2 2B model (~1.4GB) from Hugging Face 3. Loads model into GPU memory 4. Starts interactive chat</p> <p>First download takes 2-3 minutes on typical internet. Subsequent runs start instantly.</p> Model Download Location <p>Models are cached in <code>~/.cache/llcuda/models/</code>. You won't need to download again.</p>"},{"location":"llcuda/quickstart/#your-first-conversation","title":"Your First Conversation","text":"<p>Once the model loads, you'll see:</p> <pre><code>llcuda v0.1.0 - LLM Inference on Legacy GPUs\nLoaded: gemma-2-2b-it (Q4_K_M)\nGPU: GeForce 940M (1GB VRAM)\n\nType your message (or 'quit' to exit):\n&gt;\n</code></pre> <p>Try asking a question:</p> <pre><code>&gt; Explain quantum computing in simple terms\n</code></pre> <p>The model will generate a response at ~15 tokens/second on GeForce 940M.</p> <p>Tips: - Type <code>quit</code> or <code>exit</code> to close - Press Ctrl+C to interrupt generation - Each message maintains conversation context</p>"},{"location":"llcuda/quickstart/#using-llcuda-in-python-1-minute","title":"Using llcuda in Python (1 minute)","text":"<p>Create a Python file <code>test_llcuda.py</code>:</p> <pre><code>from llcuda import LLM\n\n# Initialize LLM (downloads model on first run)\nllm = LLM()\n\n# Ask a question\nresponse = llm.chat(\"What is machine learning?\")\nprint(response)\n</code></pre> <p>Run it:</p> <pre><code>python test_llcuda.py\n</code></pre> <p>Output: <pre><code>Machine learning is a type of artificial intelligence where computers\nlearn from data without being explicitly programmed. Instead of following\nrigid rules, ML systems find patterns in data and make predictions...\n</code></pre></p>"},{"location":"llcuda/quickstart/#jupyterlab-integration-30-seconds","title":"JupyterLab Integration (30 seconds)","text":"<p>llcuda works perfectly in Jupyter notebooks:</p> <pre><code># Cell 1: Import and initialize\nfrom llcuda import LLM\nllm = LLM()\n\n# Cell 2: Interactive chat\nresponse = llm.chat(\"Explain gradient descent\")\nprint(response)\n\n# Cell 3: Follow-up question (maintains context)\nresponse = llm.chat(\"Give me a Python example\")\nprint(response)\n</code></pre> <p>The LLM maintains conversation context across cells, perfect for iterative development.</p>"},{"location":"llcuda/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"llcuda/quickstart/#simple-qa","title":"Simple Q&amp;A","text":"<pre><code>from llcuda import LLM\n\nllm = LLM()\nanswer = llm.chat(\"What is the capital of France?\")\nprint(answer)\n</code></pre>"},{"location":"llcuda/quickstart/#code-generation","title":"Code Generation","text":"<pre><code>from llcuda import LLM\n\nllm = LLM()\ncode = llm.chat(\"Write a Python function to calculate factorial\")\nprint(code)\n</code></pre>"},{"location":"llcuda/quickstart/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<pre><code>from llcuda import LLM\n\nllm = LLM()\n\n# Context is maintained across calls\nllm.chat(\"My name is Alice and I'm learning Python\")\nllm.chat(\"What topics should I focus on?\")\nresponse = llm.chat(\"What was my name?\")  # Remembers \"Alice\"\nprint(response)\n</code></pre>"},{"location":"llcuda/quickstart/#data-analysis-helper","title":"Data Analysis Helper","text":"<pre><code>import pandas as pd\nfrom llcuda import LLM\n\ndf = pd.read_csv(\"sales.csv\")\nsummary = df.describe().to_string()\n\nllm = LLM()\nanalysis = llm.chat(f\"Analyze this sales data:\\n{summary}\")\nprint(analysis)\n</code></pre>"},{"location":"llcuda/quickstart/#choosing-a-different-model","title":"Choosing a Different Model","text":"<p>By default, llcuda uses Gemma 2 2B. You can specify a different model:</p>"},{"location":"llcuda/quickstart/#llama-32-1b-faster","title":"Llama 3.2 1B (Faster)","text":"<pre><code>from llcuda import LLM\n\nllm = LLM(model=\"llama-3.2-1b-instruct\")\nresponse = llm.chat(\"Hello!\")\n</code></pre> <p>Performance: ~18 tok/s on GeForce 940M (faster but less capable)</p>"},{"location":"llcuda/quickstart/#qwen-25-05b-ultra-fast","title":"Qwen 2.5 0.5B (Ultra-Fast)","text":"<pre><code>from llcuda import LLM\n\nllm = LLM(model=\"qwen-2.5-0.5b-instruct\")\nresponse = llm.chat(\"Hello!\")\n</code></pre> <p>Performance: ~25 tok/s on GeForce 940M (fastest but basic)</p>"},{"location":"llcuda/quickstart/#custom-model-from-hugging-face","title":"Custom Model from Hugging Face","text":"<pre><code>from llcuda import LLM\n\nllm = LLM(\n    model=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n    model_file=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n)\n</code></pre> <p>VRAM Limitations</p> <p>For 1GB VRAM GPUs, stick to \u22642B parameter models with Q4_K_M quantization.</p>"},{"location":"llcuda/quickstart/#performance-expectations","title":"Performance Expectations","text":"<p>On GeForce 940M (1GB VRAM):</p> Model Speed Quality Use Case Gemma 2 2B ~15 tok/s High General chat, code Llama 3.2 1B ~18 tok/s Medium Fast responses Qwen 2.5 0.5B ~25 tok/s Basic Simple tasks <p>What does ~15 tok/s feel like? - A 100-word response takes ~8 seconds - Interactive enough for real-time chat - Much faster than human reading speed</p> <p>View detailed benchmarks \u2192</p>"},{"location":"llcuda/quickstart/#configuration-options","title":"Configuration Options","text":"<p>llcuda works with zero configuration, but you can customize:</p>"},{"location":"llcuda/quickstart/#basic-options","title":"Basic Options","text":"<pre><code>from llcuda import LLM\n\nllm = LLM(\n    model=\"gemma-2-2b-it\",          # Model name\n    max_tokens=512,                  # Max response length\n    temperature=0.7,                 # Randomness (0-1)\n    context_length=2048,             # Context window size\n    gpu_layers=32,                   # Layers on GPU (auto-detected)\n)\n</code></pre>"},{"location":"llcuda/quickstart/#advanced-options","title":"Advanced Options","text":"<pre><code>from llcuda import LLM\n\nllm = LLM(\n    model=\"gemma-2-2b-it\",\n    max_tokens=1024,\n    temperature=0.8,\n    top_p=0.9,                       # Nucleus sampling\n    top_k=40,                        # Top-K sampling\n    repeat_penalty=1.1,              # Penalize repetition\n    verbose=True,                    # Show generation stats\n)\n</code></pre>"},{"location":"llcuda/quickstart/#verifying-your-setup","title":"Verifying Your Setup","text":"<p>Run this verification script to confirm everything works:</p> <pre><code>from llcuda import LLM\nimport time\n\nprint(\"llcuda Verification Script\")\nprint(\"-\" * 40)\n\n# Initialize\nprint(\"1. Initializing LLM...\")\nstart = time.time()\nllm = LLM(model=\"gemma-2-2b-it\")\nprint(f\"   \u2713 Loaded in {time.time() - start:.1f}s\")\n\n# Test inference\nprint(\"\\n2. Testing inference...\")\nstart = time.time()\nresponse = llm.chat(\"Say 'Hello World' and nothing else\")\nelapsed = time.time() - start\ntokens = len(response.split())\nprint(f\"   \u2713 Generated {tokens} tokens in {elapsed:.1f}s\")\nprint(f\"   \u2713 Speed: ~{tokens/elapsed:.1f} tok/s\")\n\n# Test context\nprint(\"\\n3. Testing context retention...\")\nllm.chat(\"Remember this number: 42\")\nresponse = llm.chat(\"What number did I tell you to remember?\")\nif \"42\" in response:\n    print(\"   \u2713 Context maintained\")\nelse:\n    print(\"   \u2717 Context not maintained\")\n\nprint(\"\\n\" + \"=\" * 40)\nprint(\"Verification complete! llcuda is working.\")\n</code></pre> <p>Expected output: <pre><code>llcuda Verification Script\n----------------------------------------\n1. Initializing LLM...\n   \u2713 Loaded in 2.3s\n\n2. Testing inference...\n   \u2713 Generated 4 tokens in 0.3s\n   \u2713 Speed: ~13.3 tok/s\n\n3. Testing context retention...\n   \u2713 Context maintained\n\n========================================\nVerification complete! llcuda is working.\n</code></pre></p>"},{"location":"llcuda/quickstart/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":""},{"location":"llcuda/quickstart/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Your GPU ran out of VRAM. Solutions:</p> <pre><code># Use a smaller model\nllm = LLM(model=\"llama-3.2-1b-instruct\")\n\n# Or reduce context length\nllm = LLM(context_length=1024)\n\n# Or offload some layers to CPU (slower)\nllm = LLM(gpu_layers=16)  # Default is auto\n</code></pre>"},{"location":"llcuda/quickstart/#model-download-failed","title":"\"Model download failed\"","text":"<p>Check your internet connection and retry:</p> <pre><code>from llcuda import LLM\n\n# Force re-download\nllm = LLM(model=\"gemma-2-2b-it\", force_download=True)\n</code></pre>"},{"location":"llcuda/quickstart/#slow-generation-speed","title":"\"Slow generation speed\"","text":"<p>If you're getting &lt;5 tok/s, the model might be using CPU instead of GPU:</p> <pre><code># Verify GPU is detected\nnvidia-smi\n\n# Check llcuda is using GPU\npython -c \"from llcuda import LLM; llm = LLM(verbose=True)\"\n# Should show \"Using GPU: GeForce XXX\"\n</code></pre> <p>For more troubleshooting: Installation Guide</p>"},{"location":"llcuda/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that llcuda is working, explore:</p> <ol> <li>Installation Guide - Deep dive into setup and troubleshooting</li> <li>Performance Guide - Optimize for your GPU</li> <li>Examples - Production-ready code samples</li> <li>Full Documentation - Complete API reference</li> </ol>"},{"location":"llcuda/quickstart/#quick-reference","title":"Quick Reference","text":""},{"location":"llcuda/quickstart/#installation","title":"Installation","text":"<pre><code>pip install llcuda\n</code></pre>"},{"location":"llcuda/quickstart/#interactive-chat","title":"Interactive Chat","text":"<pre><code>python -m llcuda\n</code></pre>"},{"location":"llcuda/quickstart/#basic-python-usage","title":"Basic Python Usage","text":"<pre><code>from llcuda import LLM\nllm = LLM()\nresponse = llm.chat(\"Your question here\")\n</code></pre>"},{"location":"llcuda/quickstart/#different-models","title":"Different Models","text":"<pre><code>llm = LLM(model=\"llama-3.2-1b-instruct\")  # Faster\nllm = LLM(model=\"qwen-2.5-0.5b-instruct\") # Ultra-fast\nllm = LLM(model=\"gemma-2-2b-it\")          # Default, best quality\n</code></pre>"},{"location":"llcuda/quickstart/#common-options","title":"Common Options","text":"<pre><code>llm = LLM(\n    model=\"gemma-2-2b-it\",\n    max_tokens=512,\n    temperature=0.7,\n    verbose=True\n)\n</code></pre> <p>You're all set! Start building with llcuda.</p>"},{"location":"resume/","title":"Resume","text":"<p>Place your resume PDF file here as <code>Muhammad_Waqas_Resume_2025.pdf</code></p> <p>The mkdocs.yml navigation is configured to link to: - <code>resume/Muhammad_Waqas_Resume_2025.pdf</code></p> <p>Make sure to add your actual resume PDF to this directory.</p>"},{"location":"ubuntu-cuda-executable/","title":"Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>Pre-built llama.cpp binary with CUDA 12.6 support for Ubuntu 22.04. The essential infrastructure that powers llcuda.</p>"},{"location":"ubuntu-cuda-executable/#overview","title":"Overview","text":"<p>Ubuntu-Cuda-Llama.cpp-Executable is a pre-compiled, statically-linked llama.cpp binary optimized for Ubuntu 22.04 with CUDA 12.6 support. It eliminates the need for users to compile llama.cpp themselves, removing a major barrier to entry for running LLMs on legacy GPUs.</p> <p>Repository: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable</p>"},{"location":"ubuntu-cuda-executable/#why-this-exists","title":"Why This Exists","text":""},{"location":"ubuntu-cuda-executable/#the-problem","title":"The Problem","text":"<p>Running llama.cpp with CUDA support traditionally requires:</p> <ol> <li>Install CUDA Toolkit (~3GB download, complex setup)</li> <li>Install build dependencies (CMake, g++, CUDA compilers)</li> <li>Clone llama.cpp repository</li> <li>Configure CMake with correct CUDA flags</li> <li>Compile (10-20 minutes, can fail with cryptic errors)</li> <li>Troubleshoot compilation issues</li> </ol> <p>This is intimidating for most users and creates a massive barrier to entry.</p>"},{"location":"ubuntu-cuda-executable/#the-solution","title":"The Solution","text":"<p>A pre-built binary that: - Requires zero compilation - Needs no CUDA toolkit installation - Works out of the box on Ubuntu 22.04 - Is statically linked (no external dependencies) - Is optimized for legacy GPUs (compute capability 5.0+)</p> <p>Just download and run.</p>"},{"location":"ubuntu-cuda-executable/#features","title":"Features","text":""},{"location":"ubuntu-cuda-executable/#zero-dependencies","title":"Zero Dependencies","text":"<p>The binary is statically linked and includes: - CUDA runtime (12.6) - cuBLAS library - All llama.cpp dependencies</p> <p>No external libraries required.</p>"},{"location":"ubuntu-cuda-executable/#optimized-for-legacy-gpus","title":"Optimized for Legacy GPUs","text":"<p>Compiled with: - Compute capability 5.0 (Maxwell architecture) - Optimizations for low-VRAM GPUs - Efficient memory management - Support for quantized models (GGUF format)</p>"},{"location":"ubuntu-cuda-executable/#tested-hardware","title":"Tested Hardware","text":"<p>Verified working on: - GeForce 940M (1GB VRAM) - Primary test platform - GeForce GTX 1050 (2GB VRAM) - GeForce GTX 1650 (4GB VRAM) - GeForce 900/800 series - Any Maxwell+ GPU (compute capability 5.0+)</p>"},{"location":"ubuntu-cuda-executable/#technical-details","title":"Technical Details","text":""},{"location":"ubuntu-cuda-executable/#compilation-flags","title":"Compilation Flags","text":"<pre><code>cmake -B build \\\n  -DLLAMA_CUDA=ON \\\n  -DLLAMA_CUDA_F16=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=50 \\\n  -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-12.6 \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DBUILD_SHARED_LIBS=OFF\n\ncmake --build build --config Release -j$(nproc)\n</code></pre> <p>Key flags: - <code>LLAMA_CUDA=ON</code>: Enable CUDA support - <code>CMAKE_CUDA_ARCHITECTURES=50</code>: Maxwell architecture (compute 5.0) - <code>BUILD_SHARED_LIBS=OFF</code>: Static linking - <code>LLAMA_CUDA_F16=ON</code>: Half-precision support</p>"},{"location":"ubuntu-cuda-executable/#build-environment","title":"Build Environment","text":"<ul> <li>OS: Ubuntu 22.04 LTS</li> <li>CUDA: 12.6.68</li> <li>GCC: 11.4.0</li> <li>CMake: 3.22.1</li> <li>llama.cpp: Latest stable release</li> </ul>"},{"location":"ubuntu-cuda-executable/#binary-details","title":"Binary Details","text":"<ul> <li>Size: ~45MB (statically linked)</li> <li>Format: ELF 64-bit LSB executable</li> <li>Architecture: x86-64</li> <li>CUDA: Runtime 12.6 embedded</li> <li>Dependencies: None (statically linked)</li> </ul> <p>Verify static linking: <pre><code>ldd llama-cli\n# Output: not a dynamic executable\n</code></pre></p>"},{"location":"ubuntu-cuda-executable/#usage","title":"Usage","text":""},{"location":"ubuntu-cuda-executable/#direct-usage","title":"Direct Usage","text":"<p>Download and run the binary directly:</p> <pre><code># Download binary\nwget https://github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable/releases/latest/download/llama-cli\n\n# Make executable\nchmod +x llama-cli\n\n# Run inference\n./llama-cli -m model.gguf -p \"Hello, how are you?\"\n</code></pre>"},{"location":"ubuntu-cuda-executable/#common-options","title":"Common Options","text":"<pre><code># Interactive chat mode\n./llama-cli -m model.gguf -cnv\n\n# Set context length\n./llama-cli -m model.gguf -c 2048 -p \"Your prompt\"\n\n# Control GPU layers (for VRAM management)\n./llama-cli -m model.gguf -ngl 32 -p \"Your prompt\"\n\n# Set generation parameters\n./llama-cli -m model.gguf \\\n  -p \"Your prompt\" \\\n  --temp 0.7 \\\n  --top-p 0.9 \\\n  --top-k 40 \\\n  --repeat-penalty 1.1\n</code></pre>"},{"location":"ubuntu-cuda-executable/#integration-with-llcuda","title":"Integration with llcuda","text":"<p>llcuda automatically uses this binary under the hood:</p> <pre><code>from llcuda import LLM\n\n# llcuda downloads and uses the pre-built binary\nllm = LLM()\nresponse = llm.chat(\"Hello!\")\n</code></pre> <p>Users never need to interact with the binary directly when using llcuda.</p>"},{"location":"ubuntu-cuda-executable/#installation","title":"Installation","text":""},{"location":"ubuntu-cuda-executable/#method-1-via-llcuda-recommended","title":"Method 1: Via llcuda (Recommended)","text":"<p>The easiest way is to install llcuda, which includes the binary:</p> <pre><code>pip install llcuda\n</code></pre> <p>llcuda automatically downloads and manages the binary.</p>"},{"location":"ubuntu-cuda-executable/#method-2-direct-download","title":"Method 2: Direct Download","text":"<p>Download the binary directly from GitHub:</p> <pre><code># Download latest release\nwget https://github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable/releases/latest/download/llama-cli\n\n# Make executable\nchmod +x llama-cli\n\n# Move to PATH (optional)\nsudo mv llama-cli /usr/local/bin/\n</code></pre>"},{"location":"ubuntu-cuda-executable/#method-3-build-from-source","title":"Method 3: Build from Source","text":"<p>If you want to build yourself:</p> <pre><code># Install dependencies\nsudo apt update\nsudo apt install cmake build-essential\n\n# Install CUDA 12.6\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda-repo-ubuntu2204-12-6-local_12.6.0-560.28.03-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-6-local_12.6.0-560.28.03-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-6-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt update\nsudo apt install cuda-toolkit-12-6\n\n# Clone llama.cpp\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\n\n# Build with CUDA\ncmake -B build \\\n  -DLLAMA_CUDA=ON \\\n  -DLLAMA_CUDA_F16=ON \\\n  -DCMAKE_CUDA_ARCHITECTURES=50 \\\n  -DCMAKE_BUILD_TYPE=Release \\\n  -DBUILD_SHARED_LIBS=OFF\n\ncmake --build build --config Release -j$(nproc)\n\n# Binary at: build/bin/llama-cli\n</code></pre>"},{"location":"ubuntu-cuda-executable/#compatibility","title":"Compatibility","text":""},{"location":"ubuntu-cuda-executable/#supported-operating-systems","title":"Supported Operating Systems","text":"<p>Tested and Supported: - Ubuntu 22.04 LTS (primary target)</p> <p>Likely Compatible: - Ubuntu 20.04 LTS - Ubuntu 24.04 LTS - Debian 11+ - Linux Mint 21+</p> <p>Not Supported: - Windows (different build required) - macOS (limited CUDA support) - WSL2 (experimental CUDA support)</p>"},{"location":"ubuntu-cuda-executable/#supported-gpus","title":"Supported GPUs","text":"<p>Minimum Requirements: - NVIDIA GPU with compute capability 5.0+ - Maxwell architecture or newer - 1GB VRAM minimum (for small models)</p> <p>Tested GPUs: - GeForce 900 series (940M, 950M, 960M, etc.) - GeForce 800 series (840M, 850M, etc.) - GeForce GTX 750/750 Ti and newer - Quadro K-series and newer</p> <p>Check your GPU: <pre><code>nvidia-smi --query-gpu=compute_cap --format=csv\n# Should output 5.0 or higher\n</code></pre></p>"},{"location":"ubuntu-cuda-executable/#performance","title":"Performance","text":"<p>Performance on GeForce 940M (1GB VRAM):</p> Model Quantization Speed VRAM Gemma 2 2B Q4_K_M ~15 tok/s 950MB Llama 3.2 1B Q4_K_M ~18 tok/s 750MB Qwen 2.5 0.5B Q4_K_M ~25 tok/s 450MB <p>Faster GPUs see proportionally better performance: - GTX 1050 (2GB): ~2x faster - GTX 1650 (4GB): ~3x faster</p>"},{"location":"ubuntu-cuda-executable/#advantages-over-standard-llamacpp","title":"Advantages Over Standard llama.cpp","text":""},{"location":"ubuntu-cuda-executable/#1-no-compilation-required","title":"1. No Compilation Required","text":"<p>Standard llama.cpp: <pre><code># ~20 minutes of compilation\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\ncmake -B build -DLLAMA_CUDA=ON\ncmake --build build --config Release\n</code></pre></p> <p>Ubuntu-Cuda-Llama.cpp-Executable: <pre><code># Instant\nwget &lt;binary-url&gt;\nchmod +x llama-cli\n</code></pre></p>"},{"location":"ubuntu-cuda-executable/#2-no-cuda-toolkit-needed","title":"2. No CUDA Toolkit Needed","text":"<p>Standard llama.cpp: - Requires CUDA toolkit installation (~3GB) - Complex environment setup - Version compatibility issues</p> <p>Ubuntu-Cuda-Llama.cpp-Executable: - Only needs NVIDIA driver - CUDA runtime embedded in binary - Works out of the box</p>"},{"location":"ubuntu-cuda-executable/#3-optimized-for-legacy-gpus","title":"3. Optimized for Legacy GPUs","text":"<p>Standard llama.cpp: - Default builds target modern GPUs - May not optimize for Maxwell architecture</p> <p>Ubuntu-Cuda-Llama.cpp-Executable: - Compiled specifically for compute 5.0 - Optimized for low-VRAM scenarios - Tested on GeForce 940M</p>"},{"location":"ubuntu-cuda-executable/#4-guaranteed-compatibility","title":"4. Guaranteed Compatibility","text":"<p>Standard llama.cpp: - Build issues with different CUDA versions - CMake configuration problems - Dependency conflicts</p> <p>Ubuntu-Cuda-Llama.cpp-Executable: - Tested on Ubuntu 22.04 - Static linking eliminates dependency issues - Known working configuration</p>"},{"location":"ubuntu-cuda-executable/#limitations","title":"Limitations","text":""},{"location":"ubuntu-cuda-executable/#platform-specific","title":"Platform-Specific","text":"<p>This binary is built specifically for: - Linux (Ubuntu 22.04) - x86-64 architecture - NVIDIA GPUs (CUDA)</p> <p>Does not support: - Windows (need different build) - macOS (limited CUDA support) - AMD GPUs (need ROCm build) - ARM processors (need ARM build)</p>"},{"location":"ubuntu-cuda-executable/#cuda-version","title":"CUDA Version","text":"<p>Binary includes CUDA 12.6 runtime. Requires: - NVIDIA driver 450.80.02 or newer</p> <p>Most modern drivers satisfy this requirement.</p>"},{"location":"ubuntu-cuda-executable/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ubuntu-cuda-executable/#cuda-driver-version-is-insufficient","title":"\"CUDA driver version is insufficient\"","text":"<p>Issue: Driver too old for CUDA 12.6</p> <p>Solution: Update NVIDIA driver <pre><code>sudo ubuntu-drivers autoinstall\nsudo reboot\n</code></pre></p>"},{"location":"ubuntu-cuda-executable/#illegal-instruction-core-dumped","title":"\"Illegal instruction (core dumped)\"","text":"<p>Issue: CPU doesn't support required instruction set</p> <p>Solution: Re-compile with broader compatibility <pre><code>cmake -B build -DLLAMA_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=\"50;60;70;75;80\"\n</code></pre></p>"},{"location":"ubuntu-cuda-executable/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Issue: Model too large for GPU VRAM</p> <p>Solution: Reduce GPU layers <pre><code>./llama-cli -m model.gguf -ngl 20  # Use fewer GPU layers\n</code></pre></p>"},{"location":"ubuntu-cuda-executable/#contributing","title":"Contributing","text":"<p>Contributions welcome! Areas for improvement:</p> <ol> <li>Support more platforms (Windows, macOS)</li> <li>Optimize for newer GPUs (Ampere, Ada Lovelace)</li> <li>Reduce binary size (current: ~45MB)</li> <li>Add more build configurations</li> </ol> <p>Repository: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable</p>"},{"location":"ubuntu-cuda-executable/#related-projects","title":"Related Projects","text":""},{"location":"ubuntu-cuda-executable/#llcuda","title":"llcuda","text":"<p>PyPI Package | Docs</p> <p>Python wrapper around this binary. Provides high-level API, automatic model downloading, and JupyterLab integration.</p> <p>Use llcuda if you want: - Python API - Automatic model management - JupyterLab integration - Zero configuration</p> <p>Use the binary directly if you want: - Command-line interface - Shell scripting - Custom integration - Maximum control</p>"},{"location":"ubuntu-cuda-executable/#license","title":"License","text":"<p>MIT License - see LICENSE</p> <p>Based on llama.cpp by Georgi Gerganov.</p>"},{"location":"ubuntu-cuda-executable/#support","title":"Support","text":"<p>Documentation: waqasm86.github.io GitHub Issues: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable/issues Email: waqasm86@gmail.com</p>"},{"location":"ubuntu-cuda-executable/#summary","title":"Summary","text":"<p>Ubuntu-Cuda-Llama.cpp-Executable makes llama.cpp accessible to everyone by: - Eliminating compilation complexity - Removing CUDA toolkit requirement - Optimizing for legacy GPUs - Providing guaranteed compatibility</p> <p>It's the foundation that makes llcuda possible.</p> <p>Explore llcuda \u2192</p>"}]}