{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"On-Device AI for Legacy NVIDIA GPUs","text":"<p>Product-minded engineer building production-ready AI tools for Ubuntu 22.04 + old NVIDIA GPUs</p>"},{"location":"#the-llcuda-ecosystem","title":"The llcuda Ecosystem","text":"<p>Making large language models accessible on legacy hardware through empirical engineering and zero-configuration design.</p>"},{"location":"#featured-project-llcuda-v101","title":"Featured Project: llcuda v1.0.1","text":"<p>llcuda on PyPI is a PyTorch-style Python package that brings LLM inference to old NVIDIA GPUs with zero configuration. Built for Ubuntu 22.04 with bundled CUDA 12.8 binaries and tested extensively on GeForce 940M (1GB VRAM).</p> <pre><code># Install or upgrade to latest version\npip install --upgrade llcuda\n</code></pre> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads from HuggingFace\nresult = engine.infer(\"What is AI?\")\nprint(result.text)\n</code></pre> <p>Key Features:</p> <ul> <li>Zero Configuration: No manual path setup, no LLAMA_SERVER_PATH needed</li> <li>Bundled Binaries: All CUDA 12.8 binaries and libraries included (47 MB wheel)</li> <li>Smart Model Loading: Auto-download from HuggingFace registry with user confirmation</li> <li>Hardware Auto-Config: Detects VRAM and optimizes settings automatically</li> <li>11 Curated Models: Ready to use out of the box</li> <li>Performance Metrics: P50/P95/P99 latency tracking built-in</li> <li>Production Ready: Published to PyPI, works like PyTorch</li> <li>Empirical Performance: ~15 tokens/second with Gemma 3 1B Q4_K_M on GeForce 940M</li> </ul>"},{"location":"#philosophy-product-minded-engineering","title":"Philosophy: Product-Minded Engineering","text":"<p>I build tools that actually work on real hardware people own. No assumptions about cutting-edge GPUs. No theoretical benchmarks.</p> <p>My Approach:</p> <ol> <li>Target Real Hardware: GeForce 940M (1GB VRAM) as the baseline</li> <li>Empirical Testing: Every claim backed by measurements on actual hardware</li> <li>Zero-Configuration: Installation should be <code>pip install</code> and done</li> <li>Production Quality: Published to PyPI, not just GitHub repos</li> <li>Documentation First: If users can't use it, it doesn't exist</li> </ol>"},{"location":"#performance-data","title":"Performance Data","text":"<p>All benchmarks run on GeForce 940M (1GB VRAM, 384 CUDA cores, Maxwell architecture) on Ubuntu 22.04 with llcuda v1.0.1.</p>"},{"location":"#gemma-3-1b-q4_k_m","title":"Gemma 3 1B Q4_K_M","text":"<pre><code>Model: google/gemma-3-1b-it (Q4_K_M quantization)\nHardware: GeForce 940M (1GB VRAM)\nPerformance: ~15 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\nMemory Usage: ~800MB VRAM\n</code></pre> <p>Auto-Configuration Details: - VRAM detected: 1.0 GB - Optimal settings calculated automatically - No manual tuning required</p>"},{"location":"#available-models-11-total","title":"Available Models (11 total)","text":"<p>llcuda includes a curated registry of models tested on GeForce 940M:</p> <ul> <li>gemma-3-1b-Q4_K_M (700 MB) - Recommended for 1GB VRAM</li> <li>tinyllama-1.1b-Q5_K_M (800 MB) - Smallest option</li> <li>phi-3-mini-Q4_K_M (2.2 GB) - For 2GB+ VRAM</li> <li>mistral-7b-Q4_K_M (4.1 GB) - For 4GB+ VRAM</li> <li>... and 7 more models</li> </ul> <p>View full model registry \u2192</p>"},{"location":"#real-world-use-cases","title":"Real-World Use Cases","text":"<ul> <li>Interactive Chat: Responsive enough for real-time conversation</li> <li>Jupyter Notebooks: Perfect for exploratory data analysis and prototyping</li> <li>Local Development: Test LLM integrations without cloud APIs</li> <li>Learning: Understand LLM behavior without expensive hardware</li> <li>Production: P50/P95/P99 latency tracking for monitoring</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Get up and running in under 5 minutes:</p> <pre><code># Install or upgrade llcuda (includes all CUDA binaries)\npip install --upgrade llcuda\n</code></pre> <pre><code># Basic usage - auto-downloads model with confirmation\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads from HuggingFace\nresult = engine.infer(\"Explain quantum computing in simple terms.\")\nprint(result.text)\nprint(f\"Performance: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>For JupyterLab integration:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Load model with auto-configuration for your GPU\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Interactive chat with performance tracking\nconversation = [\n    \"What is machine learning?\",\n    \"How does it differ from traditional programming?\",\n    \"Give me a practical example\"\n]\n\nfor message in conversation:\n    result = engine.infer(message, max_tokens=100)\n    print(f\"User: {message}\")\n    print(f\"AI: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre> <p>View detailed quick start guide \u2192</p>"},{"location":"#why-llcuda","title":"Why llcuda?","text":""},{"location":"#the-problem","title":"The Problem","text":"<p>Most LLM tools assume you have: - Modern NVIDIA GPUs (RTX series) - 8GB+ VRAM - Willingness to compile complex C++ projects - Latest CUDA toolkit installed</p> <p>Reality: Millions of users have older GPUs collecting dust.</p>"},{"location":"#the-solution","title":"The Solution","text":"<p>llcuda is designed for the hardware people actually own:</p> <ul> <li>GeForce 900 series: 940M, 950M, 960M</li> <li>GeForce 800 series: 840M, 850M</li> <li>Maxwell/Kepler architectures: Still capable, just ignored</li> <li>1-2GB VRAM: More than enough for quantized models</li> </ul>"},{"location":"#the-approach","title":"The Approach","text":"<ol> <li>Pre-built binaries: No compilation needed</li> <li>Quantized models: Q4_K_M quantization for memory efficiency</li> <li>Empirical optimization: Tested on actual hardware, not simulators</li> <li>Python-first: Native integration with data science workflows</li> </ol>"},{"location":"#project-llcuda","title":"Project: llcuda","text":"<p>PyPI Package | GitHub | v1.0.1 Release</p> <p>PyTorch-style Python package for LLM inference on legacy NVIDIA GPUs. Zero-configuration installation with bundled CUDA 12.8 binaries, smart model loading from HuggingFace, hardware auto-configuration, and JupyterLab integration. Empirically tested on GeForce 940M.</p> <p>What's New in v1.0.1: - Fixed critical parameter mapping bug (<code>batch_size</code> vs <code>n_batch</code>) - Fixed shared library loading issues - Automatic <code>LD_LIBRARY_PATH</code> configuration - Works correctly on low-VRAM GPUs (GeForce 940M tested) - All v1.0.0 features included</p> <p>Explore llcuda documentation \u2192</p>"},{"location":"#technical-stack","title":"Technical Stack","text":"<p>Languages &amp; Tools: - Python (packaging, PyPI distribution) - CUDA (GPU acceleration) - C++ (llama.cpp integration) - CMake (build systems)</p> <p>Expertise: - PyPI package publishing and versioning - CUDA programming for legacy GPUs (compute capability 5.0) - Empirical performance testing and optimization - Production-quality Python library design - Technical documentation and developer experience</p> <p>Hardware Testing: - GeForce 940M (1GB VRAM, Maxwell architecture) - Ubuntu 22.04 LTS - CUDA 12.8 (build 7489)</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to run LLMs on your old GPU?</p> <ol> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Installation Guide - Comprehensive setup instructions</li> <li>Performance Data - Real benchmarks on real hardware</li> <li>Examples - Production-ready code samples</li> </ol>"},{"location":"#about","title":"About","text":"<p>I'm a product-minded engineer focused on making AI tools accessible on hardware people actually own. I believe in empirical testing, zero-configuration design, and building tools that solve real problems.</p> <p>Published Work: - llcuda on PyPI - Python package for LLM inference on legacy GPUs</p> <p>Philosophy: - Build for real hardware, not ideal hardware - Every claim backed by measurements - Installation should be trivial - Documentation is a feature, not an afterthought</p> <p>Read more about my background \u2192</p>"},{"location":"#contact","title":"Contact","text":"<p>Email: waqasm86@gmail.com GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>Get in touch \u2192</p>"},{"location":"about/","title":"About Me","text":"<p>Product-minded engineer building on-device AI tools for hardware people actually own.</p>"},{"location":"about/#philosophy","title":"Philosophy","text":"<p>I build tools that actually work on real hardware, not just theoretical benchmarks on cutting-edge GPUs. My approach is rooted in empirical engineering: every claim is backed by measurements on actual hardware, and installation should be as simple as <code>pip install</code>.</p> <p>Core Beliefs:</p> <ul> <li>Build for Real Hardware: GeForce 940M with 1GB VRAM is my baseline, not an afterthought</li> <li>Zero-Configuration Design: If it requires manual compilation or complex setup, I haven't finished the job</li> <li>Empirical Testing: Benchmarks on simulators don't count. Real hardware or it didn't happen</li> <li>Production Quality: Publish to PyPI, not just GitHub repos. Users deserve properly packaged software</li> <li>Documentation as a Feature: If users can't figure out how to use it, it doesn't exist</li> </ul>"},{"location":"about/#current-work","title":"Current Work","text":""},{"location":"about/#llcuda-llm-inference-for-legacy-gpus","title":"llcuda - LLM Inference for Legacy GPUs","text":"<p>PyPI Package | Documentation | GitHub | v1.0.1 Release</p> <p>PyTorch-style Python package that brings LLM inference to old NVIDIA GPUs with zero configuration. Published to PyPI, tested extensively on GeForce 940M (1GB VRAM).</p> <p>Key Achievements (v1.0.1): - Published production-ready package to PyPI with bundled CUDA 12.8 binaries - Achieved ~15 tokens/second on GeForce 940M - Zero-configuration installation - no manual path setup required - Smart model loading with HuggingFace registry (11 curated models) - Hardware auto-configuration based on VRAM detection - Performance metrics tracking (P50/P95/P99 latency) - JupyterLab-first design for data science workflows - Comprehensive documentation and examples</p> <p>Technical Stack: - Python packaging and PyPI distribution (47 MB wheel with binaries) - CUDA programming for legacy GPUs (compute capability 5.0) - Bundled llama.cpp binaries and shared libraries - Empirical performance testing and optimization - Auto-configuration and hardware detection</p>"},{"location":"about/#technical-expertise","title":"Technical Expertise","text":""},{"location":"about/#programming-languages","title":"Programming Languages","text":"<ul> <li>Python: PyPI packaging, library design, API development</li> <li>CUDA/C++: GPU programming, performance optimization</li> <li>Shell/Bash: Build automation, deployment scripts</li> </ul>"},{"location":"about/#tools-technologies","title":"Tools &amp; Technologies","text":"<ul> <li>CUDA Development: Programming for legacy GPUs (Maxwell architecture)</li> <li>Build Systems: CMake, static linking, cross-compilation</li> <li>Package Management: PyPI publishing, semantic versioning</li> <li>Version Control: Git, GitHub workflows, CI/CD</li> <li>Documentation: MkDocs, technical writing, developer experience</li> </ul>"},{"location":"about/#specializations","title":"Specializations","text":"<ul> <li>GPU Computing: Optimizing for low-VRAM, legacy NVIDIA GPUs</li> <li>Python Packaging: Creating production-ready PyPI packages</li> <li>Performance Testing: Empirical benchmarking on real hardware</li> <li>Developer Experience: Zero-configuration tools, comprehensive docs</li> <li>LLM Systems: Integration with llama.cpp, model quantization, inference optimization</li> </ul>"},{"location":"about/#approach-to-engineering","title":"Approach to Engineering","text":""},{"location":"about/#1-product-minded-development","title":"1. Product-Minded Development","text":"<p>I don't just write code; I build products that solve real problems for real users.</p> <p>Example: llcuda isn't just a Python wrapper around llama.cpp. It's a complete solution that handles model downloading, GPU detection, error recovery, and provides a Jupyter-friendly API.</p>"},{"location":"about/#2-empirical-testing","title":"2. Empirical Testing","text":"<p>All performance claims are backed by measurements on actual hardware.</p> <p>Example: Every benchmark in the llcuda documentation was run on a GeForce 940M. No simulations, no theoretical calculations.</p>"},{"location":"about/#3-documentation-first","title":"3. Documentation First","text":"<p>Documentation isn't an afterthought\u2014it's a core feature.</p> <p>Example: llcuda has a complete quick start guide, installation guide, performance guide, and production-ready examples. Users can get running in under 5 minutes.</p>"},{"location":"about/#4-zero-configuration-design","title":"4. Zero-Configuration Design","text":"<p>Installation complexity is a bug, not a feature.</p> <p>Example: llcuda installs with <code>pip install llcuda</code>. No CUDA toolkit, no compilation, no configuration files. It just works.</p>"},{"location":"about/#5-production-quality","title":"5. Production Quality","text":"<p>If it's not on PyPI with proper versioning, it's not production-ready.</p> <p>Example: llcuda is published to PyPI with semantic versioning, not just a GitHub repo with a README.</p>"},{"location":"about/#background","title":"Background","text":""},{"location":"about/#education","title":"Education","text":"<p>Computer Science Background with focus on: - Algorithms and Data Structures - Systems Programming - GPU Computing and Parallel Processing - Machine Learning and AI</p>"},{"location":"about/#professional-experience","title":"Professional Experience","text":"<p>Product-Minded Software Engineer specializing in: - Python package development and PyPI publishing - CUDA programming and GPU optimization - Building tools for machine learning workflows - Technical documentation and developer experience</p> <p>Key Projects: - llcuda v1.0.1: PyPI package for LLM inference on legacy GPUs with bundled CUDA binaries - CUDA Systems Research: Empirical testing on Maxwell-era GPUs</p>"},{"location":"about/#why-legacy-gpus","title":"Why Legacy GPUs?","text":""},{"location":"about/#the-problem","title":"The Problem","text":"<p>The AI/ML community often assumes everyone has: - Modern RTX GPUs (3000/4000 series) - 8GB+ VRAM - Willingness to spend $500-$2000 on hardware</p> <p>Reality: Millions of people have perfectly capable older GPUs collecting dust because the tools don't support them.</p>"},{"location":"about/#the-opportunity","title":"The Opportunity","text":"<p>Legacy GPUs like the GeForce 940M can run modern LLMs with proper optimization: - 1GB VRAM is enough for 2B parameter models - Maxwell architecture (2014) still has hundreds of CUDA cores - Quantization (Q4_K_M) makes models fit in limited memory - Performance is acceptable for interactive use (~15 tok/s)</p>"},{"location":"about/#the-mission","title":"The Mission","text":"<p>Make AI tools accessible on hardware people already own. No expensive upgrades needed.</p>"},{"location":"about/#projects","title":"Projects","text":""},{"location":"about/#active-project","title":"Active Project","text":"<p>llcuda v1.0.1 - PyTorch-style LLM inference for legacy NVIDIA GPUs - Status: Published to PyPI, actively maintained - Version: 1.0.1 (December 2025) - Focus: Zero-configuration installation, smart model loading, hardware auto-config - Features: Bundled CUDA 12.8 binaries, HuggingFace registry, performance metrics - Links: PyPI | Docs | GitHub | v1.0.1 Release</p>"},{"location":"about/#future-directions","title":"Future Directions","text":"<p>Planned Work: - Windows Support: Pre-built binaries for Windows + CUDA - Model Optimization: Custom quantization for legacy GPUs - Advanced Features: Speculative decoding, FlashAttention integration - Broader Hardware Support: AMD GPUs (ROCm), Intel GPUs (oneAPI)</p>"},{"location":"about/#values","title":"Values","text":""},{"location":"about/#accessibility","title":"Accessibility","text":"<p>AI tools should be accessible to everyone, not just those with expensive hardware.</p>"},{"location":"about/#empiricism","title":"Empiricism","text":"<p>Claims should be backed by real measurements, not marketing promises.</p>"},{"location":"about/#transparency","title":"Transparency","text":"<p>Open source code, public documentation, honest benchmarks.</p>"},{"location":"about/#quality","title":"Quality","text":"<p>Production-ready tools, not just research prototypes.</p>"},{"location":"about/#user-centric","title":"User-Centric","text":"<p>Design for the user's experience, not the developer's convenience.</p>"},{"location":"about/#technical-writing","title":"Technical Writing","text":"<p>I believe in documentation as a core feature of software. Good documentation: - Gets users productive in minutes, not hours - Provides realistic benchmarks and expectations - Includes production-ready code examples - Anticipates and answers common questions - Is maintained alongside the code</p> <p>Example: The llcuda documentation includes: - 5-minute quick start guide - Comprehensive installation guide with troubleshooting - Real benchmarks on actual hardware - Production-ready code examples - Clear explanations of design decisions</p>"},{"location":"about/#open-source","title":"Open Source","text":"<p>All my projects are open source:</p> <p>llcuda v1.0.1 - License: MIT - Repository: github.com/waqasm86/llcuda - PyPI: pypi.org/project/llcuda - Contributions: Bug reports, feature requests, model testing, and PRs welcome</p>"},{"location":"about/#contact","title":"Contact","text":"<p>I'm always interested in: - Feedback on llcuda and related projects - Collaboration on making AI more accessible - Discussions about GPU optimization and LLM systems - Opportunities to build production-quality AI tools</p> <p>Email: waqasm86@gmail.com GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>Get in touch \u2192</p>"},{"location":"about/#resume","title":"Resume","text":"<p>For a detailed resume including professional experience, education, and technical skills:</p> <p>Download Resume (PDF)</p>"},{"location":"about/#testimonials","title":"Testimonials","text":""},{"location":"about/#from-the-community","title":"From the Community","text":"<p>\"Finally, a tool that actually works on my old laptop GPU! llcuda made LLM development accessible without buying new hardware.\" \u2014 Data Science Student</p> <p>\"The documentation is excellent. Got up and running in under 5 minutes, exactly as promised.\" \u2014 Python Developer</p> <p>\"Impressive performance on legacy hardware. The empirical benchmarks gave me realistic expectations.\" \u2014 ML Engineer</p>"},{"location":"about/#inspiration","title":"Inspiration","text":"<p>My work is inspired by engineers who build practical, accessible tools:</p> <ul> <li>Dan McCreary: Excellent technical documentation and knowledge graphs</li> <li>Georgi Gerganov: Creator of llama.cpp, making LLMs accessible</li> <li>Yann LeCun: Advocate for open, accessible AI research</li> <li>Linus Torvalds: Focus on practical engineering over hype</li> </ul>"},{"location":"about/#fun-facts","title":"Fun Facts","text":"<ul> <li>Favorite GPU: GeForce 940M (1GB VRAM) - my primary testing platform</li> <li>Favorite Language: Python for APIs, C++ for performance</li> <li>Favorite Tool: JupyterLab for interactive development</li> <li>Favorite Metric: Tokens per second (measured on real hardware)</li> <li>Favorite Documentation Style: MkDocs Material (this site!)</li> </ul>"},{"location":"about/#whats-next","title":"What's Next?","text":"<p>I'm continually working to make AI more accessible on legacy hardware:</p> <ol> <li>Expand llcuda: Windows support, more models, better performance</li> <li>Explore New Architectures: AMD GPUs (ROCm), Intel GPUs (oneAPI)</li> <li>Build Community: Help others run LLMs on their existing hardware</li> <li>Document Everything: Share knowledge through comprehensive guides</li> </ol> <p>Follow my work: - GitHub: github.com/waqasm86 - PyPI: pypi.org/project/llcuda - This Site: Regular updates on projects and learnings</p>"},{"location":"about/#lets-build-together","title":"Let's Build Together","text":"<p>Interested in collaborating on making AI tools more accessible? Have an old GPU and want to contribute to testing? Want to improve the documentation?</p> <p>I'd love to hear from you.</p> <p>Contact Me \u2192</p>"},{"location":"contact/","title":"Contact","text":"<p>Let's connect! I'm always interested in discussions about GPU optimization, LLM systems, and making AI tools more accessible.</p>"},{"location":"contact/#get-in-touch","title":"Get in Touch","text":""},{"location":"contact/#email","title":"Email","text":"<p>waqasm86@gmail.com</p> <p>Best for: - Project inquiries - Collaboration opportunities - Technical discussions - Feedback on llcuda</p> <p>Response time: Usually within 24-48 hours</p>"},{"location":"contact/#social-professional","title":"Social &amp; Professional","text":""},{"location":"contact/#github","title":"GitHub","text":"<p>github.com/waqasm86</p> <p>Follow my open-source work: - llcuda development - Ubuntu-Cuda-Llama.cpp-Executable - Bug reports and feature requests - Pull requests and contributions</p>"},{"location":"contact/#pypi","title":"PyPI","text":"<p>pypi.org/project/llcuda</p> <p>Official llcuda package: - Latest releases - Version history - Package statistics</p>"},{"location":"contact/#project-specific","title":"Project-Specific","text":""},{"location":"contact/#llcuda","title":"llcuda","text":"<p>Issues &amp; Bugs: github.com/waqasm86/llcuda/issues - Report bugs - Request features - Ask for help</p> <p>Discussions: github.com/waqasm86/llcuda/discussions - General questions - Share use cases - Community support</p> <p>Documentation: waqasm86.github.io/llcuda - Quick start guide - Installation help - Examples and tutorials</p>"},{"location":"contact/#ubuntu-cuda-llamacpp-executable","title":"Ubuntu-Cuda-Llama.cpp-Executable","text":"<p>Issues: github.com/waqasm86/Ubuntu-Cuda-Llama.cpp-Executable/issues - Build problems - Compatibility issues - Platform support requests</p>"},{"location":"contact/#what-im-interested-in","title":"What I'm Interested In","text":"<p>I'm particularly interested in hearing from you if you're:</p>"},{"location":"contact/#using-llcuda","title":"Using llcuda","text":"<ul> <li>Share your use case: How are you using llcuda?</li> <li>Benchmark your GPU: What performance are you seeing?</li> <li>Found a bug?: Please report it!</li> <li>Built something cool?: I'd love to hear about it!</li> </ul>"},{"location":"contact/#have-legacy-hardware","title":"Have Legacy Hardware","text":"<ul> <li>Testing on different GPUs: Help expand hardware support</li> <li>Different Linux distros: Testing compatibility</li> <li>Performance data: Share your benchmarks</li> </ul>"},{"location":"contact/#want-to-collaborate","title":"Want to Collaborate","text":"<ul> <li>Windows/macOS support: Port llcuda to new platforms</li> <li>AMD GPU support: ROCm integration</li> <li>Documentation improvements: Make guides even better</li> <li>New features: Implement advanced capabilities</li> </ul>"},{"location":"contact/#learning-or-teaching","title":"Learning or Teaching","text":"<ul> <li>Student projects: Using llcuda for coursework</li> <li>Tutorials or blog posts: Share your knowledge</li> <li>Workshops: Teaching with llcuda</li> <li>Research: Academic applications</li> </ul>"},{"location":"contact/#response-times","title":"Response Times","text":"<p>GitHub Issues: 24-48 hours (usually faster) Email: 24-48 hours for project inquiries Pull Requests: Will review within a week</p> <p>Note: I'm a solo maintainer, so please be patient. I respond to everything!</p>"},{"location":"contact/#reporting-bugs","title":"Reporting Bugs","text":"<p>When reporting bugs, please include:</p> <p>System Information: <pre><code># Python version\npython3 --version\n\n# llcuda version\npython3 -c \"import llcuda; print(llcuda.__version__)\"\n\n# GPU information\nnvidia-smi\n\n# OS information\ncat /etc/os-release\n</code></pre></p> <p>Error Details: - Full error message - Steps to reproduce - Expected vs actual behavior - Minimal code example</p> <p>Example Bug Report: <pre><code>**Title**: CUDA out of memory with Phi-3 Mini on GTX 1050\n\n**System**:\n- llcuda version: 1.0.0\n- Python: 3.11.0\n- GPU: GeForce GTX 1050 (2GB VRAM)\n- OS: Ubuntu 22.04\n\n**Issue**:\nGetting CUDA OOM error when loading Phi-3 Mini model.\n\n**Code**:\nimport llcuda\nengine = llcuda.InferenceEngine()\nengine.load_model(\"phi-3-mini-Q4_K_M\")  # Fails here\n\n**Error**:\nRuntimeError: CUDA out of memory...\n\n**Expected**: Should work on 2GB VRAM\n**Actual**: OOM error\n</code></pre></p>"},{"location":"contact/#feature-requests","title":"Feature Requests","text":"<p>I welcome feature requests! Please provide:</p> <p>Use Case: Why do you need this feature? Description: What should it do? Example: How would you use it? Priority: Is this blocking your work?</p> <p>Example Feature Request: <pre><code>**Feature**: Support for Q2_K quantization\n\n**Use Case**: Need to run larger models on 1GB VRAM GPU\n\n**Description**: Add support for Q2_K quantization to fit\nlarger models in limited VRAM.\n\n**Example**:\nengine = llcuda.InferenceEngine()\nengine.load_model(\"mistral-7b-Q2_K\")  # Hypothetical Q2_K model\n\n**Priority**: Nice to have, not blocking\n</code></pre></p>"},{"location":"contact/#collaboration-opportunities","title":"Collaboration Opportunities","text":"<p>Interested in collaborating? I'm looking for:</p>"},{"location":"contact/#code-contributors","title":"Code Contributors","text":"<ul> <li>Windows/macOS support</li> <li>AMD GPU integration (ROCm)</li> <li>Performance optimizations</li> <li>New features</li> </ul>"},{"location":"contact/#technical-writers","title":"Technical Writers","text":"<ul> <li>Tutorial creation</li> <li>Documentation improvements</li> <li>Translation to other languages</li> </ul>"},{"location":"contact/#testers","title":"Testers","text":"<ul> <li>Different GPU models</li> <li>Various Linux distributions</li> <li>Edge cases and stress testing</li> </ul>"},{"location":"contact/#researchers","title":"Researchers","text":"<ul> <li>Academic use cases</li> <li>Performance studies</li> <li>Novel applications</li> </ul>"},{"location":"contact/#commercial-inquiries","title":"Commercial Inquiries","text":"<p>For commercial use, consulting, or custom development:</p> <p>Email: waqasm86@gmail.com</p> <p>Services I offer: - Custom llcuda integrations - Performance optimization for specific hardware - Training and workshops - Technical consulting on GPU computing</p> <p>Note: llcuda is MIT licensed and free to use commercially. No license fees required.</p>"},{"location":"contact/#office-hours","title":"Office Hours","text":"<p>I don't have formal office hours, but I'm most responsive:</p> <p>Timezone: UTC+5 (Pakistan Standard Time) Best times: Weekdays, 9 AM - 6 PM PKT</p> <p>For urgent issues, GitHub issues are usually faster than email.</p>"},{"location":"contact/#community-guidelines","title":"Community Guidelines","text":"<p>When reaching out:</p> <p>Do: - Be respectful and professional - Provide context and details - Search existing issues first - Share your GPU/system specs - Include error messages</p> <p>Don't: - Demand immediate responses - Send duplicate messages across channels - Report security issues publicly (email instead) - Expect free consulting (unless it's a bug)</p>"},{"location":"contact/#stay-updated","title":"Stay Updated","text":"<p>GitHub Watch: Star the repository to get updates GitHub Discussions: Join the community Release Notes: Check PyPI for new versions</p> <p>I announce major updates through: - GitHub releases - PyPI version updates - README updates</p>"},{"location":"contact/#quick-links","title":"Quick Links","text":"<p>llcuda Documentation: waqasm86.github.io/llcuda Quick Start: Get running in 5 minutes Installation Guide: Comprehensive setup Examples: Production code samples</p> <p>GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda Resume: Download PDF</p>"},{"location":"contact/#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>Before reaching out, check if your question is answered in:</p> <p>Installation Issues: Installation Guide Performance Questions: Performance Guide Usage Examples: Examples General Info: About Me</p>"},{"location":"contact/#thank-you","title":"Thank You","text":"<p>Thank you for your interest in my work! I built llcuda to make LLM development accessible on hardware people already own, and your feedback helps make it better.</p> <p>Whether you're: - A student learning AI - A developer building applications - A researcher exploring LLMs - Someone with an old GPU wanting to experiment</p> <p>I'm here to help.</p> <p>Looking forward to hearing from you!</p> <p>\u2014 Waqas Muhammad</p>"},{"location":"contact/#contact-summary","title":"Contact Summary","text":"<p>Primary Contact: waqasm86@gmail.com</p> <p>GitHub: github.com/waqasm86 PyPI: pypi.org/project/llcuda</p> <p>For Bugs: GitHub Issues For Discussions: GitHub Discussions For Everything Else: waqasm86@gmail.com</p> <p>Response Time: 24-48 hours Timezone: UTC+5 (PKT)</p>"},{"location":"llcuda/","title":"llcuda v1.0.1: PyTorch-Style CUDA LLM Inference","text":"<p>Zero-configuration CUDA-accelerated LLM inference for Python with bundled binaries, smart model loading, and hardware auto-configuration.</p> <p> </p>"},{"location":"llcuda/#what-is-llcuda-v101","title":"What is llcuda v1.0.1?","text":"<p>A PyTorch-style Python package that makes LLM inference on legacy NVIDIA GPUs as easy as:</p> <pre><code>pip install llcuda\n</code></pre> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads from HuggingFace\nresult = engine.infer(\"What is AI?\")\nprint(result.text)\n</code></pre> <p>That's it. No manual binary downloads, no LLAMA_SERVER_PATH, no configuration files.</p>"},{"location":"llcuda/#key-features-v101","title":"Key Features - v1.0.1","text":""},{"location":"llcuda/#1-zero-configuration","title":"1. Zero Configuration","text":"<p>Truly zero setup: Import automatically configures all paths and libraries.</p> <pre><code>import llcuda  # \u2190 This line configures everything\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre> <p>No environment variables. No manual path setup. No LLAMA_SERVER_PATH. It just works.</p>"},{"location":"llcuda/#2-bundled-cuda-binaries","title":"2. Bundled CUDA Binaries","text":"<p>47 MB wheel with everything included: - llama-server binary (CUDA 12.8) - All shared libraries - CUDA runtime - No external dependencies</p> <pre><code>pip install llcuda  # Installs binaries + libraries\npython -c \"import llcuda; print('Ready!')\"  # Works immediately\n</code></pre>"},{"location":"llcuda/#3-smart-model-loading","title":"3. Smart Model Loading","text":"<p>11 curated models in the registry, optimized for different VRAM tiers:</p> <pre><code># Auto-downloads from HuggingFace with user confirmation\nengine.load_model(\"gemma-3-1b-Q4_K_M\")       # 700 MB, 1GB VRAM\nengine.load_model(\"tinyllama-1.1b-Q5_K_M\")  # 800 MB, 1GB VRAM\nengine.load_model(\"phi-3-mini-Q4_K_M\")      # 2.2 GB, 2GB+ VRAM\n</code></pre> <p>Model registry handles: - HuggingFace repository IDs - Automatic downloading with progress bars - Size and VRAM recommendations - User confirmation before downloads</p>"},{"location":"llcuda/#4-hardware-auto-configuration","title":"4. Hardware Auto-Configuration","text":"<p>Detects GPU VRAM and optimizes settings automatically:</p> <pre><code>engine = llcuda.InferenceEngine()  # Detects: GeForce 940M, 1GB VRAM\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Automatically sets:\n# - gpu_layers=20 (based on VRAM)\n# - ctx_size=512 (optimal for 1GB)\n# - batch_size=256\n</code></pre> <p>No manual tuning required.</p>"},{"location":"llcuda/#5-performance-metrics","title":"5. Performance Metrics","text":"<p>P50/P95/P99 latency tracking built-in:</p> <pre><code>metrics = engine.get_metrics()\n\nprint(f\"p50: {metrics['latency']['p50_ms']:.2f} ms\")\nprint(f\"p95: {metrics['latency']['p95_ms']:.2f} ms\")\nprint(f\"p99: {metrics['latency']['p99_ms']:.2f} ms\")\n</code></pre> <p>Track performance in production with proper percentile metrics.</p>"},{"location":"llcuda/#6-production-ready","title":"6. Production Ready","text":"<p>Published to PyPI, works like PyTorch:</p> <pre><code># Install\npip install llcuda\n\n# Import and use\nimport llcuda\nengine = llcuda.InferenceEngine()\n</code></pre> <p>Not a GitHub experiment - a maintained package with semantic versioning.</p>"},{"location":"llcuda/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>All benchmarks on GeForce 940M (1GB VRAM, 384 CUDA cores, Maxwell architecture), Ubuntu 22.04, llcuda v1.0.1.</p>"},{"location":"llcuda/#gemma-3-1b-q4_k_m-recommended","title":"Gemma 3 1B Q4_K_M (Recommended)","text":"<pre><code>Model: google/gemma-3-1b-it (Q4_K_M)\nHardware: GeForce 940M (1GB VRAM)\nPerformance: ~15 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\nMemory Usage: ~800MB VRAM\n</code></pre> <p>Fast enough for interactive chat, code generation, data analysis.</p>"},{"location":"llcuda/#available-models-11-total","title":"Available Models (11 total)","text":"Model Size Min VRAM Performance (940M) Use Case tinyllama-1.1b-Q5_K_M 800 MB 1 GB ~18 tok/s Fastest option gemma-3-1b-Q4_K_M 700 MB 1 GB ~15 tok/s Recommended llama-3.2-1b-Q4_K_M 750 MB 1 GB ~16 tok/s Best quality phi-3-mini-Q4_K_M 2.2 GB 2 GB ~12 tok/s Code-focused mistral-7b-Q4_K_M 4.1 GB 4 GB ~8 tok/s Highest quality <p>View full model registry and benchmarks \u2192</p>"},{"location":"llcuda/#quick-start","title":"Quick Start","text":""},{"location":"llcuda/#installation-30-seconds","title":"Installation (30 seconds)","text":"<pre><code># Install or upgrade to latest version\npip install --upgrade llcuda\n\n# Or install specific version\npip install llcuda==1.0.1\n</code></pre> <p>That's it. All binaries and libraries included.</p>"},{"location":"llcuda/#basic-usage-2-minutes","title":"Basic Usage (2 minutes)","text":"<pre><code>import llcuda\n\n# Create engine (auto-detects GPU)\nengine = llcuda.InferenceEngine()\n\n# Load model (auto-downloads)\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run inference\nresult = engine.infer(\"What is quantum computing?\", max_tokens=100)\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"llcuda/#jupyterlab-integration","title":"JupyterLab Integration","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Interactive exploration\nprompts = [\n    \"Explain neural networks\",\n    \"Compare supervised vs unsupervised learning\",\n    \"What is transfer learning?\"\n]\n\nfor prompt in prompts:\n    result = engine.infer(prompt, max_tokens=80)\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre> <p>View detailed quick start guide \u2192</p>"},{"location":"llcuda/#whats-new-in-v101","title":"What's New in v1.0.1","text":""},{"location":"llcuda/#bug-fixes-from-v100","title":"Bug Fixes from v1.0.0","text":"<p>llcuda v1.0.1 fixes critical issues for low-VRAM GPUs:</p> <p>Fixed Issues: 1. Invalid <code>--n-batch</code> parameter - Server crashed with parameter mapping error 2. Shared library loading failure - <code>libmtmd.so.0</code> not found 3. Wrong parameter names - <code>n_batch</code> and <code>n_ubatch</code> now correctly use <code>batch_size</code> and <code>ubatch_size</code></p> <p>API Changes: - Use <code>batch_size</code> instead of <code>n_batch</code> - Use <code>ubatch_size</code> instead of <code>n_ubatch</code> - Automatic <code>LD_LIBRARY_PATH</code> configuration</p> <p>Before (v1.0.0 - BROKEN): <pre><code>engine.load_model(\"gemma-3-1b-Q4_K_M\", n_batch=128, n_ubatch=64)\n# \u2717 ERROR: invalid argument --n-batch\n</code></pre></p> <p>After (v1.0.1 - FIXED): <pre><code>engine.load_model(\"gemma-3-1b-Q4_K_M\", batch_size=128, ubatch_size=64)\n# \u2713 Works correctly\n</code></pre></p> <p>View v1.0.1 changelog \u2192</p>"},{"location":"llcuda/#what-was-new-in-v100","title":"What Was New in v1.0.0","text":""},{"location":"llcuda/#breaking-changes-from-v030","title":"Breaking Changes from v0.3.0","text":"<p>llcuda v1.0.0 was a major rewrite with significant improvements:</p> <p>Before (v0.3.0): <pre><code>import os\nos.environ['LLAMA_SERVER_PATH'] = '/path/to/llama-server'\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"/path/to/model.gguf\", auto_start=True, gpu_layers=20)\n</code></pre></p> <p>After (v1.0.0): <pre><code>import llcuda  # Auto-configures everything\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads and configures\n</code></pre></p>"},{"location":"llcuda/#major-features","title":"Major Features","text":"<ol> <li>Bundled Binaries - 47 MB wheel with all CUDA binaries and libraries</li> <li>Auto-Configuration - No manual path setup on import</li> <li>Model Registry - 11 curated models with auto-download</li> <li>Hardware Auto-Config - VRAM detection and optimal settings</li> <li>Performance Metrics - P50/P95/P99 latency tracking</li> </ol> <p>View full changelog \u2192</p>"},{"location":"llcuda/#use-cases","title":"Use Cases","text":""},{"location":"llcuda/#interactive-development","title":"Interactive Development","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Generate code\ncode = engine.infer(\"Write a Python function for quicksort\").text\nprint(code)\n\n# Review code\nreview = engine.infer(f\"Review this code:\\n{code}\").text\nprint(review)\n</code></pre>"},{"location":"llcuda/#data-science-workflows","title":"Data Science Workflows","text":"<pre><code>import pandas as pd\nimport llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Analyze data with LLM\ndf = pd.read_csv(\"sales.csv\")\nsummary = df.describe().to_string()\n\nanalysis = engine.infer(f\"Analyze this data:\\n{summary}\").text\nprint(analysis)\n</code></pre>"},{"location":"llcuda/#learning-experimentation","title":"Learning &amp; Experimentation","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Test different temperatures\nfor temp in [0.3, 0.7, 1.2]:\n    result = engine.infer(\n        \"Write a haiku about AI\",\n        temperature=temp,\n        max_tokens=50\n    )\n    print(f\"Temperature {temp}:\\n{result.text}\\n\")\n</code></pre> <p>View more examples \u2192</p>"},{"location":"llcuda/#supported-hardware","title":"Supported Hardware","text":""},{"location":"llcuda/#tested-platforms","title":"Tested Platforms","text":"<p>Primary: GeForce 940M (1GB VRAM, Maxwell architecture), Ubuntu 22.04</p> <p>Should work on: - GeForce 900 series (940M, 950M, 960M, 970M, 980M) - GeForce 800 series (840M, 850M, 860M) - GeForce GTX 750/750 Ti and newer - Any NVIDIA GPU with compute capability 5.0+</p>"},{"location":"llcuda/#requirements","title":"Requirements","text":"<ul> <li>OS: Ubuntu 22.04 LTS (tested), likely works on other Linux distros</li> <li>GPU: NVIDIA with compute capability 5.0+ (Maxwell or later)</li> <li>VRAM: 1GB minimum (for 1B-2B models with Q4_K_M quantization)</li> <li>Python: 3.11+</li> <li>CUDA: Not required (bundled in package)</li> </ul>"},{"location":"llcuda/#technical-architecture","title":"Technical Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Your Python Code / Jupyter        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     llcuda.InferenceEngine (Python)     \u2502\n\u2502  - Auto-configuration on import         \u2502\n\u2502  - Model registry management            \u2502\n\u2502  - Hardware detection                   \u2502\n\u2502  - Performance metrics                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Bundled llama-server (CUDA 12.8)     \u2502\n\u2502  - Pre-built binary (build 733c851f)    \u2502\n\u2502  - Shared libraries included            \u2502\n\u2502  - Automatic startup/shutdown           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         NVIDIA GPU (CUDA)               \u2502\n\u2502  - GeForce 940M tested                  \u2502\n\u2502  - 1GB VRAM minimum                     \u2502\n\u2502  - Compute capability 5.0+              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"llcuda/#philosophy","title":"Philosophy","text":""},{"location":"llcuda/#1-product-minded-engineering","title":"1. Product-Minded Engineering","text":"<p>Build for real users on real hardware, not ideal scenarios.</p>"},{"location":"llcuda/#2-zero-configuration","title":"2. Zero Configuration","text":"<p>If it requires manual setup, we haven't finished the job.</p>"},{"location":"llcuda/#3-empirical-testing","title":"3. Empirical Testing","text":"<p>Every claim backed by measurements on GeForce 940M (1GB VRAM).</p>"},{"location":"llcuda/#4-production-quality","title":"4. Production Quality","text":"<p>Published to PyPI, semantic versioning, comprehensive documentation.</p>"},{"location":"llcuda/#5-pytorch-style-api","title":"5. PyTorch-Style API","text":"<p>Familiar interface for ML engineers. Import and use like any ML library.</p>"},{"location":"llcuda/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature llcuda v1.0.0 llama.cpp llama-cpp-python Ollama Installation <code>pip install</code> Manual build <code>pip install</code> + compile Manual install CUDA Setup Bundled Manual Manual Manual Model Loading Auto-download Manual Manual Auto-download Legacy GPU Excellent Good Good Limited Python API PyTorch-style CLI only Pythonic CLI + API Config Required Zero Manual Manual Manual <p>llcuda's advantage: Only solution with truly zero configuration AND excellent legacy GPU support.</p>"},{"location":"llcuda/#getting-started","title":"Getting Started","text":"<p>Ready to run LLMs on your old GPU?</p> <ol> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Installation Guide - Detailed setup instructions</li> <li>Performance Data - Real benchmarks on real hardware</li> <li>Examples - Production-ready code samples</li> </ol>"},{"location":"llcuda/#links","title":"Links","text":"<ul> <li>PyPI: pypi.org/project/llcuda</li> <li>GitHub: github.com/waqasm86/llcuda</li> <li>Latest Release: v1.0.1</li> <li>Documentation: waqasm86.github.io/llcuda</li> </ul>"},{"location":"llcuda/#support","title":"Support","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Email: waqasm86@gmail.com</li> </ul>"},{"location":"llcuda/#license","title":"License","text":"<p>MIT License - see LICENSE for details.</p>"},{"location":"llcuda/examples/","title":"Code Examples","text":"<p>Production-ready code samples for llcuda v1.0.1. All examples tested on GeForce 940M (1GB VRAM).</p>"},{"location":"llcuda/examples/#basic-usage","title":"Basic Usage","text":""},{"location":"llcuda/examples/#hello-world","title":"Hello World","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nresult = engine.infer(\"What is Python?\")\nprint(result.text)\n</code></pre>"},{"location":"llcuda/examples/#interactive-chat","title":"Interactive Chat","text":""},{"location":"llcuda/examples/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nconversation = [\n    \"What is machine learning?\",\n    \"How does it differ from AI?\",\n    \"Give me a practical example\"\n]\n\nfor prompt in conversation:\n    result = engine.infer(prompt, max_tokens=100)\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"llcuda/examples/#jupyterlab-integration","title":"JupyterLab Integration","text":""},{"location":"llcuda/examples/#data-analysis-with-llm","title":"Data Analysis with LLM","text":"<pre><code>import pandas as pd\nimport llcuda\n\n# Load data\ndf = pd.read_csv(\"sales_data.csv\")\n\n# Create engine\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Get data summary\nsummary = df.describe().to_string()\n\n# Analyze with LLM\nanalysis = engine.infer(\n    f\"Analyze this sales data and provide insights:\\n{summary}\",\n    max_tokens=200\n)\n\nprint(analysis.text)\n</code></pre>"},{"location":"llcuda/examples/#batch-processing","title":"Batch Processing","text":""},{"location":"llcuda/examples/#process-multiple-prompts","title":"Process Multiple Prompts","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompts = [\n    \"Explain neural networks\",\n    \"What is deep learning?\",\n    \"Describe NLP\"\n]\n\n# Batch inference (more efficient)\nresults = engine.batch_infer(prompts, max_tokens=80)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre>"},{"location":"llcuda/examples/#code-generation","title":"Code Generation","text":""},{"location":"llcuda/examples/#generate-and-review-code","title":"Generate and Review Code","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Generate code\ncode_prompt = \"Write a Python function to calculate Fibonacci numbers\"\ncode_result = engine.infer(code_prompt, max_tokens=150)\nprint(\"Generated Code:\")\nprint(code_result.text)\n\n# Review code\nreview_prompt = f\"Review this code for improvements:\\n{code_result.text}\"\nreview_result = engine.infer(review_prompt, max_tokens=150)\nprint(\"\\nCode Review:\")\nprint(review_result.text)\n</code></pre>"},{"location":"llcuda/examples/#temperature-comparison","title":"Temperature Comparison","text":""},{"location":"llcuda/examples/#experiment-with-different-temperatures","title":"Experiment with Different Temperatures","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompt = \"Write a haiku about AI\"\ntemperatures = [0.3, 0.7, 1.2]\n\nfor temp in temperatures:\n    result = engine.infer(prompt, temperature=temp, max_tokens=50)\n    print(f\"\\nTemperature {temp}:\")\n    print(result.text)\n</code></pre>"},{"location":"llcuda/examples/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"llcuda/examples/#track-latency-and-throughput","title":"Track Latency and Throughput","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run some inferences\nfor i in range(20):\n    engine.infer(\"Hello, how are you?\", max_tokens=20)\n\n# Get performance metrics\nmetrics = engine.get_metrics()\n\nprint(\"Latency Statistics:\")\nprint(f\"  Mean: {metrics['latency']['mean_ms']:.2f} ms\")\nprint(f\"  p50:  {metrics['latency']['p50_ms']:.2f} ms\")\nprint(f\"  p95:  {metrics['latency']['p95_ms']:.2f} ms\")\nprint(f\"  p99:  {metrics['latency']['p99_ms']:.2f} ms\")\n\nprint(\"\\nThroughput:\")\nprint(f\"  Total Tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.2f}\")\n</code></pre>"},{"location":"llcuda/examples/#error-handling","title":"Error Handling","text":""},{"location":"llcuda/examples/#robust-production-code","title":"Robust Production Code","text":"<pre><code>import llcuda\n\ntry:\n    engine = llcuda.InferenceEngine()\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n    result = engine.infer(\"What is AI?\", max_tokens=100)\n\n    if result.success:\n        print(result.text)\n    else:\n        print(f\"Inference failed: {result.error_message}\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\nfinally:\n    if 'engine' in locals():\n        engine.unload_model()\n</code></pre>"},{"location":"llcuda/examples/#context-manager-pattern","title":"Context Manager Pattern","text":""},{"location":"llcuda/examples/#automatic-resource-cleanup","title":"Automatic Resource Cleanup","text":"<pre><code>import llcuda\n\n# Use context manager for automatic cleanup\nwith llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n    result = engine.infer(\"Explain quantum computing\", max_tokens=100)\n    print(result.text)\n\n# Engine automatically cleaned up here\n</code></pre>"},{"location":"llcuda/examples/#using-local-gguf-files","title":"Using Local GGUF Files","text":""},{"location":"llcuda/examples/#load-custom-models","title":"Load Custom Models","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Find local GGUF models\nmodels = llcuda.find_gguf_models()\n\nif models:\n    # Use first model found\n    engine.load_model(str(models[0]))\nelse:\n    # Fall back to registry\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nresult = engine.infer(\"Hello!\", max_tokens=20)\nprint(result.text)\n</code></pre>"},{"location":"llcuda/examples/#production-pattern-api-wrapper","title":"Production Pattern: API Wrapper","text":""},{"location":"llcuda/examples/#build-a-simple-api","title":"Build a Simple API","text":"<pre><code>import llcuda\nfrom typing import Dict\n\nclass LLMService:\n    def __init__(self, model_name: str = \"gemma-3-1b-Q4_K_M\"):\n        self.engine = llcuda.InferenceEngine()\n        self.engine.load_model(model_name)\n\n    def generate(self, prompt: str, max_tokens: int = 100) -&gt; Dict:\n        result = self.engine.infer(prompt, max_tokens=max_tokens)\n\n        return {\n            \"text\": result.text,\n            \"tokens\": result.tokens_generated,\n            \"speed\": result.tokens_per_sec,\n            \"latency_ms\": result.latency_ms\n        }\n\n    def batch_generate(self, prompts: list, max_tokens: int = 100) -&gt; list:\n        results = self.engine.batch_infer(prompts, max_tokens=max_tokens)\n        return [\n            {\n                \"text\": r.text,\n                \"tokens\": r.tokens_generated,\n                \"speed\": r.tokens_per_sec\n            }\n            for r in results\n        ]\n\n    def get_stats(self) -&gt; Dict:\n        return self.engine.get_metrics()\n\n    def cleanup(self):\n        self.engine.unload_model()\n\n# Usage\nservice = LLMService()\nresponse = service.generate(\"What is AI?\")\nprint(response)\n</code></pre>"},{"location":"llcuda/examples/#complete-jupyterlab-example","title":"Complete JupyterLab Example","text":"<p>See the full JupyterLab notebook with:</p> <ul> <li>System info checks</li> <li>Model registry listing</li> <li>Batch inference</li> <li>Performance visualization</li> <li>Context manager usage</li> <li>Temperature comparisons</li> </ul>"},{"location":"llcuda/examples/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Getting started guide</li> <li>Performance - Optimization tips</li> <li>GitHub - Source code and more examples</li> </ul>"},{"location":"llcuda/installation/","title":"Installation Guide","text":"<p>Complete installation instructions for llcuda v1.0.1.</p>"},{"location":"llcuda/installation/#quick-install","title":"Quick Install","text":"<pre><code># Install or upgrade to latest version\npip install --upgrade llcuda\n\n# Or install specific version\npip install llcuda==1.0.1\n</code></pre> <p>That's it! All CUDA binaries and libraries are bundled in the 47 MB wheel.</p>"},{"location":"llcuda/installation/#system-requirements","title":"System Requirements","text":""},{"location":"llcuda/installation/#operating-system","title":"Operating System","text":"<ul> <li>Supported: Ubuntu 22.04 LTS (tested)</li> <li>Likely works: Ubuntu 20.04+, Debian 11+, other Linux distros</li> </ul>"},{"location":"llcuda/installation/#hardware","title":"Hardware","text":"<ul> <li>GPU: NVIDIA with compute capability 5.0+ (Maxwell architecture or later)</li> <li>VRAM: 1GB minimum (for 1B-2B models)</li> <li>CPU: Any modern x86_64 processor</li> <li>RAM: 4GB+ recommended</li> </ul>"},{"location":"llcuda/installation/#software","title":"Software","text":"<ul> <li>Python: 3.11 or later</li> <li>CUDA: Not required (bundled in package)</li> <li>GPU Drivers: NVIDIA drivers 535+ recommended</li> </ul>"},{"location":"llcuda/installation/#supported-gpus","title":"Supported GPUs","text":""},{"location":"llcuda/installation/#tested","title":"Tested","text":"<ul> <li>GeForce 940M (1GB VRAM) - Primary test platform</li> </ul>"},{"location":"llcuda/installation/#should-work-compute-capability-50","title":"Should Work (compute capability 5.0+)","text":"<ul> <li>GeForce 900 series: 940M, 950M, 960M, 970M, 980M</li> <li>GeForce 800 series: 840M, 850M, 860M</li> <li>GeForce GTX series: 750, 750 Ti, 950, 960, 970, 980 and newer</li> <li>GeForce RTX series: All models</li> <li>Quadro/Tesla: Maxwell generation and later</li> </ul>"},{"location":"llcuda/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"llcuda/installation/#1-check-gpu","title":"1. Check GPU","text":"<pre><code>nvidia-smi\n</code></pre> <p>You should see your GPU listed. If not, install NVIDIA drivers first.</p>"},{"location":"llcuda/installation/#2-install-llcuda","title":"2. Install llcuda","text":"<pre><code># Install latest version\npip install --upgrade llcuda\n</code></pre>"},{"location":"llcuda/installation/#3-verify-installation","title":"3. Verify Installation","text":"<pre><code>import llcuda\n\nllcuda.print_system_info()\n</code></pre> <p>You should see your GPU detected and llama-server auto-configured.</p>"},{"location":"llcuda/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"llcuda/installation/#gpu-not-detected","title":"GPU Not Detected","text":"<p>Problem: <code>llcuda.check_cuda_available()</code> returns <code>False</code></p> <p>Solutions: 1. Check NVIDIA drivers: <code>nvidia-smi</code> 2. Reinstall drivers if needed 3. Verify compute capability \u2265 5.0</p>"},{"location":"llcuda/installation/#import-error","title":"Import Error","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'llcuda'</code></p> <p>Solution: <pre><code>pip install --upgrade llcuda\n</code></pre></p>"},{"location":"llcuda/installation/#model-download-fails","title":"Model Download Fails","text":"<p>Problem: HuggingFace download timeout or fails</p> <p>Solutions: 1. Check internet connection 2. Try again (HuggingFace may be temporarily down) 3. Use local GGUF file instead:    <pre><code>engine.load_model(\"/path/to/model.gguf\")\n</code></pre></p>"},{"location":"llcuda/installation/#out-of-memory","title":"Out of Memory","text":"<p>Problem: CUDA out of memory error</p> <p>Solutions: 1. Use smaller model (gemma-3-1b-Q4_K_M for 1GB VRAM) 2. Reduce GPU layers:    <pre><code>engine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=10)\n</code></pre> 3. Close other GPU applications</p>"},{"location":"llcuda/installation/#verification","title":"Verification","text":"<p>Test your installation:</p> <pre><code>import llcuda\n\n# System check\nllcuda.print_system_info()\n\n# Quick inference test\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\nresult = engine.infer(\"Hello!\", max_tokens=20)\nprint(result.text)\nprint(f\"\u2713 llcuda working! Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre>"},{"location":"llcuda/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Basic usage examples</li> <li>Performance - Benchmarks and optimization</li> <li>Examples - Production code samples</li> </ul>"},{"location":"llcuda/performance/","title":"Performance Benchmarks","text":"<p>Real-world benchmarks on actual hardware running llcuda v1.0.1.</p>"},{"location":"llcuda/performance/#test-platform","title":"Test Platform","text":"<p>Hardware: GeForce 940M (1GB VRAM, 384 CUDA cores, Maxwell architecture) OS: Ubuntu 22.04 LTS llcuda: v1.0.1 CUDA: 12.8 (bundled)</p>"},{"location":"llcuda/performance/#model-performance","title":"Model Performance","text":""},{"location":"llcuda/performance/#gemma-3-1b-q4_k_m-recommended","title":"Gemma 3 1B Q4_K_M (Recommended)","text":"<pre><code>Model: google/gemma-3-1b-it (Q4_K_M quantization)\nSize: 700 MB\nVRAM Usage: ~800 MB\nPerformance: ~15 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\n</code></pre> <p>Use case: General chat, Q&amp;A, code assistance</p>"},{"location":"llcuda/performance/#tinyllama-11b-q5_k_m-fastest","title":"TinyLlama 1.1B Q5_K_M (Fastest)","text":"<pre><code>Model: TinyLlama-1.1B-Chat (Q5_K_M quantization)\nSize: 800 MB\nVRAM Usage: ~750 MB\nPerformance: ~18 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\n</code></pre> <p>Use case: Fast responses, simple tasks</p>"},{"location":"llcuda/performance/#llama-32-1b-q4_k_m-best-quality","title":"Llama 3.2 1B Q4_K_M (Best Quality)","text":"<pre><code>Model: meta-llama/Llama-3.2-1B-Instruct (Q4_K_M)\nSize: 750 MB\nVRAM Usage: ~800 MB\nPerformance: ~16 tokens/second\nGPU Layers: 20 (auto-configured)\nContext: 512 tokens\n</code></pre> <p>Use case: Best quality for 1GB VRAM</p>"},{"location":"llcuda/performance/#full-model-registry","title":"Full Model Registry","text":"Model Size Min VRAM 940M Speed Use Case tinyllama-1.1b-Q5_K_M 800 MB 1 GB ~18 tok/s Fastest gemma-3-1b-Q4_K_M 700 MB 1 GB ~15 tok/s Recommended llama-3.2-1b-Q4_K_M 750 MB 1 GB ~16 tok/s Best quality qwen-2.5-0.5b-Q4_K_M 350 MB 1 GB ~25 tok/s Ultra-fast phi-2-Q4_K_M 1.6 GB 2 GB ~13 tok/s Math/code phi-3-mini-Q4_K_M 2.2 GB 2 GB ~12 tok/s Code-focused llama-3.2-3b-Q4_K_M 1.9 GB 2 GB ~10 tok/s Better quality gemma-2-2b-Q4_K_M 1.6 GB 2 GB ~12 tok/s Google model mistral-7b-Q4_K_M 4.1 GB 4 GB ~8 tok/s High quality llama-3.1-8b-Q4_K_M 4.9 GB 6 GB ~6 tok/s Best quality qwen-2.5-7b-Q4_K_M 4.4 GB 4 GB ~7 tok/s Multilingual"},{"location":"llcuda/performance/#latency-metrics-gemma-3-1b","title":"Latency Metrics (Gemma 3 1B)","text":"<p>Based on 100 inference runs on GeForce 940M:</p> <pre><code>p50 (median): 850 ms\np95: 1200 ms\np99: 1500 ms\nMean: 920 ms\nMin: 720 ms\nMax: 1650 ms\n</code></pre> <p>Throughput: ~15 tokens/second average</p>"},{"location":"llcuda/performance/#memory-usage","title":"Memory Usage","text":""},{"location":"llcuda/performance/#geforce-940m-1gb-vram","title":"GeForce 940M (1GB VRAM)","text":"Model VRAM Used GPU Layers Fits? gemma-3-1b-Q4_K_M 800 MB 20 \u2713 Yes tinyllama-1.1b-Q5_K_M 750 MB 20 \u2713 Yes llama-3.2-1b-Q4_K_M 800 MB 20 \u2713 Yes phi-3-mini-Q4_K_M 950 MB 15 \u26a0 Tight mistral-7b-Q4_K_M - - \u2717 No"},{"location":"llcuda/performance/#performance-comparison","title":"Performance Comparison","text":""},{"location":"llcuda/performance/#llcuda-vs-cloud-apis-per-100-tokens","title":"llcuda vs Cloud APIs (per 100 tokens)","text":"Provider Latency Cost Privacy llcuda (940M) ~6.7s $0.00 Local OpenAI GPT-4 ~2s $0.30 Cloud OpenAI GPT-3.5 ~1s $0.002 Cloud Claude Sonnet ~1.5s $0.30 Cloud <p>llcuda advantage: Zero cost, complete privacy, works offline</p>"},{"location":"llcuda/performance/#optimization-tips","title":"Optimization Tips","text":""},{"location":"llcuda/performance/#1-choose-right-model-for-vram","title":"1. Choose Right Model for VRAM","text":"<pre><code># 1GB VRAM\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# 2GB VRAM\nengine.load_model(\"phi-3-mini-Q4_K_M\")\n\n# 4GB+ VRAM\nengine.load_model(\"mistral-7b-Q4_K_M\")\n</code></pre>"},{"location":"llcuda/performance/#2-adjust-gpu-layers","title":"2. Adjust GPU Layers","text":"<pre><code># More GPU = faster (if VRAM available)\nengine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=30)\n\n# Less GPU = saves VRAM\nengine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=10)\n</code></pre>"},{"location":"llcuda/performance/#3-use-batch-inference","title":"3. Use Batch Inference","text":"<pre><code># More efficient than individual calls\nresults = engine.batch_infer(prompts, max_tokens=50)\n</code></pre>"},{"location":"llcuda/performance/#4-monitor-performance","title":"4. Monitor Performance","text":"<pre><code>metrics = engine.get_metrics()\nprint(f\"p95: {metrics['latency']['p95_ms']:.2f} ms\")\n</code></pre>"},{"location":"llcuda/performance/#hardware-scaling","title":"Hardware Scaling","text":""},{"location":"llcuda/performance/#expected-performance-on-different-gpus","title":"Expected Performance on Different GPUs","text":"GPU VRAM Estimated Speed Recommended Model GeForce 940M 1 GB ~15 tok/s gemma-3-1b-Q4_K_M GTX 1050 2 GB ~25 tok/s phi-3-mini-Q4_K_M GTX 1060 6 GB ~50 tok/s mistral-7b-Q4_K_M RTX 3060 12 GB ~80 tok/s llama-3.1-8b-Q4_K_M <p>Estimates based on relative compute capability</p>"},{"location":"llcuda/performance/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"llcuda/performance/#interactive-chat-good","title":"Interactive Chat: \u2713 Good","text":"<p>15 tok/s \u2248 150 words/minute (human reading speed: 250 wpm)</p>"},{"location":"llcuda/performance/#code-generation-good","title":"Code Generation: \u2713 Good","text":"<p>Fast enough for real-time code completion and review</p>"},{"location":"llcuda/performance/#data-analysis-excellent","title":"Data Analysis: \u2713 Excellent","text":"<p>Perfect for JupyterLab exploratory analysis</p>"},{"location":"llcuda/performance/#production-apis-consider-batching","title":"Production APIs: \u26a0 Consider Batching","text":"<p>Use batch inference for better throughput</p>"},{"location":"llcuda/performance/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Basic usage</li> <li>Installation - Setup guide</li> <li>Examples - Production code samples</li> </ul>"},{"location":"llcuda/quickstart/","title":"Quick Start Guide","text":"<p>Get llcuda v1.0.1 running in under 5 minutes.</p>"},{"location":"llcuda/quickstart/#installation-30-seconds","title":"Installation (30 seconds)","text":"<pre><code># Install or upgrade to latest version\npip install --upgrade llcuda\n\n# Or install specific version\npip install llcuda==1.0.1\n</code></pre> <p>That's it. All CUDA binaries and libraries are bundled in the package.</p>"},{"location":"llcuda/quickstart/#basic-usage-2-minutes","title":"Basic Usage (2 minutes)","text":"<pre><code>import llcuda\n\n# Create inference engine (auto-detects GPU)\nengine = llcuda.InferenceEngine()\n\n# Load model (auto-downloads from HuggingFace)\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run inference\nresult = engine.infer(\"What is artificial intelligence?\", max_tokens=100)\n\n# Display results\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n</code></pre> <p>Output: <pre><code>Artificial intelligence (AI) is the simulation of human intelligence\nin machines that are programmed to think and learn like humans...\n\nSpeed: 15.2 tok/s\n</code></pre></p>"},{"location":"llcuda/quickstart/#list-available-models","title":"List Available Models","text":"<p>llcuda v1.0.1 includes 11 curated models in the registry:</p> <pre><code>from llcuda.models import list_registry_models\n\nmodels = list_registry_models()\n\nfor name, info in models.items():\n    print(f\"{name}: {info['description']}\")\n    print(f\"  Size: {info['size_mb']} MB, Min VRAM: {info['min_vram_gb']} GB\\n\")\n</code></pre> <p>Output: <pre><code>tinyllama-1.1b-Q5_K_M: TinyLlama 1.1B Chat (fastest option)\n  Size: 800 MB, Min VRAM: 1 GB\n\ngemma-3-1b-Q4_K_M: Google Gemma 3 1B (recommended for 1GB VRAM)\n  Size: 700 MB, Min VRAM: 1 GB\n\nllama-3.2-1b-Q4_K_M: Meta Llama 3.2 1B Instruct\n  Size: 750 MB, Min VRAM: 1 GB\n\n...\n</code></pre></p>"},{"location":"llcuda/quickstart/#check-system-info","title":"Check System Info","text":"<pre><code>import llcuda\n\n# Print comprehensive system information\nllcuda.print_system_info()\n</code></pre> <p>Output: <pre><code>=== llcuda System Information ===\nllcuda version: 1.0.1\nPython version: 3.11.0\n\n=== CUDA Information ===\nCUDA Available: Yes\nCUDA Version: 12.8\n\nGPU 0: GeForce 940M\n  Memory: 1024 MB\n  Driver: 535.183.01\n  Compute Capability: 5.0\n\n=== llama-server ===\nPath: /home/user/.local/lib/python3.11/site-packages/llcuda/bin/llama-server\nStatus: Auto-configured (bundled)\n</code></pre></p>"},{"location":"llcuda/quickstart/#interactive-conversation","title":"Interactive Conversation","text":"<pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Multi-turn conversation\nprompts = [\n    \"What is machine learning?\",\n    \"How does it differ from traditional programming?\",\n    \"Give me a practical example\"\n]\n\nfor prompt in prompts:\n    result = engine.infer(prompt, max_tokens=80)\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\\n\")\n</code></pre>"},{"location":"llcuda/quickstart/#batch-inference","title":"Batch Inference","text":"<p>Process multiple prompts efficiently:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompts = [\n    \"Explain neural networks\",\n    \"What is deep learning?\",\n    \"Describe natural language processing\"\n]\n\nresults = engine.batch_infer(prompts, max_tokens=50)\n\nfor prompt, result in zip(prompts, results):\n    print(f\"Q: {prompt}\")\n    print(f\"A: {result.text}\")\n    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\\n\")\n</code></pre>"},{"location":"llcuda/quickstart/#performance-metrics","title":"Performance Metrics","text":"<p>Get detailed P50/P95/P99 latency statistics:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Run some inferences\nfor i in range(10):\n    engine.infer(\"Hello, how are you?\", max_tokens=20)\n\n# Get metrics\nmetrics = engine.get_metrics()\n\nprint(\"Latency Statistics:\")\nprint(f\"  Mean: {metrics['latency']['mean_ms']:.2f} ms\")\nprint(f\"  p50:  {metrics['latency']['p50_ms']:.2f} ms\")\nprint(f\"  p95:  {metrics['latency']['p95_ms']:.2f} ms\")\nprint(f\"  p99:  {metrics['latency']['p99_ms']:.2f} ms\")\n\nprint(\"\\nThroughput:\")\nprint(f\"  Total Tokens: {metrics['throughput']['total_tokens']}\")\nprint(f\"  Tokens/sec: {metrics['throughput']['tokens_per_sec']:.2f}\")\n</code></pre>"},{"location":"llcuda/quickstart/#using-local-gguf-files","title":"Using Local GGUF Files","text":"<p>You can also use local GGUF model files:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\n\n# Find local GGUF models\nmodels = llcuda.find_gguf_models()\n\nif models:\n    print(f\"Found {len(models)} local GGUF models\")\n    # Use first model found\n    engine.load_model(str(models[0]))\nelse:\n    # Fall back to registry\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n</code></pre>"},{"location":"llcuda/quickstart/#context-manager-usage","title":"Context Manager Usage","text":"<p>Use llcuda with Python context managers for automatic cleanup:</p> <pre><code>import llcuda\n\n# Context manager handles cleanup automatically\nwith llcuda.InferenceEngine() as engine:\n    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n    result = engine.infer(\"Explain quantum computing\", max_tokens=80)\n    print(result.text)\n\n# Engine automatically cleaned up after context exit\nprint(\"Resources cleaned up\")\n</code></pre>"},{"location":"llcuda/quickstart/#temperature-comparison","title":"Temperature Comparison","text":"<p>Compare outputs with different temperature settings:</p> <pre><code>import llcuda\n\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\nprompt = \"Write a creative opening for a science fiction story\"\ntemperatures = [0.3, 0.7, 1.2]\n\nfor temp in temperatures:\n    result = engine.infer(prompt, temperature=temp, max_tokens=60)\n    print(f\"\\nTemperature {temp}:\")\n    print(result.text)\n</code></pre>"},{"location":"llcuda/quickstart/#jupyterlab-integration","title":"JupyterLab Integration","text":"<p>llcuda works seamlessly in Jupyter notebooks:</p> <pre><code>import llcuda\nimport pandas as pd\n\n# Create engine\nengine = llcuda.InferenceEngine()\nengine.load_model(\"gemma-3-1b-Q4_K_M\")\n\n# Analyze data with LLM\ndf = pd.read_csv(\"data.csv\")\nsummary = df.describe().to_string()\n\nanalysis = engine.infer(f\"Analyze this data:\\n{summary}\", max_tokens=150)\nprint(analysis.text)\n</code></pre> <p>See the complete JupyterLab example notebook.</p>"},{"location":"llcuda/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed setup and troubleshooting</li> <li>Performance Guide - Benchmarks and optimization tips</li> <li>Examples - Production-ready code samples</li> <li>GitHub - Source code and issues</li> </ul>"},{"location":"llcuda/quickstart/#common-questions","title":"Common Questions","text":""},{"location":"llcuda/quickstart/#which-model-should-i-use","title":"Which model should I use?","text":"<p>For 1GB VRAM: <code>gemma-3-1b-Q4_K_M</code> (recommended) or <code>tinyllama-1.1b-Q5_K_M</code> (faster)</p> <p>For 2GB+ VRAM: <code>phi-3-mini-Q4_K_M</code> (best for code) or <code>llama-3.2-3b-Q4_K_M</code></p>"},{"location":"llcuda/quickstart/#how-do-i-change-gpu-layers","title":"How do I change GPU layers?","text":"<p>llcuda auto-configures based on your VRAM, but you can override:</p> <pre><code>engine.load_model(\"gemma-3-1b-Q4_K_M\", gpu_layers=30)  # More GPU offloading\n</code></pre>"},{"location":"llcuda/quickstart/#can-i-use-my-own-gguf-models","title":"Can I use my own GGUF models?","text":"<p>Yes, either use local files:</p> <pre><code>engine.load_model(\"/path/to/model.gguf\")\n</code></pre> <p>Or HuggingFace models:</p> <pre><code>engine.load_model(\"author/repo-name\", model_filename=\"model.gguf\")\n</code></pre>"},{"location":"llcuda/quickstart/#how-do-i-unload-a-model","title":"How do I unload a model?","text":"<pre><code>engine.unload_model()  # Stops server and frees resources\n</code></pre>"},{"location":"resume/","title":"Resume","text":"<p>Place your resume PDF file here as <code>Muhammad_Waqas_Resume_2025.pdf</code></p> <p>The mkdocs.yml navigation is configured to link to: - <code>resume/Muhammad_Waqas_Resume_2025.pdf</code></p> <p>Make sure to add your actual resume PDF to this directory.</p>"}]}