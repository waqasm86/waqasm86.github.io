# Mohammad Waqas (waqasm86)

Welcome to my personal engineering hub.

Here I publish **CUDA + C++ + LLM inference** work, benchmarks, and architecture notes.

## Featured Projects

### [llcuda](https://github.com/waqasm86/llcuda) - Python Package
CUDA-accelerated LLM inference for Python with automatic server management. Zero-configuration setup, JupyterLab integration, production-ready performance. Available on [PyPI](https://pypi.org/project/llcuda/).

### CUDA Systems Engineering
- [cuda-nvidia-systems-engg](projects/cuda-nvidia-systems-engg.md) - Production-grade distributed inference system
- CUDA networking experiments for llama.cpp
- Multi-rank scheduling / throughput tests
- Storage pipeline experiments for LLM workloads
