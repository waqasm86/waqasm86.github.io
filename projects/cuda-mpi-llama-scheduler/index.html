
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://waqasm86.github.io/projects/cuda-mpi-llama-scheduler/">
      
      
        <link rel="prev" href="../cuda-openmpi/">
      
      
        <link rel="next" href="../cuda-llm-storage-pipeline/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>cuda-mpi-llama-scheduler - Waqas — CUDA Projects</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#cuda-mpi-llama-scheduler" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Waqas — CUDA Projects" class="md-header__button md-logo" aria-label="Waqas — CUDA Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Waqas — CUDA Projects
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              cuda-mpi-llama-scheduler
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  
  CUDA Projects

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../about/" class="md-tabs__link">
        
  
  
    
  
  About

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Waqas — CUDA Projects" class="md-nav__button md-logo" aria-label="Waqas — CUDA Projects" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Waqas — CUDA Projects
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    CUDA Projects
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    CUDA Projects
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cuda-tcp-llama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    cuda-tcp-llama.cpp
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cuda-openmpi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    cuda-openmpi
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    cuda-mpi-llama-scheduler
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    cuda-mpi-llama-scheduler
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      
        Overview
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    <span class="md-ellipsis">
      
        Features
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hardware-model-configuration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hardware &amp; Model Configuration
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Hardware &amp; Model Configuration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        Test Environment
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-cuda-backed-inference-engine" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. CUDA-backed Inference Engine
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-http-server-layer" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. HTTP Server Layer
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-scheduler-load-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Scheduler &amp; Load Distribution
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-structure" class="md-nav__link">
    <span class="md-ellipsis">
      
        Project Structure
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#dependencies" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dependencies
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#build" class="md-nav__link">
    <span class="md-ellipsis">
      
        Build
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Build">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prerequisites
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run" class="md-nav__link">
    <span class="md-ellipsis">
      
        Run
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Run">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-start-the-llamacpp-server" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Start the llama.cpp Server
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-run-load-tests" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Run Load Tests
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance" class="md-nav__link">
    <span class="md-ellipsis">
      
        Performance
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Performance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#observed-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Observed Metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latency-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      
        Latency Distribution
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        Memory Efficiency
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#api-usage" class="md-nav__link">
    <span class="md-ellipsis">
      
        API Usage
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="API Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#openai-compatible-endpoint" class="md-nav__link">
    <span class="md-ellipsis">
      
        OpenAI-Compatible Endpoint
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#response-format" class="md-nav__link">
    <span class="md-ellipsis">
      
        Response Format
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-rank-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Multi-Rank Scheduling
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi-Rank Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scheduling-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scheduling Strategy
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-2-rank-test" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example: 2-Rank Test
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      
        Optimization Techniques
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-prompt-caching" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Prompt Caching
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-memory-management" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Memory Management
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-quantization" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Quantization
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#benchmarking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Benchmarking
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Benchmarking">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#running-benchmarks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Running Benchmarks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#metrics-collected" class="md-nav__link">
    <span class="md-ellipsis">
      
        Metrics Collected
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#use-cases" class="md-nav__link">
    <span class="md-ellipsis">
      
        Use Cases
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#troubleshooting" class="md-nav__link">
    <span class="md-ellipsis">
      
        Troubleshooting
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Troubleshooting">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#out-of-memory-errors" class="md-nav__link">
    <span class="md-ellipsis">
      
        Out of Memory Errors
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slow-performance" class="md-nav__link">
    <span class="md-ellipsis">
      
        Slow Performance
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#connection-refused" class="md-nav__link">
    <span class="md-ellipsis">
      
        Connection Refused
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-enhancements" class="md-nav__link">
    <span class="md-ellipsis">
      
        Future Enhancements
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Notes
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../cuda-llm-storage-pipeline/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    cuda-llm-storage-pipeline
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../about/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="cuda-mpi-llama-scheduler">cuda-mpi-llama-scheduler</h1>
<p>A production-style GPU inference pipeline for running large language models on constrained hardware with CUDA acceleration and multi-rank scheduling.</p>
<p><a href="https://github.com/waqasm86/cuda-mpi-llama-scheduler">:fontawesome-brands-github: View on GitHub</a>{ .md-button }</p>
<hr />
<h2 id="overview">Overview</h2>
<p>This project demonstrates a complete GPU inference pipeline that combines CUDA acceleration with llama.cpp and quantized models to run large language models efficiently on resource-constrained hardware. It features OpenAI-compatible API endpoints and multi-rank scheduling inspired by MPI-style load distribution.</p>
<p><strong>Key Achievement:</strong></p>
<p>Successfully running a 1B parameter LLM (Gemma-3-1B-IT) on a GPU with only 1GB VRAM while maintaining acceptable performance and providing production-ready HTTP endpoints.</p>
<hr />
<h2 id="features">Features</h2>
<ul>
<li><strong>llama.cpp HTTP Server</strong>: OpenAI-compatible <code>/v1/chat/completions</code> endpoint</li>
<li><strong>CUDA Acceleration</strong>: GPU-accelerated inference on low-VRAM devices</li>
<li><strong>Quantized Models</strong>: GGUF Q4_K_M format for extreme memory efficiency</li>
<li><strong>Multi-rank Scheduling</strong>: MPI-inspired load distribution across concurrent requests</li>
<li><strong>Performance Metrics</strong>: Detailed latency percentiles (p50, p95, p99) and throughput tracking</li>
<li><strong>Prompt Caching</strong>: Optimized handling of repeated requests</li>
<li><strong>Zero External Dependencies</strong>: No Docker, Python, or ML framework requirements (beyond build tools)</li>
</ul>
<hr />
<h2 id="hardware-model-configuration">Hardware &amp; Model Configuration</h2>
<h3 id="test-environment">Test Environment</h3>
<ul>
<li><strong>GPU</strong>: NVIDIA GeForce 940M</li>
<li>Compute Capability 5.0</li>
<li>VRAM: 1GB</li>
<li><strong>Model</strong>: Gemma-3-1B-IT</li>
<li>Format: GGUF Q4_K_M quantization</li>
<li>Size: ~762 MiB</li>
<li>Context: 32k tokens</li>
</ul>
<p>This configuration demonstrates that modern LLM inference is possible even on older, resource-constrained hardware.</p>
<hr />
<h2 id="architecture">Architecture</h2>
<p>The system consists of three layers working together:</p>
<h3 id="1-cuda-backed-inference-engine">1. CUDA-backed Inference Engine</h3>
<ul>
<li>llama.cpp with CUDA support</li>
<li>Quantized model loading (Q4_K_M)</li>
<li>GPU memory management for constrained devices</li>
<li>Kernel execution optimization</li>
</ul>
<h3 id="2-http-server-layer">2. HTTP Server Layer</h3>
<ul>
<li>OpenAI-compatible REST API</li>
<li>JSON request/response handling</li>
<li>Connection management</li>
<li>Error handling and logging</li>
</ul>
<h3 id="3-scheduler-load-distribution">3. Scheduler &amp; Load Distribution</h3>
<ul>
<li>Multi-rank request distribution</li>
<li>Concurrent request handling</li>
<li>Load balancing across ranks</li>
<li>Performance monitoring</li>
</ul>
<hr />
<h2 id="project-structure">Project Structure</h2>
<pre><code>cuda-mpi-llama-scheduler/
├── src/                    # Scheduler and client implementations
│   ├── scheduler.cpp       # Multi-rank scheduling logic
│   └── client.cpp          # HTTP client for testing
├── include/mls/            # Header files
├── scripts/                # Build and execution scripts
│   ├── build.sh           # Compilation script
│   └── run_2ranks.sh      # 2-rank load test
├── docs/                   # Architecture and analysis guides
│   ├── architecture.md    # System design
│   ├── setup.md          # Installation guide
│   └── analysis.md       # Performance analysis
└── README.md
</code></pre>
<hr />
<h2 id="dependencies">Dependencies</h2>
<ul>
<li><strong>llama.cpp</strong>: CUDA-enabled build</li>
<li><strong>CMake</strong>: Build system (3.15+)</li>
<li><strong>Ninja</strong>: Fast build tool</li>
<li><strong>nlohmann/json</strong>: JSON parsing library</li>
<li><strong>CUDA Toolkit</strong>: 12.x</li>
<li><strong>C++17 compiler</strong>: GCC 11+ or Clang 14+</li>
</ul>
<hr />
<h2 id="build">Build</h2>
<pre><code class="language-bash"># Run the build script
./scripts/build.sh
</code></pre>
<p>The build script:
1. Configures CMake with CUDA support
2. Compiles llama.cpp with GPU acceleration
3. Builds the scheduler and client components
4. Links against required libraries</p>
<h3 id="prerequisites">Prerequisites</h3>
<pre><code class="language-bash"># Install build dependencies (Ubuntu/Debian)
sudo apt-get install cmake ninja-build libcurl4-openssl-dev

# Install nlohmann/json
sudo apt-get install nlohmann-json3-dev
</code></pre>
<hr />
<h2 id="run">Run</h2>
<h3 id="1-start-the-llamacpp-server">1. Start the llama.cpp Server</h3>
<pre><code class="language-bash">./llama-server \
    -m /path/to/gemma-3-1b-it-Q4_K_M.gguf \
    --port 8090 \
    --mlock
</code></pre>
<p><strong>Parameters:</strong>
- <code>-m</code>: Path to GGUF model file
- <code>--port</code>: HTTP server port (default: 8090)
- <code>--mlock</code>: Lock model in RAM to prevent swapping</p>
<h3 id="2-run-load-tests">2. Run Load Tests</h3>
<pre><code class="language-bash"># 2-rank concurrent test
./scripts/run_2ranks.sh

# Custom test with N ranks
./build/scheduler --ranks N --endpoint http://localhost:8090
</code></pre>
<hr />
<h2 id="performance">Performance</h2>
<h3 id="observed-metrics">Observed Metrics</h3>
<p>On NVIDIA GeForce 940M (1GB VRAM):</p>
<ul>
<li><strong>Mean Latency</strong>: ~3342ms</li>
<li><strong>Throughput</strong>: 16 tokens/sec</li>
<li><strong>VRAM Usage</strong>: 850-900 MiB</li>
<li><strong>GPU Utilization</strong>: Stable during inference</li>
</ul>
<h3 id="latency-distribution">Latency Distribution</h3>
<ul>
<li><strong>p50</strong>: ~3200ms</li>
<li><strong>p95</strong>: ~4500ms</li>
<li><strong>p99</strong>: ~5200ms</li>
</ul>
<h3 id="memory-efficiency">Memory Efficiency</h3>
<p>The Q4_K_M quantization enables:
- 4-bit weight quantization
- ~762 MiB model footprint
- ~100 MiB headroom for KV cache and activations</p>
<hr />
<h2 id="api-usage">API Usage</h2>
<h3 id="openai-compatible-endpoint">OpenAI-Compatible Endpoint</h3>
<pre><code class="language-bash">curl -X POST http://localhost:8090/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Explain CUDA in one sentence.&quot;}
    ],
    &quot;max_tokens&quot;: 50,
    &quot;temperature&quot;: 0.7
  }'
</code></pre>
<h3 id="response-format">Response Format</h3>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;chatcmpl-123&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;created&quot;: 1234567890,
  &quot;model&quot;: &quot;gemma-3-1b-it&quot;,
  &quot;choices&quot;: [{
    &quot;index&quot;: 0,
    &quot;message&quot;: {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;CUDA is NVIDIA's parallel computing platform...&quot;
    },
    &quot;finish_reason&quot;: &quot;stop&quot;
  }],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 12,
    &quot;completion_tokens&quot;: 45,
    &quot;total_tokens&quot;: 57
  }
}
</code></pre>
<hr />
<h2 id="multi-rank-scheduling">Multi-Rank Scheduling</h2>
<p>The scheduler distributes requests across multiple "ranks" (concurrent workers), inspired by MPI programming models:</p>
<h3 id="scheduling-strategy">Scheduling Strategy</h3>
<ol>
<li><strong>Round-Robin Distribution</strong>: Requests assigned to ranks in rotation</li>
<li><strong>Concurrent Execution</strong>: Each rank processes requests independently</li>
<li><strong>Aggregated Metrics</strong>: Combined latency and throughput statistics</li>
</ol>
<h3 id="example-2-rank-test">Example: 2-Rank Test</h3>
<pre><code class="language-bash"># Terminal 1: Start server
./llama-server -m model.gguf --port 8090

# Terminal 2: Run 2-rank scheduler
./scripts/run_2ranks.sh
</code></pre>
<p>Output shows per-rank and aggregated performance metrics.</p>
<hr />
<h2 id="optimization-techniques">Optimization Techniques</h2>
<h3 id="1-prompt-caching">1. Prompt Caching</h3>
<ul>
<li>Cache repeated prompt prefixes</li>
<li>Reduce redundant computation</li>
<li>Improve throughput for similar requests</li>
</ul>
<h3 id="2-memory-management">2. Memory Management</h3>
<ul>
<li>Model locking with <code>--mlock</code></li>
<li>Careful KV cache sizing</li>
<li>Dynamic batch sizing based on VRAM</li>
</ul>
<h3 id="3-quantization">3. Quantization</h3>
<ul>
<li>Q4_K_M: 4-bit weights with optimized kernels</li>
<li>Minimal accuracy loss</li>
<li>4x memory reduction vs FP16</li>
</ul>
<hr />
<h2 id="benchmarking">Benchmarking</h2>
<h3 id="running-benchmarks">Running Benchmarks</h3>
<pre><code class="language-bash"># Standard benchmark (10 requests)
./build/client --endpoint http://localhost:8090 --requests 10

# High-load test (100 requests, 4 ranks)
./build/scheduler --ranks 4 --requests 100
</code></pre>
<h3 id="metrics-collected">Metrics Collected</h3>
<ul>
<li>Request latency (p50, p95, p99, max)</li>
<li>Throughput (tokens/sec, requests/sec)</li>
<li>GPU utilization</li>
<li>Memory usage</li>
</ul>
<hr />
<h2 id="use-cases">Use Cases</h2>
<ul>
<li><strong>Edge Inference</strong>: Running LLMs on resource-constrained devices</li>
<li><strong>Research</strong>: Studying performance characteristics of quantized models</li>
<li><strong>Education</strong>: Learning CUDA-accelerated inference pipelines</li>
<li><strong>Development</strong>: Prototyping LLM applications without cloud dependencies</li>
</ul>
<hr />
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="out-of-memory-errors">Out of Memory Errors</h3>
<p>Reduce context size or use more aggressive quantization:</p>
<pre><code class="language-bash">./llama-server -m model.gguf --ctx-size 2048
</code></pre>
<h3 id="slow-performance">Slow Performance</h3>
<p>Enable GPU layers:</p>
<pre><code class="language-bash">./llama-server -m model.gguf --n-gpu-layers 99
</code></pre>
<h3 id="connection-refused">Connection Refused</h3>
<p>Check server is running:</p>
<pre><code class="language-bash">curl http://localhost:8090/health
</code></pre>
<hr />
<h2 id="future-enhancements">Future Enhancements</h2>
<ul>
<li>Support for multi-GPU systems</li>
<li>Advanced scheduling algorithms (priority queues, backpressure)</li>
<li>Integration with NCCL for distributed inference</li>
<li>Dynamic model swapping</li>
<li>Quantization-aware fine-tuning support</li>
</ul>
<hr />
<h2 id="notes">Notes</h2>
<p>This project showcases that sophisticated LLM inference is achievable on modest hardware through careful optimization, quantization, and efficient resource management. The combination of CUDA acceleration, quantized models, and smart scheduling enables production-quality inference on GPUs that would typically be considered insufficient for modern LLM workloads.</p>
<p><strong>Key Insight</strong>: The bottleneck often isn't the GPU compute capability, but rather memory bandwidth and capacity. Quantization and efficient memory management unlock LLM inference on previously unsuitable hardware.</p>
<p><strong>Author</strong>: Mohammad Waqas</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "search.suggest", "search.highlight"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
    
  </body>
</html>